{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<VirtualWorker id:alice #objects:0>, <VirtualWorker id:bob #objects:0>]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch as th\n",
    "from torchvision import datasets, transforms\n",
    "from opacus import PrivacyEngine \n",
    "import syft as sy\n",
    "\n",
    "hook = sy.TorchHook(th)\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "workers = [alice, bob]\n",
    "\n",
    "sy.local_worker.is_client_worker = False\n",
    "\n",
    "\n",
    "print(workers)\n",
    "train_datasets = datasets.MNIST('../mnist',\n",
    "                 train=True, download=True,\n",
    "                 transform=transforms.Compose([transforms.ToTensor(),\n",
    "                 transforms.Normalize((0.1307,), (0.3081,)),])\n",
    "                 ).federate(workers=workers)\n",
    "\n",
    "\n",
    "# federated_train_datasets = sy.FederatedDataset([train_datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py:753: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  \"The sample rate will be defined from ``batch_size`` and ``sample_size``.\"\n",
      "m:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py:237: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
      "m:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py:753: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  \"The sample rate will be defined from ``batch_size`` and ``sample_size``.\"\n",
      "m:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py:237: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
      "  0%|          | 0/234 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "The following layers do not have gradients: ['_module.0.weight', '_module.0.bias', '_module.3.weight', '_module.3.bias', '_module.7.weight', '_module.7.bias', '_module.9.weight', '_module.9.bias']. Are you sure they were included in the backward pass?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fefe35862f7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-fefe35862f7a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, delta)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;31m# optimizer.step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvirtual_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mm:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py\u001b[0m in \u001b[0;36mvirtual_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprivacy_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ddp_hooks\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DDP hook does not support virtual steps.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprivacy_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvirtual_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvirtual_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvirtual_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mm:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py\u001b[0m in \u001b[0;36mvirtual_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[0mgradients\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mevery\u001b[0m \u001b[0mmini\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mWe\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mthus\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0mof\u001b[0m \u001b[0marbitrary\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \"\"\"\n\u001b[1;32m--> 594\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclipper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_and_accumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     def _local_layer_ddp_hook(\n",
      "\u001b[1;32mm:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\per_sample_gradient_clip.py\u001b[0m in \u001b[0;36mclip_and_accumulate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;31m# step 0 : calculate the layer norms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         all_norms = calc_sample_norms(\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[0mnamed_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_named_grad_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m             \u001b[0mflat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm_clipper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_per_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         )\n",
      "\u001b[1;32mm:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\per_sample_gradient_clip.py\u001b[0m in \u001b[0;36m_named_grad_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_grad_samples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             raise AttributeError(\n\u001b[1;32m--> 245\u001b[1;33m                 \u001b[1;34mf\"The following layers do not have gradients: {no_grad_samples}. Are you sure they were included in the backward pass?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m             )\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: The following layers do not have gradients: ['_module.0.weight', '_module.0.bias', '_module.3.weight', '_module.3.bias', '_module.7.weight', '_module.7.bias', '_module.9.weight', '_module.9.bias']. Are you sure they were included in the backward pass?"
     ]
    }
   ],
   "source": [
    "def make_model():\n",
    "    return th.nn.Sequential(\n",
    "        th.nn.Conv2d(1, 16, 8, 2, padding=3),\n",
    "        th.nn.ReLU(),\n",
    "        th.nn.MaxPool2d(2, 1),\n",
    "        th.nn.Conv2d(16, 32, 4, 2),\n",
    "        th.nn.ReLU(),\n",
    "        th.nn.MaxPool2d(2, 1),\n",
    "        th.nn.Flatten(), \n",
    "        th.nn.Linear(32 * 4 * 4, 32),\n",
    "        th.nn.ReLU(),\n",
    "        th.nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "# the local version that we will use to do the aggregation\n",
    "local_model = make_model()\n",
    "\n",
    "models, dataloaders, optimizers, privacy_engines = [], [], [], []\n",
    "for worker in workers:\n",
    "    model = make_model()\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=0.1)\n",
    "    model.send(worker)\n",
    "    dataset = train_datasets[worker.id]\n",
    "    # dataloader = sy.FederatedDataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "    dataloader = th.utils.data.DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "    privacy_engine = PrivacyEngine(model,\n",
    "                                   batch_size=128, \n",
    "                                   sample_size=len(dataset), \n",
    "                                   alphas=range(2,32), \n",
    "                                   noise_multiplier=1.2,\n",
    "                                   max_grad_norm=1.0)\n",
    "    privacy_engine.attach(optimizer)\n",
    "    \n",
    "    models.append(model)\n",
    "    dataloaders.append(dataloader)\n",
    "    optimizers.append(optimizer)\n",
    "    privacy_engines.append(privacy_engine)\n",
    "    \n",
    "def send_new_models(local_model, models):\n",
    "    with th.no_grad():\n",
    "        for remote_model in models:\n",
    "            for new_param, remote_param in zip(local_model.parameters(), remote_model.parameters()):\n",
    "                worker = remote_param.location\n",
    "                remote_value = new_param.send(worker)\n",
    "                remote_param.set_(remote_value)\n",
    "\n",
    "            \n",
    "def federated_aggregation(local_model, models):\n",
    "    with th.no_grad():\n",
    "        for local_param, *remote_params in zip(*([local_model.parameters()] + [model.parameters() for model in models])):\n",
    "            param_stack = th.zeros(*remote_params[0].shape)\n",
    "            for remote_param in remote_params:\n",
    "                param_stack += remote_param.copy().get()\n",
    "            param_stack /= len(remote_params)\n",
    "            local_param.set_(param_stack)\n",
    "\n",
    "def train(epoch, delta):\n",
    "        \n",
    "    # 1. Send new version of the model\n",
    "    send_new_models(local_model, models)\n",
    "\n",
    "    # 2. Train remotely the models\n",
    "    for i, worker in enumerate(workers):\n",
    "        dataloader = dataloaders[i]\n",
    "        model = models[i]\n",
    "        optimizer = optimizers[i]\n",
    "        \n",
    "        model.train()\n",
    "        criterion = th.nn.CrossEntropyLoss()\n",
    "        losses = []   \n",
    "        for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # optimizer.virtual_step()\n",
    "            losses.append(loss.get().item()) \n",
    "\n",
    "        sy.local_worker.clear_objects()\n",
    "        epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(delta) \n",
    "        print(\n",
    "            f\"[{worker.id}]\\t\"\n",
    "            f\"Train Epoch: {epoch} \\t\"\n",
    "            f\"Loss: {sum(losses)/len(losses):.4f} \"\n",
    "            f\"(ε = {epsilon:.2f}, δ = {delta}) for α = {best_alpha}\")\n",
    "\n",
    "    # 3. Federated aggregation of the updated models\n",
    "    federated_aggregation(local_model, models)\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(epoch, delta=1e-5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27726a6b9d0b3aecf19ddb8fb5d165165384e9a9dccccc704489801e8c9c2418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('spam_classifier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

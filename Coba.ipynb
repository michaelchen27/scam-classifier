{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<VirtualWorker id:alice #objects:0>, <VirtualWorker id:bob #objects:0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py:753: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  \"The sample rate will be defined from ``batch_size`` and ``sample_size``.\"\n",
      "m:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py:237: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
      "m:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py:753: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  \"The sample rate will be defined from ``batch_size`` and ``sample_size``.\"\n",
      "m:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py:237: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
      "  0%|          | 0/234 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "The following layers do not have gradients: ['_module.0.weight', '_module.0.bias', '_module.3.weight', '_module.3.bias', '_module.7.weight', '_module.7.bias', '_module.9.weight', '_module.9.bias']. Are you sure they were included in the backward pass?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-371012dad3f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-371012dad3f1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, delta)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mm:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py\u001b[0m in \u001b[0;36mdp_step\u001b[1;34m(self, closure, is_empty)\u001b[0m\n\u001b[0;32m    360\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprivacy_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m                 if isinstance(\n\u001b[0;32m    364\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprivacy_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mm:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\privacy_engine.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, is_empty)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclipper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_and_accumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m             \u001b[0mclip_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclipper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mm:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\per_sample_gradient_clip.py\u001b[0m in \u001b[0;36mclip_and_accumulate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;31m# step 0 : calculate the layer norms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         all_norms = calc_sample_norms(\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[0mnamed_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_named_grad_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m             \u001b[0mflat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm_clipper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_per_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         )\n",
      "\u001b[1;32mm:\\andrelmfarias\\private-ai\\federated_learning\\opacus\\opacus\\per_sample_gradient_clip.py\u001b[0m in \u001b[0;36m_named_grad_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_grad_samples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             raise AttributeError(\n\u001b[1;32m--> 245\u001b[1;33m                 \u001b[1;34mf\"The following layers do not have gradients: {no_grad_samples}. Are you sure they were included in the backward pass?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m             )\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: The following layers do not have gradients: ['_module.0.weight', '_module.0.bias', '_module.3.weight', '_module.3.bias', '_module.7.weight', '_module.7.bias', '_module.9.weight', '_module.9.bias']. Are you sure they were included in the backward pass?"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch as th\n",
    "from torchvision import datasets, transforms\n",
    "from opacus import PrivacyEngine \n",
    "import syft as sy\n",
    "\n",
    "hook = sy.TorchHook(th)\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "workers = [alice, bob]\n",
    "\n",
    "sy.local_worker.is_client_worker = False\n",
    "\n",
    "\n",
    "print(workers)\n",
    "train_datasets = datasets.MNIST('../mnist',\n",
    "                 train=True, download=True,\n",
    "                 transform=transforms.Compose([transforms.ToTensor(),\n",
    "                 transforms.Normalize((0.1307,), (0.3081,)),])\n",
    "                 ).federate(workers)\n",
    "\n",
    "def make_model():\n",
    "    return th.nn.Sequential(\n",
    "        th.nn.Conv2d(1, 16, 8, 2, padding=3),\n",
    "        th.nn.ReLU(),\n",
    "        th.nn.MaxPool2d(2, 1),\n",
    "        th.nn.Conv2d(16, 32, 4, 2),\n",
    "        th.nn.ReLU(),\n",
    "        th.nn.MaxPool2d(2, 1),\n",
    "        th.nn.Flatten(), \n",
    "        th.nn.Linear(32 * 4 * 4, 32),\n",
    "        th.nn.ReLU(),\n",
    "        th.nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "# the local version that we will use to do the aggregation\n",
    "local_model = make_model()\n",
    "\n",
    "models, dataloaders, optimizers, privacy_engines = [], [], [], []\n",
    "for worker in workers:\n",
    "    model = make_model()\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=0.1)\n",
    "    model.send(worker)\n",
    "    dataset = train_datasets[worker.id]\n",
    "    dataloader = th.utils.data.DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "    privacy_engine = PrivacyEngine(model,\n",
    "                                   batch_size=128, \n",
    "                                   sample_size=len(dataset), \n",
    "                                   alphas=range(2,32), \n",
    "                                   noise_multiplier=1.2,\n",
    "                                   max_grad_norm=1.0)\n",
    "    privacy_engine.attach(optimizer)\n",
    "    \n",
    "    models.append(model)\n",
    "    dataloaders.append(dataloader)\n",
    "    optimizers.append(optimizer)\n",
    "    privacy_engines.append(privacy_engine)\n",
    "    \n",
    "def send_new_models(local_model, models):\n",
    "    with th.no_grad():\n",
    "        for remote_model in models:\n",
    "            for new_param, remote_param in zip(local_model.parameters(), remote_model.parameters()):\n",
    "                worker = remote_param.location\n",
    "                remote_value = new_param.send(worker)\n",
    "                remote_param.set_(remote_value)\n",
    "\n",
    "            \n",
    "def federated_aggregation(local_model, models):\n",
    "    with th.no_grad():\n",
    "        for local_param, *remote_params in zip(*([local_model.parameters()] + [model.parameters() for model in models])):\n",
    "            param_stack = th.zeros(*remote_params[0].shape)\n",
    "            for remote_param in remote_params:\n",
    "                param_stack += remote_param.copy().get()\n",
    "            param_stack /= len(remote_params)\n",
    "            local_param.set_(param_stack)\n",
    "\n",
    "def train(epoch, delta):\n",
    "        \n",
    "    # 1. Send new version of the model\n",
    "    send_new_models(local_model, models)\n",
    "\n",
    "    # 2. Train remotely the models\n",
    "    for i, worker in enumerate(workers):\n",
    "        dataloader = dataloaders[i]\n",
    "        model = models[i]\n",
    "        optimizer = optimizers[i]\n",
    "        \n",
    "        model.train()\n",
    "        criterion = th.nn.CrossEntropyLoss()\n",
    "        losses = []   \n",
    "        for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            # optimizer.step()\n",
    "            optimizer.virtual_step()\n",
    "            losses.append(loss.get().item()) \n",
    "\n",
    "        sy.local_worker.clear_objects()\n",
    "        epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(delta) \n",
    "        print(\n",
    "            f\"[{worker.id}]\\t\"\n",
    "            f\"Train Epoch: {epoch} \\t\"\n",
    "            f\"Loss: {sum(losses)/len(losses):.4f} \"\n",
    "            f\"(ε = {epsilon:.2f}, δ = {delta}) for α = {best_alpha}\")\n",
    "\n",
    "    # 3. Federated aggregation of the updated models\n",
    "    federated_aggregation(local_model, models)\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(epoch, delta=1e-5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27726a6b9d0b3aecf19ddb8fb5d165165384e9a9dccccc704489801e8c9c2418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('spam_classifier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

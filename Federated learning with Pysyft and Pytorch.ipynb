{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.160008Z",
     "start_time": "2019-06-03T19:33:39.344527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:cuda\n",
      "Torch Ver: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import sqrt, argmax\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "# import opacus\n",
    "# from opacus import PrivacyEngine\n",
    "\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# from syft.frameworks.torch.nn import GRU\n",
    "from handcrafted_GRU import GRU\n",
    "# from opacus.layers import DPGRU\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device:{device}\")\n",
    "\n",
    "print(f\"Torch Ver: {torch.__version__}\")\n",
    "# print(f\"Opacus Ver: {opacus.__version__}\")\n",
    "# print(f\"Syft Ver: {sy.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('indonesian'))\n",
    "# print(f\"STOPWORDS:\\n {STOPWORDS}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # print(f\"\\n\\nOriginal Text: {text}\")\n",
    "    text = text.lower()\n",
    "    # print(f\"\\nCase Lowered Text: {text}\")\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # print(f\"\\nRegexed Text: {text}\")\n",
    "    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n",
    "    # print(f\"\\nStopwords Removed Text: {text}\")\n",
    "    return text\n",
    "\n",
    "def tokenize(text, word_to_idx):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        if word in word_to_idx:\n",
    "            tokens.append(word_to_idx[word])\n",
    "        else:\n",
    "            tokens.append(0)\n",
    "    return tokens\n",
    "\n",
    "def pad_and_truncate(messages, max_length=30):\n",
    "    features = np.zeros((len(messages), max_length), dtype=int)\n",
    "    # pprint(f\"Messages: {messages}\\nFeatures: {features}\")\n",
    "    for i, sms in enumerate(messages):\n",
    "        # print(f\"\\ni: {i}\\nSMS:{sms}\")\n",
    "        if len(sms):\n",
    "            features[i, -len(sms):] = sms[:max_length]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Messages: 0                           [3454, 2763, 2128, 761, 1621]\\n'\n",
      " '1       [1043, 3719, 3504, 1572, 367, 3096, 3298, 2085...\\n'\n",
      " '2       [1016, 4103, 1722, 3148, 3916, 3029, 3570, 451...\\n'\n",
      " '3       [851, 1146, 2059, 4116, 3284, 2784, 1574, 1146...\\n'\n",
      " '4       [933, 1296, 2982, 281, 4234, 3757, 3425, 176, ...\\n'\n",
      " '                              ...                        \\n'\n",
      " '1235    [4120, 3952, 3157, 428, 1425, 3032, 3607, 2732...\\n'\n",
      " '1236            [3454, 1120, 2501, 1816, 2483, 1816, 412]\\n'\n",
      " '1237    [1016, 1638, 2389, 2488, 2472, 1161, 1109, 286...\\n'\n",
      " '1238                                    [3453, 995, 1620]\\n'\n",
      " '1239        [706, 2657, 1520, 706, 2206, 251, 2657, 2967]\\n'\n",
      " 'Name: Teks, Length: 1240, dtype: object\\n'\n",
      " 'Features: [[0 0 0 ... 0 0 0]\\n'\n",
      " ' [0 0 0 ... 0 0 0]\\n'\n",
      " ' [0 0 0 ... 0 0 0]\\n'\n",
      " ' ...\\n'\n",
      " ' [0 0 0 ... 0 0 0]\\n'\n",
      " ' [0 0 0 ... 0 0 0]\\n'\n",
      " ' [0 0 0 ... 0 0 0]]')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('train_sms_1240.csv', sep=',', names=['Teks', 'Label'], encoding= 'unicode_escape')\n",
    "    # data = data.sample(frac = 1)\n",
    "    # Lowercase, remove unnecessary char with regex, remove stop words\n",
    "    data.Teks = data.Teks.apply(clean_text)\n",
    "    # print(data.Teks)\n",
    "    words = set((' '.join(data.Teks)).split())\n",
    "    # print(words)\n",
    "    word_to_idx = {word: i for i, word in enumerate(words, start=1)}\n",
    "    # print(word_to_idx)\n",
    "    tokens = data.Teks.apply(lambda x: tokenize(x, word_to_idx))\n",
    "    # print(tokens)\n",
    "    inputs = pad_and_truncate(tokens)\n",
    "    # pprint(inputs)\n",
    "    labels = np.array((data.Label == '1').astype(int))\n",
    "\n",
    "    np.save('labels.npy', labels)\n",
    "    np.save('inputs.npy', inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.207763Z",
     "start_time": "2019-06-03T19:33:40.201292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training params\n",
    "TRAIN_VOCAB_SIZE = int(inputs.max()) + 1\n",
    "EPOCHS = 200\n",
    "CLIP = 5 # gradient clipping - to avoid gradient explosion (frequent in RNNs)\n",
    "lr = 0.001\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.197935Z",
     "start_time": "2019-06-03T19:33:40.186270Z"
    }
   },
   "outputs": [],
   "source": [
    "original_inputs = np.load('inputs.npy')\n",
    "original_labels = np.load('labels.npy')\n",
    "\n",
    "inputs = torch.tensor(original_inputs)\n",
    "labels = torch.tensor(original_labels)\n",
    "\n",
    "# splitting training and test data\n",
    "# 20% of the data will be for Tests.\n",
    "pct_test = 0.2\n",
    "\n",
    "#20% of total data\n",
    "pct_test_count = -int(len(labels)*pct_test)\n",
    "\n",
    "# Get 80% of Train LABELS from left.\n",
    "train_labels = labels[:pct_test_count]\n",
    "print(f\"Train Labels: [:{pct_test_count}]\")\n",
    "\n",
    "# Get 80% of Train INPUTS from left.\n",
    "train_inputs = inputs[:pct_test_count]\n",
    "print(f\"Train Inputs: [:{pct_test_count}]\")\n",
    "\n",
    "# Get the rest of the LABEL data for test on the right (20%) \n",
    "test_labels = labels[pct_test_count:]\n",
    "print(f\"Test Labels: [{pct_test_count}:]\")\n",
    "\n",
    "# Get the rest of the INPUT data for test on the right (20%)\n",
    "test_inputs = inputs[pct_test_count:]\n",
    "print(f\"Test Inputs: [{pct_test_count}:]\")\n",
    "\n",
    "\n",
    "SAMPLE_SIZE = len(labels)\n",
    "print(f\"Sample Size: {SAMPLE_SIZE}\")\n",
    "\n",
    "print(f\"20% of Sample Size: {SAMPLE_SIZE*pct_test}\")\n",
    "\n",
    "# For Local Model Evaluation\n",
    "original_test_inputs = original_inputs[pct_test_count:]\n",
    "original_test_labels = original_labels[pct_test_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VirtualWorkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.591430Z",
     "start_time": "2019-06-03T19:33:41.969220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hook that extends the Pytorch library \n",
    "# to enable all computations with pointers of tensors sent to other workers\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Creating 2 virtual workers Syft v0.2.9\n",
    "anne = sy.VirtualWorker(hook, id=\"anne\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "\n",
    "workers = [anne, bob]\n",
    "\n",
    "# this is done to have the local worker (you on your notebook!) have a registry\n",
    "# of objects like every other workers, which is disabled by default but needed here\n",
    "# sy.local_worker.is_client_worker = False\n",
    "\n",
    "\n",
    "# threshold indexes for dataset split (one half for Bob, other half for Anne)\n",
    "train_idx = int(len(train_labels)/2)\n",
    "test_idx = int(len(test_labels)/2)\n",
    "\n",
    "# Sending toy datasets to virtual workers\n",
    "bob_train_dataset = sy.BaseDataset(train_inputs[:train_idx], train_labels[:train_idx]).send(bob)\n",
    "anne_train_dataset = sy.BaseDataset(train_inputs[train_idx:], train_labels[train_idx:]).send(anne)\n",
    "bob_test_dataset = sy.BaseDataset(test_inputs[:test_idx], test_labels[:test_idx]).send(bob)\n",
    "anne_test_dataset = sy.BaseDataset(test_inputs[test_idx:], test_labels[test_idx:]).send(anne)\n",
    "\n",
    "# print(f\"Train Index: {train_idx}\")\n",
    "# print(f\"Test Index: {test_idx}\")\n",
    "\n",
    "# print(f\"Anne's Data Amount: {len(train_inputs[train_idx:])}\")\n",
    "# print(f\"Bob's Data Amount: {len(train_inputs[:train_idx])}\")\n",
    "\n",
    "\n",
    "# Creating federated datasets, an extension of Pytorch TensorDataset class \n",
    "# for TRAINING METHOD #1 (with aggregation)\n",
    "bob_federated_train_dataset = sy.FederatedDataset([bob_train_dataset])\n",
    "anne_federated_train_dataset = sy.FederatedDataset([anne_train_dataset])\n",
    "bob_federated_test_dataset = sy.FederatedDataset([bob_test_dataset])\n",
    "anne_federated_test_dataset = sy.FederatedDataset([anne_test_dataset])\n",
    "\n",
    "\n",
    "merged_test_dataset = list(zip(original_test_inputs, original_test_labels))\n",
    "\n",
    "# print(f\"Input:{original_test_inputs[1]}\\t Label:{original_test_labels[1]}\")\n",
    "# print(merged_test_dataset[0])\n",
    "\n",
    "def collate_batch(batch):\n",
    "        label_list, text_list = [], []\n",
    "        for (_label, _text) in batch:\n",
    "                label_list.append(_label)\n",
    "                text_list.append(_text)\n",
    "        return label_list, text_list\n",
    "\n",
    "original_test_dataloader = DataLoader(merged_test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.638046Z",
     "start_time": "2019-06-03T19:33:42.617601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiating the model\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# model = GRU(vocab_size=TRAIN_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "# torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "def make_model():\n",
    "    model = GRU(vocab_size=TRAIN_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "    return model\n",
    "    \n",
    "local_model = make_model()\n",
    "\n",
    "models, train_dataloaders, test_dataloaders, optimizers, privacy_engines = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching model, dataloaders, and optimizers to each worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in workers:\n",
    "    model = make_model()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    model.send(worker)\n",
    "    if(worker == anne):\n",
    "        train_dataset = anne_federated_train_dataset\n",
    "        test_dataset = anne_federated_test_dataset\n",
    "    elif(worker == bob):\n",
    "        train_dataset = bob_federated_train_dataset\n",
    "        test_dataset = bob_federated_test_dataset\n",
    "\n",
    "\n",
    "    train_dataloader = sy.FederatedDataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "    test_dataloader = sy.FederatedDataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    models.append(model)\n",
    "    train_dataloaders.append(train_dataloader)\n",
    "    test_dataloaders.append(test_dataloader)\n",
    "    optimizers.append(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to aggregate remote models and to send new updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_aggregation(local_model, models):\n",
    "    with torch.no_grad():\n",
    "        for local_param, *remote_params in zip(*([local_model.parameters()] + [model.parameters() for model in models])):\n",
    "            param_stack = torch.zeros(*remote_params[0].shape)\n",
    "            for remote_param in remote_params:\n",
    "                param_stack += remote_param.copy().get()\n",
    "                # print(f\"Param Stack Sum: {param_stack}\")\n",
    "            param_stack /= len(remote_params)\n",
    "            # print(f\"Param Stack Division: {param_stack}\")\n",
    "            local_param.set_(param_stack)\n",
    "\n",
    "def send_new_models(local_model, models):\n",
    "    with torch.no_grad():\n",
    "        for remote_model in models:\n",
    "            for new_param, remote_param in zip(local_model.parameters(), remote_model.parameters()):\n",
    "                worker = remote_param.location\n",
    "                remote_value = new_param.send(worker)\n",
    "                remote_param.set_(remote_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [[], []]\n",
    "test_losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    # 1. Send new version of the model\n",
    "    send_new_models(local_model, models)\n",
    "\n",
    "    # 2. Train remotely the models\n",
    "    for i, worker in enumerate(workers):\n",
    "        train_dataloader = train_dataloaders[i]\n",
    "        model = models[i]\n",
    "        optimizer = optimizers[i]\n",
    "        \n",
    "        model.train()\n",
    "        criterion = nn.BCELoss() # for two class classification\n",
    "        losses = []   \n",
    "    \n",
    "        for data, target in train_dataloader:            \n",
    "            data = data.to(torch.long)\n",
    "            h = torch.Tensor(torch.zeros(BATCH_SIZE, HIDDEN_DIM)).send(worker)  \n",
    "            \n",
    "            # Call zero grad to clear previous gradient before every training passses.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print(f\"Data:{data}\\nTarget: {target}\\n\")\n",
    "\n",
    "            # print(f\"Worker: {worker}\\nWorker Objects: {worker.object_store._objects}\")\n",
    "\n",
    "            output, _ = model(data.to(torch.long), h)\n",
    "            # print(f\"Output: {output}\")\n",
    "            loss = criterion(output.squeeze(), target.float())\n",
    "            loss.backward()\n",
    "\n",
    "            # # Clipping the gradient to avoid explosion\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            losses.append(loss.get()) \n",
    "            optimizer.step()\n",
    "\n",
    "        sy.local_worker.clear_objects()\n",
    "        \n",
    "\n",
    "        train_loss = sum(losses) / len(losses)\n",
    "        train_losses[i].append(train_loss.item())\n",
    "\n",
    "        print(\n",
    "            f\"[{worker.id}]\\t\"\n",
    "            f\"Train Epoch: {epoch+1}/{EPOCHS} \\t\"\n",
    "            f\"Train Loss: {train_loss:.4f} \")\n",
    "\n",
    "    # 3. Federated aggregation of the updated models\n",
    "    federated_aggregation(local_model, models)\n",
    "\n",
    "\n",
    "def eval(epoch, last_loss, trigger_times, patience):\n",
    "    # 4. Evaluate the model\n",
    "    local_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        test_labels_list = []\n",
    "        eval_losses = []\n",
    "\n",
    "        for inputs, labels in original_test_dataloader:\n",
    "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "            output, _ = local_model(torch.LongTensor(inputs), h)\n",
    "            criterion = nn.BCELoss()\n",
    "            labels = torch.LongTensor(labels)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            eval_losses.append(loss)\n",
    "            preds = output.squeeze()\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.numpy().astype(int))\n",
    "    \n",
    "    # score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "    eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    test_losses.append(eval_loss.item())\n",
    "\n",
    "            \n",
    "    # Early Stopping\n",
    "    if eval_loss > last_loss:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times: {trigger_times}\")\n",
    "        \n",
    "        if trigger_times >= patience:\n",
    "            print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Trigger Times: 0\")\n",
    "        trigger_times = 0\n",
    "    \n",
    "    last_loss = eval_loss\n",
    "\n",
    "    print(\n",
    "        f\"Eval Epoch: {epoch} \\t\"\n",
    "        f\"Eval Loss: {eval_loss:.4f} \\n\\n\")\n",
    "        # f\"AUC: {score:.3%} \\t\"\n",
    "\n",
    "    return last_loss, trigger_times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[anne]\tTrain Epoch: 1/200 \tTrain Loss: 0.7080 \n",
      "[bob]\tTrain Epoch: 1/200 \tTrain Loss: 0.6656 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 0 \tEval Loss: 0.6534 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 2/200 \tTrain Loss: 0.7036 \n",
      "[bob]\tTrain Epoch: 2/200 \tTrain Loss: 0.6702 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 1 \tEval Loss: 0.6483 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 3/200 \tTrain Loss: 0.6983 \n",
      "[bob]\tTrain Epoch: 3/200 \tTrain Loss: 0.6650 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 2 \tEval Loss: 0.6431 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 4/200 \tTrain Loss: 0.6908 \n",
      "[bob]\tTrain Epoch: 4/200 \tTrain Loss: 0.6556 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 3 \tEval Loss: 0.6381 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 5/200 \tTrain Loss: 0.6838 \n",
      "[bob]\tTrain Epoch: 5/200 \tTrain Loss: 0.6522 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 4 \tEval Loss: 0.6331 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 6/200 \tTrain Loss: 0.6805 \n",
      "[bob]\tTrain Epoch: 6/200 \tTrain Loss: 0.6451 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 5 \tEval Loss: 0.6282 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 7/200 \tTrain Loss: 0.6752 \n",
      "[bob]\tTrain Epoch: 7/200 \tTrain Loss: 0.6343 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 6 \tEval Loss: 0.6234 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 8/200 \tTrain Loss: 0.6718 \n",
      "[bob]\tTrain Epoch: 8/200 \tTrain Loss: 0.6375 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 7 \tEval Loss: 0.6185 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 9/200 \tTrain Loss: 0.6644 \n",
      "[bob]\tTrain Epoch: 9/200 \tTrain Loss: 0.6242 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 8 \tEval Loss: 0.6138 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 10/200 \tTrain Loss: 0.6619 \n",
      "[bob]\tTrain Epoch: 10/200 \tTrain Loss: 0.6234 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 9 \tEval Loss: 0.6091 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 11/200 \tTrain Loss: 0.6627 \n",
      "[bob]\tTrain Epoch: 11/200 \tTrain Loss: 0.6146 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 10 \tEval Loss: 0.6045 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 12/200 \tTrain Loss: 0.6514 \n",
      "[bob]\tTrain Epoch: 12/200 \tTrain Loss: 0.6140 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 11 \tEval Loss: 0.5999 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 13/200 \tTrain Loss: 0.6476 \n",
      "[bob]\tTrain Epoch: 13/200 \tTrain Loss: 0.6056 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 12 \tEval Loss: 0.5954 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 14/200 \tTrain Loss: 0.6448 \n",
      "[bob]\tTrain Epoch: 14/200 \tTrain Loss: 0.6003 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 13 \tEval Loss: 0.5909 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 15/200 \tTrain Loss: 0.6416 \n",
      "[bob]\tTrain Epoch: 15/200 \tTrain Loss: 0.5959 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 14 \tEval Loss: 0.5865 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 16/200 \tTrain Loss: 0.6268 \n",
      "[bob]\tTrain Epoch: 16/200 \tTrain Loss: 0.5963 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 15 \tEval Loss: 0.5821 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 17/200 \tTrain Loss: 0.6291 \n",
      "[bob]\tTrain Epoch: 17/200 \tTrain Loss: 0.5803 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 16 \tEval Loss: 0.5778 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 18/200 \tTrain Loss: 0.6243 \n",
      "[bob]\tTrain Epoch: 18/200 \tTrain Loss: 0.5830 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 17 \tEval Loss: 0.5735 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 19/200 \tTrain Loss: 0.6171 \n",
      "[bob]\tTrain Epoch: 19/200 \tTrain Loss: 0.5726 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 18 \tEval Loss: 0.5693 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 20/200 \tTrain Loss: 0.6129 \n",
      "[bob]\tTrain Epoch: 20/200 \tTrain Loss: 0.5697 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 19 \tEval Loss: 0.5652 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 21/200 \tTrain Loss: 0.6165 \n",
      "[bob]\tTrain Epoch: 21/200 \tTrain Loss: 0.5702 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 20 \tEval Loss: 0.5610 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 22/200 \tTrain Loss: 0.6075 \n",
      "[bob]\tTrain Epoch: 22/200 \tTrain Loss: 0.5630 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 21 \tEval Loss: 0.5569 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 23/200 \tTrain Loss: 0.6060 \n",
      "[bob]\tTrain Epoch: 23/200 \tTrain Loss: 0.5580 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 22 \tEval Loss: 0.5529 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 24/200 \tTrain Loss: 0.5950 \n",
      "[bob]\tTrain Epoch: 24/200 \tTrain Loss: 0.5559 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 23 \tEval Loss: 0.5489 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 25/200 \tTrain Loss: 0.5915 \n",
      "[bob]\tTrain Epoch: 25/200 \tTrain Loss: 0.5524 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 24 \tEval Loss: 0.5449 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 26/200 \tTrain Loss: 0.5884 \n",
      "[bob]\tTrain Epoch: 26/200 \tTrain Loss: 0.5455 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 25 \tEval Loss: 0.5411 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 27/200 \tTrain Loss: 0.5876 \n",
      "[bob]\tTrain Epoch: 27/200 \tTrain Loss: 0.5416 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 26 \tEval Loss: 0.5372 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 28/200 \tTrain Loss: 0.5785 \n",
      "[bob]\tTrain Epoch: 28/200 \tTrain Loss: 0.5372 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 27 \tEval Loss: 0.5334 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 29/200 \tTrain Loss: 0.5718 \n",
      "[bob]\tTrain Epoch: 29/200 \tTrain Loss: 0.5276 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 28 \tEval Loss: 0.5296 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 30/200 \tTrain Loss: 0.5732 \n",
      "[bob]\tTrain Epoch: 30/200 \tTrain Loss: 0.5244 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 29 \tEval Loss: 0.5259 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 31/200 \tTrain Loss: 0.5691 \n",
      "[bob]\tTrain Epoch: 31/200 \tTrain Loss: 0.5283 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 30 \tEval Loss: 0.5222 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 32/200 \tTrain Loss: 0.5573 \n",
      "[bob]\tTrain Epoch: 32/200 \tTrain Loss: 0.5166 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 31 \tEval Loss: 0.5186 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 33/200 \tTrain Loss: 0.5611 \n",
      "[bob]\tTrain Epoch: 33/200 \tTrain Loss: 0.5119 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 32 \tEval Loss: 0.5150 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 34/200 \tTrain Loss: 0.5670 \n",
      "[bob]\tTrain Epoch: 34/200 \tTrain Loss: 0.5151 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 33 \tEval Loss: 0.5113 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 35/200 \tTrain Loss: 0.5468 \n",
      "[bob]\tTrain Epoch: 35/200 \tTrain Loss: 0.5104 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 34 \tEval Loss: 0.5078 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 36/200 \tTrain Loss: 0.5516 \n",
      "[bob]\tTrain Epoch: 36/200 \tTrain Loss: 0.5017 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 35 \tEval Loss: 0.5043 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 37/200 \tTrain Loss: 0.5445 \n",
      "[bob]\tTrain Epoch: 37/200 \tTrain Loss: 0.4980 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 36 \tEval Loss: 0.5009 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 38/200 \tTrain Loss: 0.5435 \n",
      "[bob]\tTrain Epoch: 38/200 \tTrain Loss: 0.4947 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 37 \tEval Loss: 0.4975 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 39/200 \tTrain Loss: 0.5363 \n",
      "[bob]\tTrain Epoch: 39/200 \tTrain Loss: 0.4931 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 38 \tEval Loss: 0.4941 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 40/200 \tTrain Loss: 0.5359 \n",
      "[bob]\tTrain Epoch: 40/200 \tTrain Loss: 0.4923 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 39 \tEval Loss: 0.4907 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 41/200 \tTrain Loss: 0.5297 \n",
      "[bob]\tTrain Epoch: 41/200 \tTrain Loss: 0.4869 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 40 \tEval Loss: 0.4874 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 42/200 \tTrain Loss: 0.5250 \n",
      "[bob]\tTrain Epoch: 42/200 \tTrain Loss: 0.4796 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 41 \tEval Loss: 0.4841 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 43/200 \tTrain Loss: 0.5208 \n",
      "[bob]\tTrain Epoch: 43/200 \tTrain Loss: 0.4789 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 42 \tEval Loss: 0.4809 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 44/200 \tTrain Loss: 0.5172 \n",
      "[bob]\tTrain Epoch: 44/200 \tTrain Loss: 0.4784 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 43 \tEval Loss: 0.4777 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 45/200 \tTrain Loss: 0.5131 \n",
      "[bob]\tTrain Epoch: 45/200 \tTrain Loss: 0.4717 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 44 \tEval Loss: 0.4745 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 46/200 \tTrain Loss: 0.5155 \n",
      "[bob]\tTrain Epoch: 46/200 \tTrain Loss: 0.4746 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 45 \tEval Loss: 0.4714 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 47/200 \tTrain Loss: 0.5094 \n",
      "[bob]\tTrain Epoch: 47/200 \tTrain Loss: 0.4656 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 46 \tEval Loss: 0.4683 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 48/200 \tTrain Loss: 0.5057 \n",
      "[bob]\tTrain Epoch: 48/200 \tTrain Loss: 0.4658 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 47 \tEval Loss: 0.4652 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 49/200 \tTrain Loss: 0.4992 \n",
      "[bob]\tTrain Epoch: 49/200 \tTrain Loss: 0.4587 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 48 \tEval Loss: 0.4621 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 50/200 \tTrain Loss: 0.4954 \n",
      "[bob]\tTrain Epoch: 50/200 \tTrain Loss: 0.4565 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 49 \tEval Loss: 0.4591 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 51/200 \tTrain Loss: 0.4918 \n",
      "[bob]\tTrain Epoch: 51/200 \tTrain Loss: 0.4541 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 50 \tEval Loss: 0.4561 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 52/200 \tTrain Loss: 0.4925 \n",
      "[bob]\tTrain Epoch: 52/200 \tTrain Loss: 0.4495 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 51 \tEval Loss: 0.4532 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 53/200 \tTrain Loss: 0.4861 \n",
      "[bob]\tTrain Epoch: 53/200 \tTrain Loss: 0.4482 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 52 \tEval Loss: 0.4503 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 54/200 \tTrain Loss: 0.4791 \n",
      "[bob]\tTrain Epoch: 54/200 \tTrain Loss: 0.4443 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 53 \tEval Loss: 0.4474 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 55/200 \tTrain Loss: 0.4826 \n",
      "[bob]\tTrain Epoch: 55/200 \tTrain Loss: 0.4436 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 54 \tEval Loss: 0.4445 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 56/200 \tTrain Loss: 0.4762 \n",
      "[bob]\tTrain Epoch: 56/200 \tTrain Loss: 0.4388 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 55 \tEval Loss: 0.4417 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 57/200 \tTrain Loss: 0.4704 \n",
      "[bob]\tTrain Epoch: 57/200 \tTrain Loss: 0.4362 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 56 \tEval Loss: 0.4389 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 58/200 \tTrain Loss: 0.4691 \n",
      "[bob]\tTrain Epoch: 58/200 \tTrain Loss: 0.4318 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 57 \tEval Loss: 0.4361 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 59/200 \tTrain Loss: 0.4675 \n",
      "[bob]\tTrain Epoch: 59/200 \tTrain Loss: 0.4305 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 58 \tEval Loss: 0.4334 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 60/200 \tTrain Loss: 0.4665 \n",
      "[bob]\tTrain Epoch: 60/200 \tTrain Loss: 0.4244 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 59 \tEval Loss: 0.4306 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 61/200 \tTrain Loss: 0.4612 \n",
      "[bob]\tTrain Epoch: 61/200 \tTrain Loss: 0.4237 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 60 \tEval Loss: 0.4279 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 62/200 \tTrain Loss: 0.4553 \n",
      "[bob]\tTrain Epoch: 62/200 \tTrain Loss: 0.4223 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 61 \tEval Loss: 0.4253 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 63/200 \tTrain Loss: 0.4587 \n",
      "[bob]\tTrain Epoch: 63/200 \tTrain Loss: 0.4150 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 62 \tEval Loss: 0.4226 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 64/200 \tTrain Loss: 0.4545 \n",
      "[bob]\tTrain Epoch: 64/200 \tTrain Loss: 0.4188 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 63 \tEval Loss: 0.4200 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 65/200 \tTrain Loss: 0.4507 \n",
      "[bob]\tTrain Epoch: 65/200 \tTrain Loss: 0.4142 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 64 \tEval Loss: 0.4174 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 66/200 \tTrain Loss: 0.4491 \n",
      "[bob]\tTrain Epoch: 66/200 \tTrain Loss: 0.4112 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 65 \tEval Loss: 0.4148 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 67/200 \tTrain Loss: 0.4481 \n",
      "[bob]\tTrain Epoch: 67/200 \tTrain Loss: 0.4047 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 66 \tEval Loss: 0.4123 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 68/200 \tTrain Loss: 0.4398 \n",
      "[bob]\tTrain Epoch: 68/200 \tTrain Loss: 0.4024 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 67 \tEval Loss: 0.4098 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 69/200 \tTrain Loss: 0.4356 \n",
      "[bob]\tTrain Epoch: 69/200 \tTrain Loss: 0.4044 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 68 \tEval Loss: 0.4073 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 70/200 \tTrain Loss: 0.4359 \n",
      "[bob]\tTrain Epoch: 70/200 \tTrain Loss: 0.4000 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 69 \tEval Loss: 0.4048 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 71/200 \tTrain Loss: 0.4297 \n",
      "[bob]\tTrain Epoch: 71/200 \tTrain Loss: 0.3955 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 70 \tEval Loss: 0.4023 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 72/200 \tTrain Loss: 0.4279 \n",
      "[bob]\tTrain Epoch: 72/200 \tTrain Loss: 0.3950 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 71 \tEval Loss: 0.3999 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 73/200 \tTrain Loss: 0.4321 \n",
      "[bob]\tTrain Epoch: 73/200 \tTrain Loss: 0.3934 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 72 \tEval Loss: 0.3975 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 74/200 \tTrain Loss: 0.4214 \n",
      "[bob]\tTrain Epoch: 74/200 \tTrain Loss: 0.3891 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 73 \tEval Loss: 0.3951 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 75/200 \tTrain Loss: 0.4208 \n",
      "[bob]\tTrain Epoch: 75/200 \tTrain Loss: 0.3887 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 74 \tEval Loss: 0.3928 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 76/200 \tTrain Loss: 0.4191 \n",
      "[bob]\tTrain Epoch: 76/200 \tTrain Loss: 0.3864 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 75 \tEval Loss: 0.3904 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 77/200 \tTrain Loss: 0.4234 \n",
      "[bob]\tTrain Epoch: 77/200 \tTrain Loss: 0.3799 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 76 \tEval Loss: 0.3881 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 78/200 \tTrain Loss: 0.4172 \n",
      "[bob]\tTrain Epoch: 78/200 \tTrain Loss: 0.3808 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 77 \tEval Loss: 0.3858 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 79/200 \tTrain Loss: 0.4122 \n",
      "[bob]\tTrain Epoch: 79/200 \tTrain Loss: 0.3789 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 78 \tEval Loss: 0.3835 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 80/200 \tTrain Loss: 0.4066 \n",
      "[bob]\tTrain Epoch: 80/200 \tTrain Loss: 0.3734 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 79 \tEval Loss: 0.3813 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 81/200 \tTrain Loss: 0.4074 \n",
      "[bob]\tTrain Epoch: 81/200 \tTrain Loss: 0.3718 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 80 \tEval Loss: 0.3790 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 82/200 \tTrain Loss: 0.4064 \n",
      "[bob]\tTrain Epoch: 82/200 \tTrain Loss: 0.3661 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 81 \tEval Loss: 0.3768 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 83/200 \tTrain Loss: 0.4016 \n",
      "[bob]\tTrain Epoch: 83/200 \tTrain Loss: 0.3686 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 82 \tEval Loss: 0.3746 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 84/200 \tTrain Loss: 0.4000 \n",
      "[bob]\tTrain Epoch: 84/200 \tTrain Loss: 0.3696 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 83 \tEval Loss: 0.3725 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 85/200 \tTrain Loss: 0.4009 \n",
      "[bob]\tTrain Epoch: 85/200 \tTrain Loss: 0.3661 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 84 \tEval Loss: 0.3703 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 86/200 \tTrain Loss: 0.3936 \n",
      "[bob]\tTrain Epoch: 86/200 \tTrain Loss: 0.3622 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 85 \tEval Loss: 0.3682 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 87/200 \tTrain Loss: 0.3933 \n",
      "[bob]\tTrain Epoch: 87/200 \tTrain Loss: 0.3573 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 86 \tEval Loss: 0.3660 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 88/200 \tTrain Loss: 0.3898 \n",
      "[bob]\tTrain Epoch: 88/200 \tTrain Loss: 0.3553 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 87 \tEval Loss: 0.3640 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 89/200 \tTrain Loss: 0.3893 \n",
      "[bob]\tTrain Epoch: 89/200 \tTrain Loss: 0.3527 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 88 \tEval Loss: 0.3619 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 90/200 \tTrain Loss: 0.3829 \n",
      "[bob]\tTrain Epoch: 90/200 \tTrain Loss: 0.3561 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 89 \tEval Loss: 0.3598 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 91/200 \tTrain Loss: 0.3798 \n",
      "[bob]\tTrain Epoch: 91/200 \tTrain Loss: 0.3512 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 90 \tEval Loss: 0.3578 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 92/200 \tTrain Loss: 0.3798 \n",
      "[bob]\tTrain Epoch: 92/200 \tTrain Loss: 0.3499 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 91 \tEval Loss: 0.3558 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 93/200 \tTrain Loss: 0.3809 \n",
      "[bob]\tTrain Epoch: 93/200 \tTrain Loss: 0.3480 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 92 \tEval Loss: 0.3538 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 94/200 \tTrain Loss: 0.3724 \n",
      "[bob]\tTrain Epoch: 94/200 \tTrain Loss: 0.3449 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 93 \tEval Loss: 0.3518 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 95/200 \tTrain Loss: 0.3693 \n",
      "[bob]\tTrain Epoch: 95/200 \tTrain Loss: 0.3446 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 94 \tEval Loss: 0.3498 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 96/200 \tTrain Loss: 0.3694 \n",
      "[bob]\tTrain Epoch: 96/200 \tTrain Loss: 0.3433 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 95 \tEval Loss: 0.3479 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 97/200 \tTrain Loss: 0.3671 \n",
      "[bob]\tTrain Epoch: 97/200 \tTrain Loss: 0.3350 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 96 \tEval Loss: 0.3459 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 98/200 \tTrain Loss: 0.3676 \n",
      "[bob]\tTrain Epoch: 98/200 \tTrain Loss: 0.3378 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 97 \tEval Loss: 0.3440 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 99/200 \tTrain Loss: 0.3603 \n",
      "[bob]\tTrain Epoch: 99/200 \tTrain Loss: 0.3355 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 98 \tEval Loss: 0.3421 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 100/200 \tTrain Loss: 0.3601 \n",
      "[bob]\tTrain Epoch: 100/200 \tTrain Loss: 0.3324 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 99 \tEval Loss: 0.3402 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 101/200 \tTrain Loss: 0.3615 \n",
      "[bob]\tTrain Epoch: 101/200 \tTrain Loss: 0.3307 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 100 \tEval Loss: 0.3384 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 102/200 \tTrain Loss: 0.3584 \n",
      "[bob]\tTrain Epoch: 102/200 \tTrain Loss: 0.3293 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 101 \tEval Loss: 0.3365 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 103/200 \tTrain Loss: 0.3522 \n",
      "[bob]\tTrain Epoch: 103/200 \tTrain Loss: 0.3253 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 102 \tEval Loss: 0.3347 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 104/200 \tTrain Loss: 0.3551 \n",
      "[bob]\tTrain Epoch: 104/200 \tTrain Loss: 0.3271 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 103 \tEval Loss: 0.3329 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 105/200 \tTrain Loss: 0.3519 \n",
      "[bob]\tTrain Epoch: 105/200 \tTrain Loss: 0.3238 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 104 \tEval Loss: 0.3311 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 106/200 \tTrain Loss: 0.3538 \n",
      "[bob]\tTrain Epoch: 106/200 \tTrain Loss: 0.3219 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 105 \tEval Loss: 0.3293 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 107/200 \tTrain Loss: 0.3481 \n",
      "[bob]\tTrain Epoch: 107/200 \tTrain Loss: 0.3219 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 106 \tEval Loss: 0.3275 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 108/200 \tTrain Loss: 0.3482 \n",
      "[bob]\tTrain Epoch: 108/200 \tTrain Loss: 0.3194 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 107 \tEval Loss: 0.3257 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 109/200 \tTrain Loss: 0.3450 \n",
      "[bob]\tTrain Epoch: 109/200 \tTrain Loss: 0.3166 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 108 \tEval Loss: 0.3240 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 110/200 \tTrain Loss: 0.3382 \n",
      "[bob]\tTrain Epoch: 110/200 \tTrain Loss: 0.3165 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 109 \tEval Loss: 0.3223 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 111/200 \tTrain Loss: 0.3391 \n",
      "[bob]\tTrain Epoch: 111/200 \tTrain Loss: 0.3117 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 110 \tEval Loss: 0.3206 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 112/200 \tTrain Loss: 0.3394 \n",
      "[bob]\tTrain Epoch: 112/200 \tTrain Loss: 0.3130 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 111 \tEval Loss: 0.3189 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 113/200 \tTrain Loss: 0.3400 \n",
      "[bob]\tTrain Epoch: 113/200 \tTrain Loss: 0.3125 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 112 \tEval Loss: 0.3172 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 114/200 \tTrain Loss: 0.3384 \n",
      "[bob]\tTrain Epoch: 114/200 \tTrain Loss: 0.3106 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 113 \tEval Loss: 0.3155 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 115/200 \tTrain Loss: 0.3333 \n",
      "[bob]\tTrain Epoch: 115/200 \tTrain Loss: 0.3091 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 114 \tEval Loss: 0.3138 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 116/200 \tTrain Loss: 0.3338 \n",
      "[bob]\tTrain Epoch: 116/200 \tTrain Loss: 0.3061 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 115 \tEval Loss: 0.3122 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 117/200 \tTrain Loss: 0.3324 \n",
      "[bob]\tTrain Epoch: 117/200 \tTrain Loss: 0.3020 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 116 \tEval Loss: 0.3105 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 118/200 \tTrain Loss: 0.3298 \n",
      "[bob]\tTrain Epoch: 118/200 \tTrain Loss: 0.3018 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 117 \tEval Loss: 0.3089 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 119/200 \tTrain Loss: 0.3247 \n",
      "[bob]\tTrain Epoch: 119/200 \tTrain Loss: 0.2969 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 118 \tEval Loss: 0.3073 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 120/200 \tTrain Loss: 0.3275 \n",
      "[bob]\tTrain Epoch: 120/200 \tTrain Loss: 0.2977 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 119 \tEval Loss: 0.3057 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 121/200 \tTrain Loss: 0.3251 \n",
      "[bob]\tTrain Epoch: 121/200 \tTrain Loss: 0.2995 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 120 \tEval Loss: 0.3041 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 122/200 \tTrain Loss: 0.3208 \n",
      "[bob]\tTrain Epoch: 122/200 \tTrain Loss: 0.2933 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 121 \tEval Loss: 0.3026 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 123/200 \tTrain Loss: 0.3166 \n",
      "[bob]\tTrain Epoch: 123/200 \tTrain Loss: 0.2936 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 122 \tEval Loss: 0.3010 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 124/200 \tTrain Loss: 0.3182 \n",
      "[bob]\tTrain Epoch: 124/200 \tTrain Loss: 0.2913 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 123 \tEval Loss: 0.2995 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 125/200 \tTrain Loss: 0.3178 \n",
      "[bob]\tTrain Epoch: 125/200 \tTrain Loss: 0.2915 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 124 \tEval Loss: 0.2979 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 126/200 \tTrain Loss: 0.3118 \n",
      "[bob]\tTrain Epoch: 126/200 \tTrain Loss: 0.2885 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 125 \tEval Loss: 0.2964 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 127/200 \tTrain Loss: 0.3094 \n",
      "[bob]\tTrain Epoch: 127/200 \tTrain Loss: 0.2898 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 126 \tEval Loss: 0.2949 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 128/200 \tTrain Loss: 0.3135 \n",
      "[bob]\tTrain Epoch: 128/200 \tTrain Loss: 0.2872 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 127 \tEval Loss: 0.2934 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 129/200 \tTrain Loss: 0.3072 \n",
      "[bob]\tTrain Epoch: 129/200 \tTrain Loss: 0.2852 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 128 \tEval Loss: 0.2920 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 130/200 \tTrain Loss: 0.3049 \n",
      "[bob]\tTrain Epoch: 130/200 \tTrain Loss: 0.2856 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 129 \tEval Loss: 0.2905 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 131/200 \tTrain Loss: 0.3010 \n",
      "[bob]\tTrain Epoch: 131/200 \tTrain Loss: 0.2843 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 130 \tEval Loss: 0.2890 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 132/200 \tTrain Loss: 0.3065 \n",
      "[bob]\tTrain Epoch: 132/200 \tTrain Loss: 0.2801 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 131 \tEval Loss: 0.2876 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 133/200 \tTrain Loss: 0.3034 \n",
      "[bob]\tTrain Epoch: 133/200 \tTrain Loss: 0.2783 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 132 \tEval Loss: 0.2861 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 134/200 \tTrain Loss: 0.3059 \n",
      "[bob]\tTrain Epoch: 134/200 \tTrain Loss: 0.2784 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 133 \tEval Loss: 0.2847 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 135/200 \tTrain Loss: 0.2999 \n",
      "[bob]\tTrain Epoch: 135/200 \tTrain Loss: 0.2765 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 134 \tEval Loss: 0.2833 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 136/200 \tTrain Loss: 0.2956 \n",
      "[bob]\tTrain Epoch: 136/200 \tTrain Loss: 0.2750 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 135 \tEval Loss: 0.2819 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 137/200 \tTrain Loss: 0.2950 \n",
      "[bob]\tTrain Epoch: 137/200 \tTrain Loss: 0.2750 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 136 \tEval Loss: 0.2805 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 138/200 \tTrain Loss: 0.2939 \n",
      "[bob]\tTrain Epoch: 138/200 \tTrain Loss: 0.2715 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 137 \tEval Loss: 0.2791 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 139/200 \tTrain Loss: 0.2882 \n",
      "[bob]\tTrain Epoch: 139/200 \tTrain Loss: 0.2708 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 138 \tEval Loss: 0.2778 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 140/200 \tTrain Loss: 0.2912 \n",
      "[bob]\tTrain Epoch: 140/200 \tTrain Loss: 0.2701 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 139 \tEval Loss: 0.2764 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 141/200 \tTrain Loss: 0.2897 \n",
      "[bob]\tTrain Epoch: 141/200 \tTrain Loss: 0.2688 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 140 \tEval Loss: 0.2751 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 142/200 \tTrain Loss: 0.2835 \n",
      "[bob]\tTrain Epoch: 142/200 \tTrain Loss: 0.2645 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 141 \tEval Loss: 0.2737 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 143/200 \tTrain Loss: 0.2843 \n",
      "[bob]\tTrain Epoch: 143/200 \tTrain Loss: 0.2667 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 142 \tEval Loss: 0.2724 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 144/200 \tTrain Loss: 0.2835 \n",
      "[bob]\tTrain Epoch: 144/200 \tTrain Loss: 0.2646 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 143 \tEval Loss: 0.2711 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 145/200 \tTrain Loss: 0.2839 \n",
      "[bob]\tTrain Epoch: 145/200 \tTrain Loss: 0.2634 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 144 \tEval Loss: 0.2698 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 146/200 \tTrain Loss: 0.2817 \n",
      "[bob]\tTrain Epoch: 146/200 \tTrain Loss: 0.2630 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 145 \tEval Loss: 0.2685 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 147/200 \tTrain Loss: 0.2804 \n",
      "[bob]\tTrain Epoch: 147/200 \tTrain Loss: 0.2618 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 146 \tEval Loss: 0.2672 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 148/200 \tTrain Loss: 0.2798 \n",
      "[bob]\tTrain Epoch: 148/200 \tTrain Loss: 0.2581 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 147 \tEval Loss: 0.2659 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 149/200 \tTrain Loss: 0.2783 \n",
      "[bob]\tTrain Epoch: 149/200 \tTrain Loss: 0.2592 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 148 \tEval Loss: 0.2647 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 150/200 \tTrain Loss: 0.2784 \n",
      "[bob]\tTrain Epoch: 150/200 \tTrain Loss: 0.2575 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 149 \tEval Loss: 0.2634 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 151/200 \tTrain Loss: 0.2698 \n",
      "[bob]\tTrain Epoch: 151/200 \tTrain Loss: 0.2569 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 150 \tEval Loss: 0.2622 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 152/200 \tTrain Loss: 0.2721 \n",
      "[bob]\tTrain Epoch: 152/200 \tTrain Loss: 0.2545 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 151 \tEval Loss: 0.2609 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 153/200 \tTrain Loss: 0.2698 \n",
      "[bob]\tTrain Epoch: 153/200 \tTrain Loss: 0.2527 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 152 \tEval Loss: 0.2597 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 154/200 \tTrain Loss: 0.2709 \n",
      "[bob]\tTrain Epoch: 154/200 \tTrain Loss: 0.2551 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 153 \tEval Loss: 0.2585 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 155/200 \tTrain Loss: 0.2668 \n",
      "[bob]\tTrain Epoch: 155/200 \tTrain Loss: 0.2500 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 154 \tEval Loss: 0.2572 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 156/200 \tTrain Loss: 0.2690 \n",
      "[bob]\tTrain Epoch: 156/200 \tTrain Loss: 0.2505 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 155 \tEval Loss: 0.2560 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 157/200 \tTrain Loss: 0.2678 \n",
      "[bob]\tTrain Epoch: 157/200 \tTrain Loss: 0.2490 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 156 \tEval Loss: 0.2549 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 158/200 \tTrain Loss: 0.2669 \n",
      "[bob]\tTrain Epoch: 158/200 \tTrain Loss: 0.2473 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 157 \tEval Loss: 0.2537 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 159/200 \tTrain Loss: 0.2661 \n",
      "[bob]\tTrain Epoch: 159/200 \tTrain Loss: 0.2462 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 158 \tEval Loss: 0.2525 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 160/200 \tTrain Loss: 0.2634 \n",
      "[bob]\tTrain Epoch: 160/200 \tTrain Loss: 0.2452 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 159 \tEval Loss: 0.2513 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 161/200 \tTrain Loss: 0.2612 \n",
      "[bob]\tTrain Epoch: 161/200 \tTrain Loss: 0.2478 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 160 \tEval Loss: 0.2502 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 162/200 \tTrain Loss: 0.2598 \n",
      "[bob]\tTrain Epoch: 162/200 \tTrain Loss: 0.2481 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 161 \tEval Loss: 0.2490 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 163/200 \tTrain Loss: 0.2610 \n",
      "[bob]\tTrain Epoch: 163/200 \tTrain Loss: 0.2413 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 162 \tEval Loss: 0.2479 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 164/200 \tTrain Loss: 0.2531 \n",
      "[bob]\tTrain Epoch: 164/200 \tTrain Loss: 0.2395 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 163 \tEval Loss: 0.2467 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 165/200 \tTrain Loss: 0.2563 \n",
      "[bob]\tTrain Epoch: 165/200 \tTrain Loss: 0.2432 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 164 \tEval Loss: 0.2456 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 166/200 \tTrain Loss: 0.2520 \n",
      "[bob]\tTrain Epoch: 166/200 \tTrain Loss: 0.2373 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 165 \tEval Loss: 0.2445 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 167/200 \tTrain Loss: 0.2544 \n",
      "[bob]\tTrain Epoch: 167/200 \tTrain Loss: 0.2380 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 166 \tEval Loss: 0.2434 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 168/200 \tTrain Loss: 0.2503 \n",
      "[bob]\tTrain Epoch: 168/200 \tTrain Loss: 0.2379 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 167 \tEval Loss: 0.2423 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 169/200 \tTrain Loss: 0.2464 \n",
      "[bob]\tTrain Epoch: 169/200 \tTrain Loss: 0.2342 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 168 \tEval Loss: 0.2412 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 170/200 \tTrain Loss: 0.2485 \n",
      "[bob]\tTrain Epoch: 170/200 \tTrain Loss: 0.2351 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 169 \tEval Loss: 0.2401 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 171/200 \tTrain Loss: 0.2462 \n",
      "[bob]\tTrain Epoch: 171/200 \tTrain Loss: 0.2340 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 170 \tEval Loss: 0.2390 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 172/200 \tTrain Loss: 0.2450 \n",
      "[bob]\tTrain Epoch: 172/200 \tTrain Loss: 0.2331 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 171 \tEval Loss: 0.2380 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 173/200 \tTrain Loss: 0.2472 \n",
      "[bob]\tTrain Epoch: 173/200 \tTrain Loss: 0.2287 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 172 \tEval Loss: 0.2369 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 174/200 \tTrain Loss: 0.2449 \n",
      "[bob]\tTrain Epoch: 174/200 \tTrain Loss: 0.2316 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 173 \tEval Loss: 0.2358 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 175/200 \tTrain Loss: 0.2429 \n",
      "[bob]\tTrain Epoch: 175/200 \tTrain Loss: 0.2288 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 174 \tEval Loss: 0.2348 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 176/200 \tTrain Loss: 0.2408 \n",
      "[bob]\tTrain Epoch: 176/200 \tTrain Loss: 0.2304 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 175 \tEval Loss: 0.2337 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 177/200 \tTrain Loss: 0.2415 \n",
      "[bob]\tTrain Epoch: 177/200 \tTrain Loss: 0.2271 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 176 \tEval Loss: 0.2327 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 178/200 \tTrain Loss: 0.2418 \n",
      "[bob]\tTrain Epoch: 178/200 \tTrain Loss: 0.2274 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 177 \tEval Loss: 0.2317 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 179/200 \tTrain Loss: 0.2382 \n",
      "[bob]\tTrain Epoch: 179/200 \tTrain Loss: 0.2235 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 178 \tEval Loss: 0.2307 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 180/200 \tTrain Loss: 0.2378 \n",
      "[bob]\tTrain Epoch: 180/200 \tTrain Loss: 0.2234 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 179 \tEval Loss: 0.2296 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 181/200 \tTrain Loss: 0.2383 \n",
      "[bob]\tTrain Epoch: 181/200 \tTrain Loss: 0.2261 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 180 \tEval Loss: 0.2286 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 182/200 \tTrain Loss: 0.2332 \n",
      "[bob]\tTrain Epoch: 182/200 \tTrain Loss: 0.2234 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 181 \tEval Loss: 0.2276 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 183/200 \tTrain Loss: 0.2324 \n",
      "[bob]\tTrain Epoch: 183/200 \tTrain Loss: 0.2244 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 182 \tEval Loss: 0.2266 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 184/200 \tTrain Loss: 0.2325 \n",
      "[bob]\tTrain Epoch: 184/200 \tTrain Loss: 0.2180 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 183 \tEval Loss: 0.2257 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 185/200 \tTrain Loss: 0.2289 \n",
      "[bob]\tTrain Epoch: 185/200 \tTrain Loss: 0.2195 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 184 \tEval Loss: 0.2247 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 186/200 \tTrain Loss: 0.2339 \n",
      "[bob]\tTrain Epoch: 186/200 \tTrain Loss: 0.2173 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 185 \tEval Loss: 0.2237 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 187/200 \tTrain Loss: 0.2333 \n",
      "[bob]\tTrain Epoch: 187/200 \tTrain Loss: 0.2168 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 186 \tEval Loss: 0.2228 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 188/200 \tTrain Loss: 0.2276 \n",
      "[bob]\tTrain Epoch: 188/200 \tTrain Loss: 0.2170 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 187 \tEval Loss: 0.2218 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 189/200 \tTrain Loss: 0.2257 \n",
      "[bob]\tTrain Epoch: 189/200 \tTrain Loss: 0.2162 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 188 \tEval Loss: 0.2208 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 190/200 \tTrain Loss: 0.2229 \n",
      "[bob]\tTrain Epoch: 190/200 \tTrain Loss: 0.2156 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 189 \tEval Loss: 0.2199 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 191/200 \tTrain Loss: 0.2242 \n",
      "[bob]\tTrain Epoch: 191/200 \tTrain Loss: 0.2157 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 190 \tEval Loss: 0.2190 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 192/200 \tTrain Loss: 0.2208 \n",
      "[bob]\tTrain Epoch: 192/200 \tTrain Loss: 0.2125 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 191 \tEval Loss: 0.2180 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 193/200 \tTrain Loss: 0.2223 \n",
      "[bob]\tTrain Epoch: 193/200 \tTrain Loss: 0.2117 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 192 \tEval Loss: 0.2171 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 194/200 \tTrain Loss: 0.2226 \n",
      "[bob]\tTrain Epoch: 194/200 \tTrain Loss: 0.2085 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 193 \tEval Loss: 0.2162 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 195/200 \tTrain Loss: 0.2169 \n",
      "[bob]\tTrain Epoch: 195/200 \tTrain Loss: 0.2108 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 194 \tEval Loss: 0.2153 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 196/200 \tTrain Loss: 0.2232 \n",
      "[bob]\tTrain Epoch: 196/200 \tTrain Loss: 0.2104 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 195 \tEval Loss: 0.2144 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 197/200 \tTrain Loss: 0.2191 \n",
      "[bob]\tTrain Epoch: 197/200 \tTrain Loss: 0.2090 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 196 \tEval Loss: 0.2135 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 198/200 \tTrain Loss: 0.2173 \n",
      "[bob]\tTrain Epoch: 198/200 \tTrain Loss: 0.2071 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 197 \tEval Loss: 0.2126 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 199/200 \tTrain Loss: 0.2165 \n",
      "[bob]\tTrain Epoch: 199/200 \tTrain Loss: 0.2061 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 198 \tEval Loss: 0.2117 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 200/200 \tTrain Loss: 0.2153 \n",
      "[bob]\tTrain Epoch: 200/200 \tTrain Loss: 0.2062 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 199 \tEval Loss: 0.2108 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Early Stopping\n",
    "last_loss = 100\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    last_loss, trigger_times = eval(epoch, last_loss, trigger_times, patience)\n",
    "    if trigger_times >= patience:\n",
    "        print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anne Losses: [0.7079751491546631, 0.7035773992538452, 0.6983179450035095, 0.6907765865325928, 0.6838275790214539, 0.6804913878440857, 0.6752387881278992, 0.6717817783355713, 0.6644486784934998, 0.6619433164596558, 0.66266930103302, 0.6513778567314148, 0.6476473808288574, 0.6447980403900146, 0.6415961384773254, 0.6268424391746521, 0.6291244029998779, 0.624331533908844, 0.6171491742134094, 0.6129047274589539, 0.6165077090263367, 0.6074989438056946, 0.6059665083885193, 0.5950222015380859, 0.5914958119392395, 0.5884127616882324, 0.5876101851463318, 0.5785477757453918, 0.5718375444412231, 0.5732410550117493, 0.5691128373146057, 0.5572907328605652, 0.5611247420310974, 0.5669932961463928, 0.546832799911499, 0.5516247153282166, 0.5445107221603394, 0.5434781908988953, 0.5362805724143982, 0.5359236598014832, 0.5297248959541321, 0.5250058174133301, 0.5207681059837341, 0.5171814560890198, 0.513131320476532, 0.515540599822998, 0.5093647837638855, 0.5057358741760254, 0.4991568922996521, 0.49537837505340576, 0.49179279804229736, 0.49247774481773376, 0.4860829710960388, 0.47912362217903137, 0.48259881138801575, 0.47624319791793823, 0.4703826308250427, 0.4690897464752197, 0.46747052669525146, 0.4664526581764221, 0.46117281913757324, 0.4552924633026123, 0.45874539017677307, 0.4545220136642456, 0.4506765604019165, 0.4491047263145447, 0.4480721950531006, 0.4397505521774292, 0.43559935688972473, 0.43588733673095703, 0.42972543835639954, 0.4279150366783142, 0.43206915259361267, 0.4213501513004303, 0.4208068251609802, 0.4191429615020752, 0.42342686653137207, 0.4172365069389343, 0.4121578335762024, 0.4065893888473511, 0.4073995053768158, 0.4064278304576874, 0.401579886674881, 0.39998120069503784, 0.4008653461933136, 0.3936178386211395, 0.3933229148387909, 0.3898068964481354, 0.3892761766910553, 0.3828524053096771, 0.3797609508037567, 0.379751056432724, 0.38091611862182617, 0.37244269251823425, 0.3693113625049591, 0.36938998103141785, 0.36708366870880127, 0.36759501695632935, 0.3602883517742157, 0.3601173162460327, 0.3614942729473114, 0.35842815041542053, 0.3521858751773834, 0.35505667328834534, 0.3519432544708252, 0.3537687361240387, 0.34814754128456116, 0.3482358157634735, 0.34498435258865356, 0.33818358182907104, 0.3390792906284332, 0.3394354581832886, 0.34003376960754395, 0.3383735120296478, 0.3332655727863312, 0.3337627947330475, 0.332380473613739, 0.32978078722953796, 0.3246614933013916, 0.3274986743927002, 0.32510828971862793, 0.3207915723323822, 0.31657907366752625, 0.31824734807014465, 0.317791223526001, 0.31179189682006836, 0.3094273507595062, 0.3134574592113495, 0.30721354484558105, 0.30490943789482117, 0.3009819984436035, 0.306458979845047, 0.3034341633319855, 0.30589449405670166, 0.29985734820365906, 0.2956179678440094, 0.29495155811309814, 0.2938982844352722, 0.28820279240608215, 0.29118919372558594, 0.28973427414894104, 0.2835218012332916, 0.2843363285064697, 0.2835428714752197, 0.2838718891143799, 0.281701922416687, 0.28044334053993225, 0.279771089553833, 0.2783334255218506, 0.27835774421691895, 0.2697978615760803, 0.2720704972743988, 0.26978111267089844, 0.2708698809146881, 0.2667718231678009, 0.26904118061065674, 0.2677931487560272, 0.26686331629753113, 0.2661103904247284, 0.2633962333202362, 0.261233389377594, 0.2597949504852295, 0.2609999179840088, 0.25314757227897644, 0.2562551200389862, 0.25203678011894226, 0.25440821051597595, 0.25026100873947144, 0.24639996886253357, 0.24847204983234406, 0.24622569978237152, 0.24499091506004333, 0.2471538484096527, 0.2449309080839157, 0.24292173981666565, 0.24082046747207642, 0.241453617811203, 0.2417897880077362, 0.23823696374893188, 0.23779594898223877, 0.2383410781621933, 0.23322665691375732, 0.2324303388595581, 0.2324707806110382, 0.22887714207172394, 0.23393629491329193, 0.2333088368177414, 0.22756437957286835, 0.22567813098430634, 0.22292006015777588, 0.22416895627975464, 0.22082728147506714, 0.22231073677539825, 0.22264190018177032, 0.21691519021987915, 0.22324448823928833, 0.21906030178070068, 0.21731968224048615, 0.2164962738752365, 0.215307354927063]\n",
      "\n",
      "Bob Losses: [0.6656239032745361, 0.6701837182044983, 0.664956271648407, 0.6556107401847839, 0.6521942615509033, 0.6450701355934143, 0.6342790126800537, 0.6375420689582825, 0.6242350339889526, 0.6233963966369629, 0.6145976185798645, 0.614019513130188, 0.6056118607521057, 0.6003404855728149, 0.5959323644638062, 0.596286416053772, 0.5802664160728455, 0.5830347537994385, 0.5726121068000793, 0.5696638822555542, 0.5701853632926941, 0.5629600286483765, 0.5580402612686157, 0.5558911561965942, 0.5524314045906067, 0.5455448627471924, 0.5415759682655334, 0.537186861038208, 0.5276229977607727, 0.5243796110153198, 0.5283219218254089, 0.5165836811065674, 0.5119316577911377, 0.5151285529136658, 0.5103927850723267, 0.5016555190086365, 0.49800318479537964, 0.4947460889816284, 0.4931373596191406, 0.4923335909843445, 0.4868999421596527, 0.4795939028263092, 0.4788559675216675, 0.4783652722835541, 0.4716971218585968, 0.4745887219905853, 0.46556395292282104, 0.465770959854126, 0.4587422311306, 0.4564601480960846, 0.454071044921875, 0.44953054189682007, 0.44822657108306885, 0.4443153738975525, 0.44355225563049316, 0.43880414962768555, 0.43618345260620117, 0.4318331778049469, 0.43045324087142944, 0.4243730306625366, 0.42371463775634766, 0.4222826659679413, 0.41501933336257935, 0.41875120997428894, 0.4141952097415924, 0.4112306833267212, 0.40472307801246643, 0.4024136960506439, 0.4043661952018738, 0.3999543786048889, 0.3955356776714325, 0.3949606418609619, 0.39335736632347107, 0.3890880048274994, 0.38874006271362305, 0.3864288032054901, 0.3799329102039337, 0.380765825510025, 0.37890374660491943, 0.37338900566101074, 0.3718491196632385, 0.3661344051361084, 0.3686440885066986, 0.36955127120018005, 0.3661307990550995, 0.3622167706489563, 0.35727426409721375, 0.35527628660202026, 0.3526538908481598, 0.35611557960510254, 0.3511549234390259, 0.34988951683044434, 0.34803369641304016, 0.34490901231765747, 0.3446381092071533, 0.34328821301460266, 0.33504319190979004, 0.3377912938594818, 0.33554181456565857, 0.33237048983573914, 0.3307356536388397, 0.3293275535106659, 0.3252864181995392, 0.3270723223686218, 0.3237941265106201, 0.3219400942325592, 0.3218969702720642, 0.3194029927253723, 0.31662362813949585, 0.31654098629951477, 0.3117152750492096, 0.312973290681839, 0.3124846816062927, 0.3105809986591339, 0.3091277480125427, 0.3061494827270508, 0.3019648790359497, 0.30182114243507385, 0.29686054587364197, 0.2977093458175659, 0.2995143532752991, 0.29327693581581116, 0.29362189769744873, 0.29126790165901184, 0.29148319363594055, 0.2884761393070221, 0.2898053824901581, 0.2871706485748291, 0.2851889729499817, 0.285569429397583, 0.2843247652053833, 0.28005871176719666, 0.27826574444770813, 0.2784292697906494, 0.2765112519264221, 0.27502402663230896, 0.2749764323234558, 0.2715466022491455, 0.2707521319389343, 0.2700839936733246, 0.2688167095184326, 0.2645462155342102, 0.2667289972305298, 0.2645740509033203, 0.26338163018226624, 0.2630048394203186, 0.26175785064697266, 0.2580939829349518, 0.2591545283794403, 0.2574860155582428, 0.2568679749965668, 0.2545020580291748, 0.2526731491088867, 0.25510889291763306, 0.25004643201828003, 0.25051236152648926, 0.24898923933506012, 0.24734090268611908, 0.2461780607700348, 0.24515482783317566, 0.24775873124599457, 0.24807584285736084, 0.24133659899234772, 0.2395143210887909, 0.2431749403476715, 0.23731696605682373, 0.2380235344171524, 0.23791465163230896, 0.2342340499162674, 0.2350672483444214, 0.2340209037065506, 0.23306474089622498, 0.22870080173015594, 0.23162713646888733, 0.22882258892059326, 0.2304219901561737, 0.22714360058307648, 0.22736120223999023, 0.2234838306903839, 0.2234252542257309, 0.22611084580421448, 0.22338581085205078, 0.22444486618041992, 0.21797984838485718, 0.21953324973583221, 0.21730336546897888, 0.2168477475643158, 0.21697944402694702, 0.21617087721824646, 0.21557343006134033, 0.21566282212734222, 0.21249468624591827, 0.2117287814617157, 0.20845352113246918, 0.21077163517475128, 0.2104082554578781, 0.20904482901096344, 0.20713889598846436, 0.20608016848564148, 0.206191748380661]\n",
      "\n",
      "Test Losses: [0.6534323692321777, 0.6482540369033813, 0.6431434750556946, 0.6381059885025024, 0.6330956220626831, 0.628204345703125, 0.6233593821525574, 0.6185263395309448, 0.6138044595718384, 0.609123945236206, 0.6044918298721313, 0.5999085903167725, 0.5953788757324219, 0.5908908843994141, 0.5864760279655457, 0.5821148157119751, 0.5777978301048279, 0.5735121965408325, 0.569321870803833, 0.5651663541793823, 0.5610150694847107, 0.5569071769714355, 0.5528742074966431, 0.5488908290863037, 0.5449486970901489, 0.5410637259483337, 0.5371915102005005, 0.5333688259124756, 0.5296326279640198, 0.5259124636650085, 0.5222183465957642, 0.5185804963111877, 0.5149586796760559, 0.5113493800163269, 0.5078292489051819, 0.5043298602104187, 0.5008890628814697, 0.49746832251548767, 0.4941011667251587, 0.49073320627212524, 0.48741352558135986, 0.4841354787349701, 0.48088788986206055, 0.4776856005191803, 0.4745084047317505, 0.4713694751262665, 0.4682545065879822, 0.46517214179039, 0.4621301293373108, 0.4591140151023865, 0.4561458230018616, 0.4531914293766022, 0.45027175545692444, 0.4473994970321655, 0.44452545046806335, 0.44169870018959045, 0.4388863146305084, 0.43611443042755127, 0.4333593249320984, 0.4306323528289795, 0.42791855335235596, 0.42525917291641235, 0.4226095378398895, 0.4199737012386322, 0.41737985610961914, 0.4148114025592804, 0.4122724235057831, 0.40976378321647644, 0.40725889801979065, 0.4047831594944, 0.40234559774398804, 0.3999103307723999, 0.3974993824958801, 0.39512404799461365, 0.3927684724330902, 0.39042818546295166, 0.3881033658981323, 0.38579443097114563, 0.38351285457611084, 0.3812655210494995, 0.3790339529514313, 0.3768286406993866, 0.3746360242366791, 0.37245798110961914, 0.3702961206436157, 0.36816325783729553, 0.36604398488998413, 0.3639599680900574, 0.36188411712646484, 0.35982778668403625, 0.3577916622161865, 0.3557646870613098, 0.3537565767765045, 0.3517829179763794, 0.3498266935348511, 0.3478658199310303, 0.3459380865097046, 0.34402263164520264, 0.3421236276626587, 0.34023502469062805, 0.33836907148361206, 0.33651626110076904, 0.33468592166900635, 0.33286792039871216, 0.331068217754364, 0.32928502559661865, 0.3275142014026642, 0.32574498653411865, 0.32400235533714294, 0.3222759962081909, 0.32055962085723877, 0.31885990500450134, 0.31715717911720276, 0.3154747486114502, 0.31380894780158997, 0.3121623694896698, 0.31053367257118225, 0.3089117705821991, 0.30731531977653503, 0.3057311773300171, 0.30414292216300964, 0.3025871515274048, 0.30102503299713135, 0.2994738817214966, 0.2979462742805481, 0.2964347302913666, 0.29493051767349243, 0.2934320569038391, 0.2919562757015228, 0.2904880940914154, 0.28902968764305115, 0.28758805990219116, 0.2861497402191162, 0.2847225069999695, 0.2833113670349121, 0.28191548585891724, 0.2805235683917999, 0.2791389226913452, 0.2777738869190216, 0.2764192223548889, 0.2750680148601532, 0.2737468183040619, 0.2724195122718811, 0.27110353112220764, 0.26979944109916687, 0.2684922218322754, 0.2672058343887329, 0.26592713594436646, 0.26466381549835205, 0.2633972465991974, 0.2621549665927887, 0.2609109580516815, 0.2596819996833801, 0.2584577798843384, 0.257249116897583, 0.25604429841041565, 0.25485366582870483, 0.25366881489753723, 0.2524893581867218, 0.2513170838356018, 0.25015711784362793, 0.24900293350219727, 0.24785542488098145, 0.2467285543680191, 0.24560123682022095, 0.2444901168346405, 0.24337784945964813, 0.24227513372898102, 0.2411874681711197, 0.24009643495082855, 0.23901870846748352, 0.2379520684480667, 0.23689134418964386, 0.23582670092582703, 0.23477458953857422, 0.23373249173164368, 0.2326948642730713, 0.23166748881340027, 0.2306557446718216, 0.2296449840068817, 0.2286367416381836, 0.22763901948928833, 0.2266475409269333, 0.22566920518875122, 0.22469455003738403, 0.22372160851955414, 0.22275479137897491, 0.22179660201072693, 0.22084945440292358, 0.21991106867790222, 0.21897254884243011, 0.21804426610469818, 0.21712245047092438, 0.21620342135429382, 0.21528811752796173, 0.21437925100326538, 0.21347683668136597, 0.2125815600156784, 0.21169202029705048, 0.2108101099729538]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Anne Losses: {train_losses[0]}\\n\")\n",
    "print(f\"Bob Losses: {train_losses[1]}\\n\")\n",
    "print(f\"Test Losses: {test_losses}\\n\")\n",
    "plt.plot(train_losses[0], 'r')\n",
    "plt.plot(train_losses[1], 'g')\n",
    "plt.plot(test_losses, 'b')\n",
    "plt.legend(['Training Loss Anne', 'Training Loss Bob' , 'Eval Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "PATH = \"local_state_dict_model.pt\"\n",
    "torch.save(local_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test_sms_540.csv', sep=',', encoding= 'unicode_escape', names=['Teks', 'Label'])\n",
    "# data = data.sample(frac = 1)\n",
    "# Lowercase, remove unnecessary char with regex, remove stop words\n",
    "data.Teks = data.Teks.apply(clean_text)\n",
    "#     print(data.Teks)\n",
    "words = set((' '.join(data.Teks)).split())\n",
    "#     print(words)\n",
    "word_to_idx = {word: i for i, word in enumerate(words, start=1)}\n",
    "#     pprint(word_to_idx)\n",
    "tokens = data.Teks.apply(lambda x: tokenize(x, word_to_idx))\n",
    "#     print(tokens)\n",
    "inputs = pad_and_truncate(tokens)\n",
    "#     pprint(inputs)\n",
    "labels = np.array((data.Label == '1').astype(int))\n",
    "\n",
    "np.save('test_labels.npy', labels)\n",
    "np.save('test_inputs.npy', inputs)\n",
    "\n",
    "test_inputs = torch.tensor(np.load('test_inputs.npy'))\n",
    "test_labels = torch.tensor(np.load('test_labels.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing params\n",
    "# VOCAB_SIZE = int(test_inputs.max()) + 1\n",
    "TEST_VOCAB_SIZE = TRAIN_VOCAB_SIZE \n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Test First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.189229, G-Mean=0.540\n",
      "Amount of test data: 540\n",
      "ROC Accuracy Score: 0.5455197025578236\n",
      "\n",
      "Accuracy Score: 0.5314814814814814\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAHhCAYAAAAFwEUqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAphElEQVR4nO3dd5gV1f3H8ffXQlWQDlbUYE8EJaJRo4klsdefJTbsoiZqNJaoUWM0McZEDRpLrIklxqghxti7IhZULLEQRVHpVRGknd8fM+Dusgd2YZdLeb+e5z5375kzM99ZuPu5M3NmbqSUkCRJc1qm0gVIkrSoMiQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkpfkUEd0j4rGIGBcRKSLOb6T19C6Xv21jLH9JUv6ebq50HVpyGJJa7EREi4g4OSKeiYixETEtIkZExANloCy3EGpYDvgH0A04FzgEuKex11spEdG1DKAUEfdn+iwfEaPKPkMWYF17NtYHDqm+wpsJaHESEd8A/g2sAzwKPAyMBjoC25ePS1NKpzdyHesA7wKnppR+38jrWhZYHpiaUprZmOuaSw1dgQ+BKWUtq6WUhtXosw9wd9lnREqp63yu62bgsJRSzMe8zYAZKaVp87NuqaZG/8QtNZSIaA7cD6wF7JNSqrnndklEfBv49kIop3P5PLaxV5RSmgHMaOz11NH9wJ4Ue86/rTHtCGAQsCywwsIqqPx/MS2lND2lNGVhrVdLBw+3anFyFLAucFktAQlASumllNLVVdvKw3fPRcSkiPii/HmPmvNGxJCIeDIi1ouIf0fE5xExISLujojOVfo9CTxVvrypymHIrnM7f1gue0iNtu9ExH8iYnhETImIT8vDxptX6VPrMiOifURcFRFDI2Jq+XxVRLSr0W/W/N+PiNMi4n8R8VVEvBcRh9X2e5yLEcADwOE11tEF+AFwU20zRcRmEXFzuc4vy9/tcxGxV83fEXBY+XOq8uhdtt1cvu4QETdGxAhgErBqlXlurrK848u2c2usZ+Xy0PB/I6JlPX8HWoq4J6nFyb7l83V1nSEijgeuAt4Bflk29wbui4hjU0o1l7UK8CRwL/AzYGPgWKAVsGPZ5yLgOeDnZS3PlO2j6r4pEBHrAo8Aw4ErKAKoE7BVud4X5jJva+B54BvAjcBAoAfQB/h+RGyWUvq8xmwXA82Ba4Gvyr43R8TglNJz9Sj9Rorf3xYppf5l22EUe7t/pfgwU9NewHrAXcBHQLtynnsi4qCU0u1lv4soPrxvTbG3OsvzNZY36/d2IdAS+KK2QlNKV0fEdsB5EfFESunZiFgGuA1YEdg+pTSp7puupU5KyYePxeIBjAEm1KN/G4o/noOBVlXaWwH/Az4HVqrSPgRIwH41lnNV2b5ulbZty7beNfr2Ltu3raWeJ4EhVV7/pOy72Ty2Y45lUoRJAo6v0feEsv3CWuZ/FWhSpX0VirC8ow6/y67lMvpSfLgeDlxXZfq7wN3lz29W3c6yrWUty2xRzvd2jfabiz9NtdZxc1nHXzPTE3BzLf8PhgAflz+fW/Y7sdL/p30s+g8Pt2px0ooi2OpqB4q9jCtTShNnNZY/X0lx3mz7GvN8llK6q0bb4+Vzt/qVO08Tyuc9ygEn9bEXxZ5rzT3ha8v2veaYA65OKU2d9SKl9CnwHvXcrpTSdOAvwP4R0TwitqQYSHXjXOaZvbdWjk5uRxGSjwPrR0Sr+tQA/K4e9Y4DfgR0Af4DnAf0Syn1rec6tRQyJLU4mUhxiKyu1iyf36pl2qy2tWq0f1BL3zHlc7tapi2IOylG6P4cGBsRj0fEGRGxRh3mXRN4twys2crX7zHndkF+2+Znu26i+NCyD8WAnc+Ah3KdI6JjRFxX5RziaIowP67sslI91/9efTqnlJ4HLgF6les9op7r01LKkNTi5E2gVUTUFgANZW6jSOtyScLcrqmqNgYgpfRVSmkHij/cvy7X/UvgnZoDWhpIbtvqfalFSultYADF4d39gFtTMQp3zoVHBMWlOocBtwD7Az+k2NOfdS6yXn+LUkpf1qd/RDShGFgE0BZYvT7za+llSGpx8o/yubaBIbWZtee0YS3TNqjRp6HMuiSkbS3T1qyljZTSiymlC8vA/AbFntav5rGeD4B1a944oXy9Dg2/XbW5Edic4rB19lAr8C2KgUi/SSmdnlK6K6X0UErpUYrLRWpqjIu3fw30BE6nOCJxp6NaVReGpBYnf6YY6HFabZdwAETEpuWIVihGQE4CfhwRK1bpsyLwY4pBPY80cI2zDgNWO9cZEQcCK9doa1/L/J9QHA6sLWSrug/owJwfGI4u2++tW7kL5E7gAuCklNL7c+k3aw+z2h5rRGxE7edOvyinz+t3UCcRsRNwCnBLSulSistX1qEYhCTNlZeAaLGRUvoyInaluOPOfRHxMEXIjaEIhu9RHFL7bdl/fEScTjE6dUCV6+d6U+yxHZtSmkADSim9GxGPAseWhxlfA7pThMFgirvVzHJOROxIcYH+hxQhshvFpRI1L9Sv6bfA/wFXRcQmFCNXewBHUnyQmNf8C6wcAHV+Hbr+l+Ic8OkRMWtE6zoUl9a8AWxao/8LwInA1RHxb2AaMCCl9GF9ayyv37wFeL9cJiml+yPiCuCkiHgopXRnfZerpYchqcVKSmlwRPSg+AO7D3A2xeG+scDLFOe9bq/S/+qIGEZxzeN5ZfPrwF4ppfsaqcxDgD8CB5U/P0MR4H+iuJRilvsoRlzuR3F95GSKP+ZHAzfMbQUppQnlqNILgN0p9o5GANcA56U5r5GsmJTSjIjYhWJE6mEUI47fLH/emDlD8g6KwD+A4oPAMhTbV6+QLK+H/AvlNa4pparXUp4OfBe4NiLmK4C1dPDerZIkZXhOUpKkDENSkqQMQ1KSpAxDUpKkDENSkqQMQ1LzLSJ+GBHvRsTgiDiz0vVIi6ryuy9HRsSbla5F9WNIar5ExLIUF+nvRHGLtwMjYoO5zyUttW6muF+tFjOGpObXZsDglNIH5dcv3QnUeqs4aWmXUnqar+/rq8WIIan5tQowtMrrT8o2SVpiGJKSJGUYkppfnwKrVXm9atkmSUsMQ1Lz6yWgW0SsWX6h7QFAvwrXJEkNypDUfEkpTaf46qGHKL4K6a6U0luVrUpaNEXEHUB/ii/K/iQijqx0TaobvwVEkqQM9yQlScowJCVJyjAkJUnKMCQlScowJCVJyjAktcAi4phK1yAtDnyvLH4MSTUE3/hS3fheWcwYkpIkZSxWNxNovVKb1LHzypUuQzVMGD+O1iu1qXQZqimWrXQFqmHC+LG0XqltpctQDYPffXtimjm9dW3TllvYxSyIjp1X5orr7qx0GdLioXmrSlcgLRZ22XKDkblpHm6VJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKWO5ShegRcPkL7/kH3fexPvvvMV777zFxAnj2P+Qozn0qB9X6/f7X5/DYw/2yy7nkKNO5IBDjpn9euTwz7jl+isZ+FJ/Jk+exKqrdWWP/zuEHXbao9G2RWpsk7+cxD/+ei3vvz2I994exMTxY9n/8BM5tM9ptfYfM2oEt11/OS8//wQTxo2ldZu2rLthd04591JarLDi7H6jRnzGbdddzqCXn2fc2FG0adeRHr224oAjfkyHTisvrM1TFYakAJg4YRx33HIt7Tt0Yu1u6/Hqy/1r7bfTbv9H9003n6O939238f67b9Gz11az20aPGsEpfQ5i2tSp7Lb3gbRp254Xn3+Ky39zLpO+mMie/3dIo22P1Jgmjh/LHX++kvYdu7D2uhvy6oBnsn2HDhnMmccdQPMWLdlprx/RrkNnxo8bzX9ff4UpUybPDsmJ48dxSu89mDF9OjvvczAdO6/Cxx++z3/uuY2XnnuCa+58pFqgauEwJAVA23YduPUfj9KufUdGDPuUIw7YqdZ+62+0MetvtHG1tilTJnP1Hy6i61rd+MY6G8xu//ttNzJh3Fgu7Xvr7Hl23esAfvnzn/CXG/ry/R13o1XrlRptm6TG0rZ9R2799wDadejEiM+GcsSeW9faL6XEpb84mXYdO3PJNX+jeYuW2WU+/ej9jBszil9c9md6bb397PZOK6/KtZddwMABz7DVdjs3+LZo7jwnKQCWb9KEdu07zte8/Z95nMlfTmK7H+5erf2tQa/QZeXV5gjV7+2wK1MmT6b/s4/Pd71SJS3fpCntOnSaZ7/XX3qe/73zJgcffQrNW7TkqylTmD59Wq19v5z0OQBt21V/H7YpXzdt1nwBq9b8qGhIRsQPI+LdiBgcEWdWshbNv8ce7Meyyy7H93bYtVr7tGlTadqs2Rz9mzUv3uyD33lrodQnVcrAF54GoHnLlpx21D7s/d312GurdTmzzwEMGfxOtb4b9/wOANf87jzeHvQKo0cO59UBz3Drn37Hehv1YJNete+tqnFVLCQjYlngKmAnYAPgwIjYYO5zaVEzetQIXh84gE02+w5t2rarNm3V1dfkk6FDGDtmdLX2Qa++WMw7euRCq1OqhE8//gCAi888npXatufMi6/iqJPPYcj773DGcfszesSw2X3X3bA7x59+IZ98/AE/O2ofDtt1c8758SGs2nVtLrrqNpZdzrNjlVDJPcnNgMEppQ9SSlOBOwGHPC5mHn/4fmbOnMn2NQ61Auy65wFMmzqVi3/xU95+8zWGD/uEfnffxn/++XcAvvpqysIuV1qoJk/+EoA1u63POb+9lq2334U9DjiCcy69ji8mTuCe266v1r99py6s981NOOrkczj3d9dzyLGn8ubAAfzytKOZ6vulIir50WQVYGiV158AvSpUi+bT4w/9ixVbtabXd7adY1qPb2/BT04/nxuuvoyfnXAoAC1XWJHjTzmbyy4+mxbN84MYpCVB06bF6Ybv77RXtfaNemxGxy6r8OZrL85u6//Uw/z6zOP5418fYI211wFg8+/uwNrrbcT5pxzOA/fcxp4HHrnwihewGIxujYhjgGMAOnTqUuFqVNV7/32ToR99wC577s/yTZrU2ucHu+zN93fYlQ//9x4zZsxgrW7rMnJ4cYhp5dXWWJjlSgtd2w7FoJuV2rWfY1qbdh0YP3bM7Nf/vPNGVl696+yAnKXnd7alabPmvDlwgCFZAZU83PopsFqV16uWbdWklK5LKfVMKfVsvVKbhVac5u2xh4qbCtQc1VrT8k2asM76G7H+RhvTtGkzXn3peQA2+fYWjV6jVEnrrF+M7B4zYvgc00aPHE7rNm1nvx47aiQzZ8yco9/MmTNJaSbTp09vvEKVVcmQfAnoFhFrRkQT4AAgfysXLVKmTZvGU4/9h9XWWIt11/9mnecbO2YUf7/9Rr6x7gZsvIlH17Vk23ybHWjatBkP9buTGTNmzG5/8ZnHGDNyOJtsvs3stlW7rsVnQz/knTdfrbaMZx/7N1O/+opu9XifqeFU7HBrSml6RJwIPAQsC9yYUvKagAr61z13MOmLz/nii4kAvP3Gq9x563UA9NpyW9aschjoxf5P8fnECexz4OHZ5Y0dM5rzTj+eLbb+Hu06dGLUiGH85193Q0qcdvaviYjG3SCpEf3rrluY9PnEr98vr7/EnTf8EYBe392eNbutT+s27Tj4uFO54YqLOKvPgWy9/S6MGTWcfn+7mU4rr1bt8Om+h/bhleef4pwfH8Iu+xxM51VWZ8jgd3jw3jto274ju+zrHaoqoaLnJFNKDwAPVLIGfe2ev93CyOGfzX79xmsv88ZrLwPQrkOnaiH52IP9WGaZZfj+jrvOsZxZmjdvQeeVV+HB+//BhHFjadW6DZtt8V0O6t2H9h07N96GSAvBPbddx8hhX58hemPgAN4YOACAdp06s2a39QHY+6CjWbH1Svzz9hu44cqLaN5iBbbabmd6n3AGK7ZqPXv+Db61KZff0o87briSpx7+F+NGj2TF1iuxzY67cfBxp7JS2znPa6rxRUqp0jXUWbf1NkxXXHdnpcuQFg/NW1W6AmmxsMuWGwxOUyd1q22at6WTJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpw5CUJCnDkJQkKcOQlCQpo84hGRGbRcTRNdr2iIg3IuLTiLi44cuTJKly6rMneR6w+6wXEbE6cAfQGZgAnBERhzdseZIkVU59QnJj4Nkqrw8AAuieUtoAeBg4pgFrkySpouoTku2AEVVe/wB4OqX0afm6H9CtoQqTJKnS6hOS44FOABHRFNgceLrK9AQ0b7DKJEmqsOXq0fc14KiIeBTYC2gGPFRl+ppU39OUJGmxVp+QvJDivOOLFOciH0kpvVxl+q7AgAasTZKkiqpzSKaUno+ITSjORU4A7pw1LSLaUQTovQ1eoSRJFVKfPUlSSu8B79XSPgY4paGKkiRpUeAddyRJysjuSUbE4/OxvJRS2m4B6pEkaZExt8Ota1Fc1iFJ0lIpG5Ippa4LsQ5JkhY5npOUJCnDkJQkKaNel4BERBvgSKAX0IY5Q9aBO5KkJUadQzIi1gCeA1amuJlAK2AsX4flaGBSI9QoSVJF1Odw66+AlYDtKL7tI4D9KcLy18DnwNYNXJ8kSRVTn5DcDrg+pfQEX18aEimlL1NKZwNvAJc0dIGSJFVKfb9P8s3y52nlc9WvxnoE2KEhipIkaVFQn5AcBbQtf/4cmAJ0rTK9CX6fpCRpCVKfkHwL2BiKIawUX5l1fESsHhFdgWOAdxq8QkmSKqQ+l4D8Ezg1IpqnlCYDv6T40uUPy+kJ2LuB65MkqWLq832SVwNXV3n9eERsAfwImAHcm1J6vuFLlCSpMup1M4GaUkovAy83UC2SJC1SvC2dJEkZ9bnjzo116JZSSkcuQD2SJC0y6nO4tXcd+iSKe7tKkrTYq/Ph1pTSMjUfwPLAusD1wAsU93GVJGmJsEDnJFNKM1JK76eUjgXG4G3pJElLkAUa3VrDg8B5QJ8GXGY1rVdozg+2+mZjLV5aokycPG3enSSxYosm2WkNObq1LbBCAy5PkqSKWuA9yYhYCdgeOAV4ZUGXJ0nSoqI+l4DM5OuvyJpjMsUXMP+0IYqSJGlRUJ89yVuZMyQTRTi+B9yRUvq8oQqTJKnS6nPv1t6NWIckSYucOg/ciYhfRMRGc5m+YUT8omHKkiSp8uozuvV84Ftzmb4RxSUgkiQtERryEpBmwPQGXJ4kSRU113OSEdEKWKlKU7uIWL2Wrm2Bg4ChDVeaJEmVNa+BO6cAs84zJuDy8lGbAE5vkKokSVoEzCsknyyfgyIs7wUG1eiTgC+AF1JKzzdodZIkVdBcQzKl9BTwFEBErAFck1IasDAKkySp0upzneThjVmIJEmLmvpcJ3lCRDw6l+kPR8SxDVOWJEmVV59LQHoD789l+nvAEQtUjSRJi5D6hGQ34I25TH+r7CNJ0hKhPiG5PMUNA3KazWO6JEmLlfqE5HvADnOZviPwvwUrR5KkRUd9QvIOYMeIuDAimsxqjIjlI+ICipC8vaELlCSpUurzfZJ/AHYCzgb6RMQ7Zft6FLelewa4rGHLkySpcuq8J5lSmkaxt3gm8AnQo3wMpbgd3XYUd+aRJGmJUK9vAUkpTUsp/Tal1D2l1LJ89ACeAK4EPmuUKiVJqoD6HG6tJiLaAgdTXBv5TYq9yPcaqC5Jkiqu3t8nGRE/iIi/AZ9SnKdsClwAfDOltF4D1ydJUsXUaU8yIrpS7DEeBqwKjAbuBn4EnJ1SuqexCpQkqVLmuicZEQdFxGPAYOAM4GVgL2AV4HwcqCNJWoLNa0/yL8AHwMnAHSmlMbMmRJiPkqQl27zOSX4FdAX2AH4YEc0bvSJJkhYR8wrJLhR7ke0o9iqHR8QNEfFdPNQqSVrCzTUkU0rjU0p9U0qbAD2Bv1Kck3wCeBZIQOtGr1KSpAqozx13BqaUTqDYuzyE4quxAP4cEa9FxDkRsWFjFClJUiXU+zrJlNJXKaXbU0rbAWsDFwFtgF8CrzdwfZIkVUy9Q7KqlNKQlNIvKAb37Ax4vaQkaYkx37elqyqllIAHy4ckSUuEBdqTlCRpSWZISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhqroYOHcrRRx3JN9ZekxVaNqfbN9aiz3HHMnTo0Nl9nnzySZZbNmp9XHTRrypYvdQ4vvjiC379qwvYb+/d6bbGyrRdoQkXXfCLWvtOnz6dS39zEd03XIcu7VakV4+NuP6aq0kpzdH33Xf+y+GHHMhG667FKh1a8+3uG3D+uWcxbuzYxt4kZSxX6QK06BozZgxbbL4Z06ZN49jj+rDG6mvw9n/f5rprr+GBB/7NG2++TatWrWb3P+KII9lmm22rLWPj7t0XbtHSQjB2zGgu/c1FrLzKqnxz4+48+fij2b6nnnwif7n5Rg7tfSSb9Pw2Tzz2CGecdjLjxo3l9LPOmd3v/ffeZfttvsNKbdpyxFHH0K59B14b+ApXXXk5Tzz2KI8/8wLLLrvswtg8VWFIKuuuu/7G8OHDufe+fuy2226z27t27copJ5/EIw8/zD777ju7vVevzTno4IMrUaq0UHXq3IW33h9Cly4r8/FHQ+i+4Tq19ntj0Gv85eYbOeEnp3DhxZcAcGjvIzji0B/xh99dwqGHH0nnzl0A+OutNzFp0iQeeuxpNtjomwAcdviRtGjZkj/1vYJBr79Gj002XTgbqNk83KqsiRMnAtClS5dq7V3KN3XzFi3mmGfSpEl89dVXjV+cVEFNmzalS5eV59nvvnvuBuDYPidUaz+mz4l89dVXPPCvfrPbPp/4OVAEcFWdOncGoHnz5gtUs+ZPxUIyIm6MiJER8WalatDcfe973wfgpJN+zPPPP8+nn37KI488wrnnnk2vzTdnxx13rNb/tNN+SutWK9CyRTM26bExf7/rrkqULS0yXh04kE6dOrPqaqtXa99k054ss8wyvPbawNltW5enKk7scxSvv/Yqn376Cff3u4++l/+e3fbYi/XW32Bhlq5SJQ+33gz0BW6tYA2ai80224y+fa/m3HPP5rtbbzm7fdddd+O22+9gueWK/z7LL788u+++BzvvvAudOnfmo4+GcFXfP3LggfszavQojj/+hNwqpCXa8GGf0bnGkRiAJk2a0LZtO4Z99tnstj333pd3/vs2V135Bx76zwOz2w/tfSSXXdF3odSrOVUsJFNKT0dE10qtX3WzyqqrsvnmW7Dd9tuz9lprM+iNQVz2u0vZa6896Nfvfpo1a8aWW27JlltuWW2+I444kk036c7ZPz+LQw45lBVXXLFCWyBVzpQpU1ixyuC2qpo2a8aUyZNnv44IVl+jK1tuvQ077bwrHTp25KUXX+Caq/7I5Mlfcs2fbyYiFlbpKjlwR1n9/vlP9ttvX14Z+BobbrghALvtvjs9emzC7rvtwrXXXsNJJ51c67wtWrTghBN/zMkn/YT+/fvPcWhWWho0a9aMqZlz9F9NmUKzKucZr726L7++6AJeHPgmHTt1AmCX3fZgtdXW4Gc//Qm777k3u+y2x0KpW19b5AfuRMQxEfFyRLw8atSoSpezVLniysvp1q3b7ICcZaeddqJFixY8/dRTc51/jdXXAGDM6NGNVqO0KOvcZWWGDxs2R/vUqVMZO3ZMtUOxf7rqSjbrtcXsgJxltz32AuC5Z59p3GJVq0U+JFNK16WUeqaUenbo0KHS5SxVhn32GTNmzJijfebMmcycOZNp06bNdf7B/xsMQIeOHRulPmlR171HD0aMGM4nQz+u1j7wlZeZOXMm3btvMrtt+LDa32/TZ0wvnufxflPjWORDUpWz7rrr8f777zNgwIBq7Xf//e9MmTKFTXv2BGDkyJFzzDt27FiuvOJy2rRpwxZbbLFQ6pUWNXvuXVxHfO2frqrWft2f+tKkSRN23m332W3d1lmXAf2f4+OPhlTre9cdtwHQ3WskK6Ji5yQj4g5gW6B9RHwCnJdSuqFS9WhOPzv9DB588D/88Ac7cFyf41lrzbV4441BXH/9dXTp0oU+fY4H4MAD92f55ZZny622okvnLnw89GNuvOHPjBgxgptuuoWWLVtWeEukhnf9NVczYcJ4JkyYAMAL/Z/nd5dcDMBOu+zKhht9i29t3IODDu3N1X+8nC8+/3z2HXfuu+duTj/rnGrXWp56+lkcceiP2PF7W3P4UcfQqVNnXhzQn7/dcRvrrb8Be++7X0W2c2kXtd0/cFHVs2fPNODFlytdxlJl0KBB/OrCX/Lyyy8xbNgw2rVrxw477MgFv7yQ1Vcvrv3q2/eP3HnnHQx+/33Gjx9Pq1at6NVrc0497Wdsu+22ld2ApdjEyR6ea0wbb9CNoR9/VOu0vtf8mR8dfCgA06ZN4/eX/obb/3orI4YPY/U11uDIo/twTJ8T5hit2v+5Z/n97y7h7TffYPToUXTq3IUf7rQLZ51zHm3atm30bVparbFKx8ETx4/rVts0Q1JaQhmSUt3MLSQ9JylJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVJGpJQqXUOdRcQo4KNK16E5tAdGV7oIaTHge2XRtEZKqUNtExarkNSiKSJeTin1rHQd0qLO98rix8OtkiRlGJKSJGUYkmoI11W6gCVZRHSNiBQR58+trbHWpQble2UxY0hqgaWUlsg3fkRsWwZG1ccXEfFKRJwUEctWusb5UQbh+RHRvdK1LG2W1PfKkmy5ShcgLQbuAB4AAlgZ6A1cDmwIHFOhmj4CmgPT52PersB5wBDgtQZcrrTEMSSleRuYUvrrrBcR8Sfgv8BREXFuSmlEzRkiYsWU0ueNVVAqhqVPWVyWKy2uPNwq1VNKaSLQn2LPcq2IGBIRT0ZEj4h4KCImAINm9Y+IbhHxl4gYFhFTy/6XRkTLmsuOiK0i4rmImBwRIyKiL7BCLf2y5w4jYp+ynvER8WVEvBsRV0ZEk4joDTxRdr2pymHkJ+e23IhYLiLOiIi3I2JKRIyJiHsj4pu5uiJi14h4qew/rNzm5Wr03zAi/h4Rn0bEVxExPCKeiIhd6vBPITU69ySleoqIAL5Rvpx1YfjqwOPA34F/UAZbRGxato8HrgU+BTYGfgJsGRHbpJSmlX17AY8CnwOXlPMcANxaj9ouAn4OvA38ARgGrA3sA/wCeBq4uOxzHfBMOesce8M13AbsBzwC/AnoDJwA9I+IrVNKr9bovzNwPHANcCOwB3AaMK5cPxHRjuJ3Q9nvI4qL7XsCvYB/13W7pUaTUvLhw0ctD2BbIFGES3ugA/At4PqyvX/Zb0j5+qhalvE68A6wYo32vcp5eldpex6YCqxTpa0J8GLZ9/wq7V1radusbHscaFZjfcHXNw/Ztua657HcHcq2v81aRtm+McW5y2dqmX8S0LXG+t8EhlVp273su1+l/619+Mg9PNwqzdsFwChgJEXoHQH0A/as0mcscFPVmcpDkd8CbgeaRkT7WQ/gWYog2bHs2xHYAvhnSum9WctIKU2l2COsi4PK57NSStXOK6ZSHZdT017l80VVl5FSeh34F7BVRNS8pdd9KaUhVddPcZi3c0TMOnw8oXzeKSJazWdtUqMyJKV5u45ib2p7iiDrkFLaI1UfsPO/lNKMGvOtXz7PCtmqj5FAS6BT2Wet8vmdWtb/dh3r7EaxZ/Z6HfvX1ZrATIrBSjW9VaVPVR/U0ndM+dwOIKX0FMWh5N7A6PJc7AURscECVyw1EM9JSvP2fkrp0Xn0+bKWtiifLwMezMw3br6rql0qH5VW8wNDVbN+L6SUDouIS4GdgK2BU4GzI+LklFLfRq5RmidDUmo875fPM+oQsh+Wz+vVMq2ue1bvUYTNxhTnMXPqG6IfUBx1Wp8qo3Zr1PYh8yml9CbF+cpLI2IlYADwm4i4agEOEUsNwsOtUuN5leKP/3ERsVbNieVlFW0BykO3LwB7RMQ6Vfo0AU6p4/puL58vLuerub5Ze3BflM9t67jc+8rns6osg4jYiGLwzbMppVF1XFbVetpGRLW/QSml8RSB2wJoVt9lSg3NPUmpkaSUUkQcQjHadFBE3EhxDq8FxSUkewNnATeXs/wUeBJ4LiKu4utLQOr0Pk0pvRgRlwBnAAMj4m/AcIrzhftSjH4dT3GO83Pg+Ij4smwbmVJ6PLPcRyLirrKWNhFxP19fAjKF4nKW+XEocEpE3AsMBqYB2wA/AO5KKU2ez+VKDcaQlBpRSum1iOhBEYa7A8dRBNQQinB8rErf/hGxA/Ab4EyK0Z93U1yX+EYd13dmRLwOnAicTnG0aCjFbfW+LPtMjogDgF9R3F6vKfAUX1+zWJuDgIEUg2wuoxiZ+xRwbkqpTrXV4kmgB7Ar0IXiPOaHFNdTej5SiwS/dFmSpAzPSUqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlPH/N+ZyM4a20qEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 540x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix: \n",
      "[[179 168]\n",
      " [ 85 108]]\n",
      "\n",
      "TN: 179\t FP: 168\n",
      "FN: 85\t TP: 108\n",
      "\n",
      "Precision: 0.391304347826087\n",
      "Recall:0.5595854922279793\n",
      "F1 Score: 0.46055437100213226\n"
     ]
    }
   ],
   "source": [
    "# Load First Model\n",
    "PATH = \"local_state_dict_model.pt\"\n",
    "load_model =    GRU(vocab_size=TEST_VOCAB_SIZE, \n",
    "                    hidden_dim=HIDDEN_DIM, \n",
    "                    embedding_dim=EMBEDDING_DIM, \n",
    "                    dropout=DROPOUT) \n",
    "\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "\n",
    "# Test Model\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(load_model.parameters(), lr=lr)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "    eval_losses = []\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        # print(f\"i: {i}\\nInputs: {inputs} Labels: {labels}\")\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "\n",
    "        output, _ = load_model(inputs.to(torch.long), h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        eval_losses.append(loss)\n",
    "        preds = output.squeeze()\n",
    "        if len(labels) > 1:\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.numpy().astype(int))\n",
    "        # print(f\"Preds: {preds}\\n Preds Type: {type(preds)}\")\n",
    "\n",
    "\n",
    "roc_acc_score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels_list, test_preds)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# Index of largest G-means\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "threshold = thresholds[ix]\n",
    "\n",
    "# Print how many data is being tested\n",
    "print(f\"Amount of test data: {len(test_labels_list)}\")\n",
    "\n",
    "\n",
    "# # Plot ROC Curve\n",
    "# plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "# plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "# # axis labels\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()\n",
    "\n",
    "    \n",
    "print(f\"ROC Accuracy Score: {roc_acc_score}\")\n",
    "\n",
    "# Normalize probability with threshold\n",
    "test_preds_thresholded = np.where(test_preds > threshold, 1, 0)\n",
    "for i in range(len(test_preds)-1140):\n",
    "    print(\"Test Preds Prob: {}    \\\n",
    "    Test Preds Label: {}  \\\n",
    "    True Label: {}  \\\n",
    "    \".format(test_preds[i], test_preds_thresholded[i], test_labels_list[i]))\n",
    "\n",
    "acc_score = accuracy_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"\\nAccuracy Score: {acc_score}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cm = confusion_matrix(test_labels_list, test_preds_thresholded)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix: \\n{cm}\")\n",
    "\n",
    "tn = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tp = cm[1][1]\n",
    "print(f\"\\nTN: {tn}\\t FP: {fp}\\nFN: {fn}\\t TP: {tp}\\n\")\n",
    "\n",
    "# Precision\n",
    "precision = tp/(tp+fp)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# Recall\n",
    "recall = tp/(tp+fn)\n",
    "print(f\"Recall:{recall}\")\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_score = f1_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, PySyft does not support optimizers with momentum. Therefore, we are going to stick with the classical [Stochastic Gradient Descent](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) optimizer.\n",
    "\n",
    "As our task consists of a binary classification, we are going to use the [Binary Cross Entropy Loss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T20:00:23.084933Z",
     "start_time": "2019-06-03T20:00:23.078688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining loss and optimizer\n",
    "second_model = make_model()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(second_model.parameters(), lr=lr)\n",
    "\n",
    "# Create data\n",
    "# # Creating federated datasets, an extension of Pytorch TensorDataset class\n",
    "federated_train_dataset = sy.FederatedDataset([bob_train_dataset, anne_train_dataset])\n",
    "federated_test_dataset = sy.FederatedDataset([bob_test_dataset, anne_test_dataset])\n",
    "\n",
    "# Creating federated dataloaders, an extension of Pytorch DataLoader class for TRAINIG METHOD #2\n",
    "federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, batch_size=BATCH_SIZE)\n",
    "federated_test_loader = sy.FederatedDataLoader(federated_test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:56:01.459697Z",
     "start_time": "2019-06-03T19:33:42.666174Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200...      Training loss: 0.68900...      Validation loss: 0.67529\n",
      "Trigger Times: 0\n",
      "Epoch 2/200...      Training loss: 0.67209...      Validation loss: 0.66354\n",
      "Trigger Times: 0\n",
      "Epoch 3/200...      Training loss: 0.66555...      Validation loss: 0.65205\n",
      "Trigger Times: 0\n",
      "Epoch 4/200...      Training loss: 0.65809...      Validation loss: 0.64079\n",
      "Trigger Times: 0\n",
      "Epoch 5/200...      Training loss: 0.64276...      Validation loss: 0.62992\n",
      "Trigger Times: 0\n",
      "Epoch 6/200...      Training loss: 0.63125...      Validation loss: 0.61934\n",
      "Trigger Times: 0\n",
      "Epoch 7/200...      Training loss: 0.61843...      Validation loss: 0.60904\n",
      "Trigger Times: 0\n",
      "Epoch 8/200...      Training loss: 0.61178...      Validation loss: 0.59895\n",
      "Trigger Times: 0\n",
      "Epoch 9/200...      Training loss: 0.60100...      Validation loss: 0.58912\n",
      "Trigger Times: 0\n",
      "Epoch 10/200...      Training loss: 0.59388...      Validation loss: 0.57953\n",
      "Trigger Times: 0\n",
      "Epoch 11/200...      Training loss: 0.58145...      Validation loss: 0.57021\n",
      "Trigger Times: 0\n",
      "Epoch 12/200...      Training loss: 0.56977...      Validation loss: 0.56112\n",
      "Trigger Times: 0\n",
      "Epoch 13/200...      Training loss: 0.56087...      Validation loss: 0.55229\n",
      "Trigger Times: 0\n",
      "Epoch 14/200...      Training loss: 0.55258...      Validation loss: 0.54367\n",
      "Trigger Times: 0\n",
      "Epoch 15/200...      Training loss: 0.54552...      Validation loss: 0.53521\n",
      "Trigger Times: 0\n",
      "Epoch 16/200...      Training loss: 0.53646...      Validation loss: 0.52701\n",
      "Trigger Times: 0\n",
      "Epoch 17/200...      Training loss: 0.52980...      Validation loss: 0.51900\n",
      "Trigger Times: 0\n",
      "Epoch 18/200...      Training loss: 0.52259...      Validation loss: 0.51113\n",
      "Trigger Times: 0\n",
      "Epoch 19/200...      Training loss: 0.51418...      Validation loss: 0.50348\n",
      "Trigger Times: 0\n",
      "Epoch 20/200...      Training loss: 0.50450...      Validation loss: 0.49601\n",
      "Trigger Times: 0\n",
      "Epoch 21/200...      Training loss: 0.49719...      Validation loss: 0.48870\n",
      "Trigger Times: 0\n",
      "Epoch 22/200...      Training loss: 0.49180...      Validation loss: 0.48155\n",
      "Trigger Times: 0\n",
      "Epoch 23/200...      Training loss: 0.48331...      Validation loss: 0.47455\n",
      "Trigger Times: 0\n",
      "Epoch 24/200...      Training loss: 0.47970...      Validation loss: 0.46769\n",
      "Trigger Times: 0\n",
      "Epoch 25/200...      Training loss: 0.47131...      Validation loss: 0.46103\n",
      "Trigger Times: 0\n",
      "Epoch 26/200...      Training loss: 0.46433...      Validation loss: 0.45451\n",
      "Trigger Times: 0\n",
      "Epoch 27/200...      Training loss: 0.45582...      Validation loss: 0.44815\n",
      "Trigger Times: 0\n",
      "Epoch 28/200...      Training loss: 0.45183...      Validation loss: 0.44185\n",
      "Trigger Times: 0\n",
      "Epoch 29/200...      Training loss: 0.44464...      Validation loss: 0.43572\n",
      "Trigger Times: 0\n",
      "Epoch 30/200...      Training loss: 0.43847...      Validation loss: 0.42973\n",
      "Trigger Times: 0\n",
      "Epoch 31/200...      Training loss: 0.43265...      Validation loss: 0.42385\n",
      "Trigger Times: 0\n",
      "Epoch 32/200...      Training loss: 0.42540...      Validation loss: 0.41812\n",
      "Trigger Times: 0\n",
      "Epoch 33/200...      Training loss: 0.42041...      Validation loss: 0.41249\n",
      "Trigger Times: 0\n",
      "Epoch 34/200...      Training loss: 0.41340...      Validation loss: 0.40701\n",
      "Trigger Times: 0\n",
      "Epoch 35/200...      Training loss: 0.41017...      Validation loss: 0.40161\n",
      "Trigger Times: 0\n",
      "Epoch 36/200...      Training loss: 0.40427...      Validation loss: 0.39633\n",
      "Trigger Times: 0\n",
      "Epoch 37/200...      Training loss: 0.39890...      Validation loss: 0.39116\n",
      "Trigger Times: 0\n",
      "Epoch 38/200...      Training loss: 0.39363...      Validation loss: 0.38611\n",
      "Trigger Times: 0\n",
      "Epoch 39/200...      Training loss: 0.39322...      Validation loss: 0.38111\n",
      "Trigger Times: 0\n",
      "Epoch 40/200...      Training loss: 0.38462...      Validation loss: 0.37626\n",
      "Trigger Times: 0\n",
      "Epoch 41/200...      Training loss: 0.37819...      Validation loss: 0.37148\n",
      "Trigger Times: 0\n",
      "Epoch 42/200...      Training loss: 0.37392...      Validation loss: 0.36679\n",
      "Trigger Times: 0\n",
      "Epoch 43/200...      Training loss: 0.36742...      Validation loss: 0.36222\n",
      "Trigger Times: 0\n",
      "Epoch 44/200...      Training loss: 0.36547...      Validation loss: 0.35771\n",
      "Trigger Times: 0\n",
      "Epoch 45/200...      Training loss: 0.35856...      Validation loss: 0.35329\n",
      "Trigger Times: 0\n",
      "Epoch 46/200...      Training loss: 0.35746...      Validation loss: 0.34894\n",
      "Trigger Times: 0\n",
      "Epoch 47/200...      Training loss: 0.35213...      Validation loss: 0.34467\n",
      "Trigger Times: 0\n",
      "Epoch 48/200...      Training loss: 0.35036...      Validation loss: 0.34048\n",
      "Trigger Times: 0\n",
      "Epoch 49/200...      Training loss: 0.34353...      Validation loss: 0.33636\n",
      "Trigger Times: 0\n",
      "Epoch 50/200...      Training loss: 0.34150...      Validation loss: 0.33234\n",
      "Trigger Times: 0\n",
      "Epoch 51/200...      Training loss: 0.33673...      Validation loss: 0.32838\n",
      "Trigger Times: 0\n",
      "Epoch 52/200...      Training loss: 0.33097...      Validation loss: 0.32449\n",
      "Trigger Times: 0\n",
      "Epoch 53/200...      Training loss: 0.32826...      Validation loss: 0.32068\n",
      "Trigger Times: 0\n",
      "Epoch 54/200...      Training loss: 0.32455...      Validation loss: 0.31693\n",
      "Trigger Times: 0\n",
      "Epoch 55/200...      Training loss: 0.32130...      Validation loss: 0.31323\n",
      "Trigger Times: 0\n",
      "Epoch 56/200...      Training loss: 0.31666...      Validation loss: 0.30961\n",
      "Trigger Times: 0\n",
      "Epoch 57/200...      Training loss: 0.31246...      Validation loss: 0.30606\n",
      "Trigger Times: 0\n",
      "Epoch 58/200...      Training loss: 0.31095...      Validation loss: 0.30257\n",
      "Trigger Times: 0\n",
      "Epoch 59/200...      Training loss: 0.30442...      Validation loss: 0.29915\n",
      "Trigger Times: 0\n",
      "Epoch 60/200...      Training loss: 0.30466...      Validation loss: 0.29578\n",
      "Trigger Times: 0\n",
      "Epoch 61/200...      Training loss: 0.29786...      Validation loss: 0.29248\n",
      "Trigger Times: 0\n",
      "Epoch 62/200...      Training loss: 0.29544...      Validation loss: 0.28922\n",
      "Trigger Times: 0\n",
      "Epoch 63/200...      Training loss: 0.29375...      Validation loss: 0.28601\n",
      "Trigger Times: 0\n",
      "Epoch 64/200...      Training loss: 0.29096...      Validation loss: 0.28286\n",
      "Trigger Times: 0\n",
      "Epoch 65/200...      Training loss: 0.28724...      Validation loss: 0.27976\n",
      "Trigger Times: 0\n",
      "Epoch 66/200...      Training loss: 0.28636...      Validation loss: 0.27671\n",
      "Trigger Times: 0\n",
      "Epoch 67/200...      Training loss: 0.28098...      Validation loss: 0.27374\n",
      "Trigger Times: 0\n",
      "Epoch 68/200...      Training loss: 0.27712...      Validation loss: 0.27080\n",
      "Trigger Times: 0\n",
      "Epoch 69/200...      Training loss: 0.27534...      Validation loss: 0.26791\n",
      "Trigger Times: 0\n",
      "Epoch 70/200...      Training loss: 0.27210...      Validation loss: 0.26506\n",
      "Trigger Times: 0\n",
      "Epoch 71/200...      Training loss: 0.27076...      Validation loss: 0.26226\n",
      "Trigger Times: 0\n",
      "Epoch 72/200...      Training loss: 0.26566...      Validation loss: 0.25951\n",
      "Trigger Times: 0\n",
      "Epoch 73/200...      Training loss: 0.26246...      Validation loss: 0.25681\n",
      "Trigger Times: 0\n",
      "Epoch 74/200...      Training loss: 0.26229...      Validation loss: 0.25415\n",
      "Trigger Times: 0\n",
      "Epoch 75/200...      Training loss: 0.25941...      Validation loss: 0.25153\n",
      "Trigger Times: 0\n",
      "Epoch 76/200...      Training loss: 0.25421...      Validation loss: 0.24895\n",
      "Trigger Times: 0\n",
      "Epoch 77/200...      Training loss: 0.25218...      Validation loss: 0.24641\n",
      "Trigger Times: 0\n",
      "Epoch 78/200...      Training loss: 0.25147...      Validation loss: 0.24391\n",
      "Trigger Times: 0\n",
      "Epoch 79/200...      Training loss: 0.24798...      Validation loss: 0.24145\n",
      "Trigger Times: 0\n",
      "Epoch 80/200...      Training loss: 0.24435...      Validation loss: 0.23903\n",
      "Trigger Times: 0\n",
      "Epoch 81/200...      Training loss: 0.24189...      Validation loss: 0.23665\n",
      "Trigger Times: 0\n",
      "Epoch 82/200...      Training loss: 0.24254...      Validation loss: 0.23431\n",
      "Trigger Times: 0\n",
      "Epoch 83/200...      Training loss: 0.23952...      Validation loss: 0.23198\n",
      "Trigger Times: 0\n",
      "Epoch 84/200...      Training loss: 0.23660...      Validation loss: 0.22970\n",
      "Trigger Times: 0\n",
      "Epoch 85/200...      Training loss: 0.23410...      Validation loss: 0.22746\n",
      "Trigger Times: 0\n",
      "Epoch 86/200...      Training loss: 0.23155...      Validation loss: 0.22525\n",
      "Trigger Times: 0\n",
      "Epoch 87/200...      Training loss: 0.22683...      Validation loss: 0.22308\n",
      "Trigger Times: 0\n",
      "Epoch 88/200...      Training loss: 0.22654...      Validation loss: 0.22093\n",
      "Trigger Times: 0\n",
      "Epoch 89/200...      Training loss: 0.22557...      Validation loss: 0.21882\n",
      "Trigger Times: 0\n",
      "Epoch 90/200...      Training loss: 0.22450...      Validation loss: 0.21674\n",
      "Trigger Times: 0\n",
      "Epoch 91/200...      Training loss: 0.22200...      Validation loss: 0.21470\n",
      "Trigger Times: 0\n",
      "Epoch 92/200...      Training loss: 0.21828...      Validation loss: 0.21269\n",
      "Trigger Times: 0\n",
      "Epoch 93/200...      Training loss: 0.21677...      Validation loss: 0.21071\n",
      "Trigger Times: 0\n",
      "Epoch 94/200...      Training loss: 0.21545...      Validation loss: 0.20875\n",
      "Trigger Times: 0\n",
      "Epoch 95/200...      Training loss: 0.21332...      Validation loss: 0.20682\n",
      "Trigger Times: 0\n",
      "Epoch 96/200...      Training loss: 0.21045...      Validation loss: 0.20493\n",
      "Trigger Times: 0\n",
      "Epoch 97/200...      Training loss: 0.20865...      Validation loss: 0.20305\n",
      "Trigger Times: 0\n",
      "Epoch 98/200...      Training loss: 0.20679...      Validation loss: 0.20120\n",
      "Trigger Times: 0\n",
      "Epoch 99/200...      Training loss: 0.20572...      Validation loss: 0.19938\n",
      "Trigger Times: 0\n",
      "Epoch 100/200...      Training loss: 0.20307...      Validation loss: 0.19758\n",
      "Trigger Times: 0\n",
      "Epoch 101/200...      Training loss: 0.20044...      Validation loss: 0.19580\n",
      "Trigger Times: 0\n",
      "Epoch 102/200...      Training loss: 0.20173...      Validation loss: 0.19404\n",
      "Trigger Times: 0\n",
      "Epoch 103/200...      Training loss: 0.19942...      Validation loss: 0.19231\n",
      "Trigger Times: 0\n",
      "Epoch 104/200...      Training loss: 0.19570...      Validation loss: 0.19062\n",
      "Trigger Times: 0\n",
      "Epoch 105/200...      Training loss: 0.19547...      Validation loss: 0.18894\n",
      "Trigger Times: 0\n",
      "Epoch 106/200...      Training loss: 0.19115...      Validation loss: 0.18729\n",
      "Trigger Times: 0\n",
      "Epoch 107/200...      Training loss: 0.19324...      Validation loss: 0.18565\n",
      "Trigger Times: 0\n",
      "Epoch 108/200...      Training loss: 0.18864...      Validation loss: 0.18405\n",
      "Trigger Times: 0\n",
      "Epoch 109/200...      Training loss: 0.18656...      Validation loss: 0.18246\n",
      "Trigger Times: 0\n",
      "Epoch 110/200...      Training loss: 0.18735...      Validation loss: 0.18089\n",
      "Trigger Times: 0\n",
      "Epoch 111/200...      Training loss: 0.18817...      Validation loss: 0.17934\n",
      "Trigger Times: 0\n",
      "Epoch 112/200...      Training loss: 0.18178...      Validation loss: 0.17783\n",
      "Trigger Times: 0\n",
      "Epoch 113/200...      Training loss: 0.18076...      Validation loss: 0.17632\n",
      "Trigger Times: 0\n",
      "Epoch 114/200...      Training loss: 0.18042...      Validation loss: 0.17484\n",
      "Trigger Times: 0\n",
      "Epoch 115/200...      Training loss: 0.17961...      Validation loss: 0.17339\n",
      "Trigger Times: 0\n",
      "Epoch 116/200...      Training loss: 0.17702...      Validation loss: 0.17194\n",
      "Trigger Times: 0\n",
      "Epoch 117/200...      Training loss: 0.17694...      Validation loss: 0.17052\n",
      "Trigger Times: 0\n",
      "Epoch 118/200...      Training loss: 0.17569...      Validation loss: 0.16911\n",
      "Trigger Times: 0\n",
      "Epoch 119/200...      Training loss: 0.17357...      Validation loss: 0.16772\n",
      "Trigger Times: 0\n",
      "Epoch 120/200...      Training loss: 0.17380...      Validation loss: 0.16635\n",
      "Trigger Times: 0\n",
      "Epoch 121/200...      Training loss: 0.17204...      Validation loss: 0.16498\n",
      "Trigger Times: 0\n",
      "Epoch 122/200...      Training loss: 0.16929...      Validation loss: 0.16365\n",
      "Trigger Times: 0\n",
      "Epoch 123/200...      Training loss: 0.16912...      Validation loss: 0.16233\n",
      "Trigger Times: 0\n",
      "Epoch 124/200...      Training loss: 0.16662...      Validation loss: 0.16102\n",
      "Trigger Times: 0\n",
      "Epoch 125/200...      Training loss: 0.16624...      Validation loss: 0.15973\n",
      "Trigger Times: 0\n",
      "Epoch 126/200...      Training loss: 0.16430...      Validation loss: 0.15845\n",
      "Trigger Times: 0\n",
      "Epoch 127/200...      Training loss: 0.16287...      Validation loss: 0.15720\n",
      "Trigger Times: 0\n",
      "Epoch 128/200...      Training loss: 0.16217...      Validation loss: 0.15596\n",
      "Trigger Times: 0\n",
      "Epoch 129/200...      Training loss: 0.16156...      Validation loss: 0.15473\n",
      "Trigger Times: 0\n",
      "Epoch 130/200...      Training loss: 0.15906...      Validation loss: 0.15352\n",
      "Trigger Times: 0\n",
      "Epoch 131/200...      Training loss: 0.15729...      Validation loss: 0.15234\n",
      "Trigger Times: 0\n",
      "Epoch 132/200...      Training loss: 0.15659...      Validation loss: 0.15116\n",
      "Trigger Times: 0\n",
      "Epoch 133/200...      Training loss: 0.15593...      Validation loss: 0.15000\n",
      "Trigger Times: 0\n",
      "Epoch 134/200...      Training loss: 0.15434...      Validation loss: 0.14885\n",
      "Trigger Times: 0\n",
      "Epoch 135/200...      Training loss: 0.15236...      Validation loss: 0.14771\n",
      "Trigger Times: 0\n",
      "Epoch 136/200...      Training loss: 0.15299...      Validation loss: 0.14658\n",
      "Trigger Times: 0\n",
      "Epoch 137/200...      Training loss: 0.14899...      Validation loss: 0.14547\n",
      "Trigger Times: 0\n",
      "Epoch 138/200...      Training loss: 0.14922...      Validation loss: 0.14437\n",
      "Trigger Times: 0\n",
      "Epoch 139/200...      Training loss: 0.14927...      Validation loss: 0.14329\n",
      "Trigger Times: 0\n",
      "Epoch 140/200...      Training loss: 0.14771...      Validation loss: 0.14222\n",
      "Trigger Times: 0\n",
      "Epoch 141/200...      Training loss: 0.14591...      Validation loss: 0.14117\n",
      "Trigger Times: 0\n",
      "Epoch 142/200...      Training loss: 0.14618...      Validation loss: 0.14012\n",
      "Trigger Times: 0\n",
      "Epoch 143/200...      Training loss: 0.14381...      Validation loss: 0.13909\n",
      "Trigger Times: 0\n",
      "Epoch 144/200...      Training loss: 0.14351...      Validation loss: 0.13806\n",
      "Trigger Times: 0\n",
      "Epoch 145/200...      Training loss: 0.14186...      Validation loss: 0.13705\n",
      "Trigger Times: 0\n",
      "Epoch 146/200...      Training loss: 0.14245...      Validation loss: 0.13605\n",
      "Trigger Times: 0\n",
      "Epoch 147/200...      Training loss: 0.14044...      Validation loss: 0.13506\n",
      "Trigger Times: 0\n",
      "Epoch 148/200...      Training loss: 0.13700...      Validation loss: 0.13409\n",
      "Trigger Times: 0\n",
      "Epoch 149/200...      Training loss: 0.13873...      Validation loss: 0.13313\n",
      "Trigger Times: 0\n",
      "Epoch 150/200...      Training loss: 0.13706...      Validation loss: 0.13217\n",
      "Trigger Times: 0\n",
      "Epoch 151/200...      Training loss: 0.13733...      Validation loss: 0.13124\n",
      "Trigger Times: 0\n",
      "Epoch 152/200...      Training loss: 0.13521...      Validation loss: 0.13031\n",
      "Trigger Times: 0\n",
      "Epoch 153/200...      Training loss: 0.13597...      Validation loss: 0.12938\n",
      "Trigger Times: 0\n",
      "Epoch 154/200...      Training loss: 0.13363...      Validation loss: 0.12847\n",
      "Trigger Times: 0\n",
      "Epoch 155/200...      Training loss: 0.13438...      Validation loss: 0.12757\n",
      "Trigger Times: 0\n",
      "Epoch 156/200...      Training loss: 0.13035...      Validation loss: 0.12668\n",
      "Trigger Times: 0\n",
      "Epoch 157/200...      Training loss: 0.13059...      Validation loss: 0.12580\n",
      "Trigger Times: 0\n",
      "Epoch 158/200...      Training loss: 0.13103...      Validation loss: 0.12493\n",
      "Trigger Times: 0\n",
      "Epoch 159/200...      Training loss: 0.13029...      Validation loss: 0.12407\n",
      "Trigger Times: 0\n",
      "Epoch 160/200...      Training loss: 0.12750...      Validation loss: 0.12323\n",
      "Trigger Times: 0\n",
      "Epoch 161/200...      Training loss: 0.12855...      Validation loss: 0.12238\n",
      "Trigger Times: 0\n",
      "Epoch 162/200...      Training loss: 0.12462...      Validation loss: 0.12155\n",
      "Trigger Times: 0\n",
      "Epoch 163/200...      Training loss: 0.12377...      Validation loss: 0.12073\n",
      "Trigger Times: 0\n",
      "Epoch 164/200...      Training loss: 0.12417...      Validation loss: 0.11992\n",
      "Trigger Times: 0\n",
      "Epoch 165/200...      Training loss: 0.12405...      Validation loss: 0.11911\n",
      "Trigger Times: 0\n",
      "Epoch 166/200...      Training loss: 0.12331...      Validation loss: 0.11831\n",
      "Trigger Times: 0\n",
      "Epoch 167/200...      Training loss: 0.12323...      Validation loss: 0.11752\n",
      "Trigger Times: 0\n",
      "Epoch 168/200...      Training loss: 0.12065...      Validation loss: 0.11674\n",
      "Trigger Times: 0\n",
      "Epoch 169/200...      Training loss: 0.11911...      Validation loss: 0.11597\n",
      "Trigger Times: 0\n",
      "Epoch 170/200...      Training loss: 0.11807...      Validation loss: 0.11522\n",
      "Trigger Times: 0\n",
      "Epoch 171/200...      Training loss: 0.11835...      Validation loss: 0.11446\n",
      "Trigger Times: 0\n",
      "Epoch 172/200...      Training loss: 0.11978...      Validation loss: 0.11371\n",
      "Trigger Times: 0\n",
      "Epoch 173/200...      Training loss: 0.11678...      Validation loss: 0.11297\n",
      "Trigger Times: 0\n",
      "Epoch 174/200...      Training loss: 0.11660...      Validation loss: 0.11224\n",
      "Trigger Times: 0\n",
      "Epoch 175/200...      Training loss: 0.11591...      Validation loss: 0.11151\n",
      "Trigger Times: 0\n",
      "Epoch 176/200...      Training loss: 0.11521...      Validation loss: 0.11079\n",
      "Trigger Times: 0\n",
      "Epoch 177/200...      Training loss: 0.11322...      Validation loss: 0.11009\n",
      "Trigger Times: 0\n",
      "Epoch 178/200...      Training loss: 0.11611...      Validation loss: 0.10938\n",
      "Trigger Times: 0\n",
      "Epoch 179/200...      Training loss: 0.11432...      Validation loss: 0.10869\n",
      "Trigger Times: 0\n",
      "Epoch 180/200...      Training loss: 0.11230...      Validation loss: 0.10800\n",
      "Trigger Times: 0\n",
      "Epoch 181/200...      Training loss: 0.11193...      Validation loss: 0.10732\n",
      "Trigger Times: 0\n",
      "Epoch 182/200...      Training loss: 0.11218...      Validation loss: 0.10664\n",
      "Trigger Times: 0\n",
      "Epoch 183/200...      Training loss: 0.11137...      Validation loss: 0.10597\n",
      "Trigger Times: 0\n",
      "Epoch 184/200...      Training loss: 0.11031...      Validation loss: 0.10531\n",
      "Trigger Times: 0\n",
      "Epoch 185/200...      Training loss: 0.11092...      Validation loss: 0.10465\n",
      "Trigger Times: 0\n",
      "Epoch 186/200...      Training loss: 0.10920...      Validation loss: 0.10400\n",
      "Trigger Times: 0\n",
      "Epoch 187/200...      Training loss: 0.10820...      Validation loss: 0.10336\n",
      "Trigger Times: 0\n",
      "Epoch 188/200...      Training loss: 0.10720...      Validation loss: 0.10272\n",
      "Trigger Times: 0\n",
      "Epoch 189/200...      Training loss: 0.10717...      Validation loss: 0.10210\n",
      "Trigger Times: 0\n",
      "Epoch 190/200...      Training loss: 0.10549...      Validation loss: 0.10147\n",
      "Trigger Times: 0\n",
      "Epoch 191/200...      Training loss: 0.10562...      Validation loss: 0.10086\n",
      "Trigger Times: 0\n",
      "Epoch 192/200...      Training loss: 0.10492...      Validation loss: 0.10025\n",
      "Trigger Times: 0\n",
      "Epoch 193/200...      Training loss: 0.10545...      Validation loss: 0.09964\n",
      "Trigger Times: 0\n",
      "Epoch 194/200...      Training loss: 0.10320...      Validation loss: 0.09905\n",
      "Trigger Times: 0\n",
      "Epoch 195/200...      Training loss: 0.10137...      Validation loss: 0.09846\n",
      "Trigger Times: 0\n",
      "Epoch 196/200...      Training loss: 0.10247...      Validation loss: 0.09787\n",
      "Trigger Times: 0\n",
      "Epoch 197/200...      Training loss: 0.10134...      Validation loss: 0.09728\n",
      "Trigger Times: 0\n",
      "Epoch 198/200...      Training loss: 0.10088...      Validation loss: 0.09671\n",
      "Trigger Times: 0\n",
      "Epoch 199/200...      Training loss: 0.10106...      Validation loss: 0.09613\n",
      "Trigger Times: 0\n",
      "Epoch 200/200...      Training loss: 0.10053...      Validation loss: 0.09557\n",
      "Trigger Times: 0\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# For Early Stopping\n",
    "last_loss = 100\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    \n",
    "    ######### Training ##########\n",
    "\n",
    "    losses = []\n",
    "    # Batch loop\n",
    "    for inputs, labels in federated_train_loader:\n",
    "        # Location of current batch\n",
    "        worker = inputs.location\n",
    "        # Initialize hidden state and send it to worker\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
    "        # Send model to current worker\n",
    "        second_model.send(worker)\n",
    "        # Setting accumulated gradients to zero before backward step\n",
    "        optimizer.zero_grad()\n",
    "        # Output from the model\n",
    "        output, _ = second_model(inputs.to(torch.long), h)\n",
    "        # print(f\"Output:{output}\")\n",
    "        # Calculate the loss and perform backprop\n",
    "        # print(f\"Output Shape: {output.shape} Labels Shape: {labels.shape}\")\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # # Clipping the gradient to avoid explosion\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        # Backpropagation step\n",
    "        optimizer.step() \n",
    "        # Get the model back to the local worker\n",
    "        second_model.get()\n",
    "        losses.append(loss.get())\n",
    "    \n",
    "    \n",
    "    ######## Evaluation ##########\n",
    "    \n",
    "    # Model in evaluation mode\n",
    "    second_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        test_labels_list = []\n",
    "        eval_losses = []\n",
    "\n",
    "        for inputs, labels in federated_test_loader:\n",
    "            # get current location\n",
    "            worker = inputs.location\n",
    "            # Initialize hidden state and send it to worker\n",
    "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)    \n",
    "            # Send model to worker\n",
    "            second_model.send(worker)\n",
    "            output, _ = second_model(inputs.to(torch.long), h)\n",
    "            # loss = criterion(output.squeeze(), labels.float())\n",
    "            loss = criterion(output, labels.float())\n",
    "            eval_losses.append(loss.get())\n",
    "            preds = output.squeeze().get()\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.get().numpy().astype(int))\n",
    "            # Get the model back to the local worker\n",
    "            second_model.get()\n",
    "\n",
    "    # score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "    # Check test preds\n",
    "\n",
    "    train_loss = sum(losses)/len(losses)\n",
    "    eval_loss = sum(eval_losses)/len(eval_losses)\n",
    "    \n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(eval_loss.item())\n",
    "    \n",
    "    print(\"Epoch {}/{}...  \\\n",
    "    Training loss: {:.5f}...  \\\n",
    "    Validation loss: {:.5f}\".format(e+1, EPOCHS, train_loss, eval_loss))\n",
    "        \n",
    "    # Early Stopping\n",
    "    if eval_loss > last_loss:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times: {trigger_times}\")\n",
    "        \n",
    "        if trigger_times >= patience:\n",
    "            print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Trigger Times: 0\")\n",
    "        trigger_times = 0\n",
    "    \n",
    "    last_loss = eval_loss\n",
    "    \n",
    "    second_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Method #2 Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Losses: [0.6889998316764832, 0.6720947623252869, 0.6655535697937012, 0.6580928564071655, 0.6427559852600098, 0.6312481164932251, 0.6184279322624207, 0.6117834448814392, 0.6010000109672546, 0.5938795804977417, 0.5814465880393982, 0.5697726607322693, 0.5608692765235901, 0.5525792837142944, 0.545518159866333, 0.5364646911621094, 0.5297957062721252, 0.5225861668586731, 0.5141797065734863, 0.5045021176338196, 0.4971862733364105, 0.49180158972740173, 0.4833100736141205, 0.4797002077102661, 0.4713115692138672, 0.4643292725086212, 0.4558200240135193, 0.45183444023132324, 0.44464194774627686, 0.4384668171405792, 0.432645708322525, 0.4253976345062256, 0.4204065799713135, 0.41340094804763794, 0.41017112135887146, 0.4042721390724182, 0.3989022374153137, 0.3936273753643036, 0.39322444796562195, 0.38461777567863464, 0.3781859874725342, 0.37391793727874756, 0.367418110370636, 0.36546599864959717, 0.3585582375526428, 0.3574621379375458, 0.35213351249694824, 0.35036250948905945, 0.34352579712867737, 0.34149670600891113, 0.3367338478565216, 0.330970823764801, 0.3282649517059326, 0.32454657554626465, 0.3213047385215759, 0.31666409969329834, 0.31245654821395874, 0.3109501600265503, 0.3044171929359436, 0.3046571612358093, 0.2978556156158447, 0.29543939232826233, 0.2937498986721039, 0.29096224904060364, 0.2872435450553894, 0.28636109828948975, 0.28098413348197937, 0.27711567282676697, 0.27534186840057373, 0.2721000909805298, 0.2707614302635193, 0.2656576335430145, 0.26246127486228943, 0.2622887194156647, 0.2594057619571686, 0.2542097270488739, 0.2521807551383972, 0.25146567821502686, 0.2479778528213501, 0.24435023963451385, 0.24188970029354095, 0.24253752827644348, 0.23951685428619385, 0.23659953474998474, 0.23409880697727203, 0.23154914379119873, 0.22682806849479675, 0.2265413999557495, 0.22557120025157928, 0.22449739277362823, 0.22200043499469757, 0.21827596426010132, 0.21676869690418243, 0.21545040607452393, 0.2133246660232544, 0.2104502022266388, 0.20865418016910553, 0.2067909836769104, 0.20572488009929657, 0.20307330787181854, 0.20043830573558807, 0.2017340362071991, 0.1994163691997528, 0.1957017183303833, 0.19547031819820404, 0.19114956259727478, 0.19324396550655365, 0.1886434555053711, 0.1865570992231369, 0.18735261261463165, 0.18816863000392914, 0.18178176879882812, 0.18076233565807343, 0.18042299151420593, 0.17961178719997406, 0.17701780796051025, 0.1769365668296814, 0.17569482326507568, 0.17357203364372253, 0.17379525303840637, 0.1720426231622696, 0.16929295659065247, 0.16911637783050537, 0.1666153520345688, 0.1662447452545166, 0.16429677605628967, 0.16287261247634888, 0.16216999292373657, 0.16156373918056488, 0.1590648740530014, 0.15728799998760223, 0.1565890610218048, 0.1559329628944397, 0.15434056520462036, 0.1523616760969162, 0.1529926359653473, 0.14898592233657837, 0.14922131597995758, 0.14926904439926147, 0.14770594239234924, 0.1459077149629593, 0.1461828052997589, 0.14381203055381775, 0.1435084491968155, 0.14185543358325958, 0.14245478808879852, 0.14044024050235748, 0.13700032234191895, 0.1387300193309784, 0.1370578408241272, 0.1373268961906433, 0.13520653545856476, 0.1359667330980301, 0.13363490998744965, 0.13438382744789124, 0.13034631311893463, 0.1305912733078003, 0.13102670013904572, 0.1302880197763443, 0.12749727070331573, 0.12855440378189087, 0.12461571395397186, 0.12376921623945236, 0.12417254596948624, 0.12405279278755188, 0.1233101338148117, 0.12322678416967392, 0.12064824253320694, 0.11911365389823914, 0.11806764453649521, 0.11834783852100372, 0.11978095769882202, 0.11677880585193634, 0.11659924685955048, 0.11590991169214249, 0.11521382629871368, 0.11321625858545303, 0.11610810458660126, 0.11431867629289627, 0.11230362206697464, 0.11193405091762543, 0.11217546463012695, 0.1113719567656517, 0.11031273007392883, 0.11091548204421997, 0.10919535160064697, 0.10819736123085022, 0.10720199346542358, 0.10716617107391357, 0.10548721998929977, 0.1056162416934967, 0.10492350906133652, 0.1054459661245346, 0.10320210456848145, 0.10136748105287552, 0.10246622562408447, 0.10133757442235947, 0.10087566077709198, 0.10106221586465836, 0.10052812844514847]\n",
      "Test Losses: [0.6752916574478149, 0.6635400056838989, 0.652053952217102, 0.6407946348190308, 0.6299166679382324, 0.6193371415138245, 0.6090415120124817, 0.5989547967910767, 0.5891205072402954, 0.5795331001281738, 0.5702129006385803, 0.5611193776130676, 0.5522927641868591, 0.5436650514602661, 0.5352071523666382, 0.5270051956176758, 0.5190030336380005, 0.5111340284347534, 0.5034798383712769, 0.49600958824157715, 0.4887034296989441, 0.4815518856048584, 0.4745517671108246, 0.4676944613456726, 0.46103039383888245, 0.4545080065727234, 0.44815367460250854, 0.441847562789917, 0.43572473526000977, 0.4297281801700592, 0.4238477349281311, 0.4181188642978668, 0.41248956322669983, 0.40700823068618774, 0.4016110301017761, 0.3963308334350586, 0.39116039872169495, 0.3861082196235657, 0.381112277507782, 0.3762606382369995, 0.37148287892341614, 0.36678797006607056, 0.36222076416015625, 0.35771021246910095, 0.35329359769821167, 0.3489374816417694, 0.3446730971336365, 0.34047842025756836, 0.3363642394542694, 0.33233681321144104, 0.32838010787963867, 0.3244885206222534, 0.3206843435764313, 0.3169337511062622, 0.3132340610027313, 0.3096148371696472, 0.3060639798641205, 0.3025740385055542, 0.2991531491279602, 0.2957823872566223, 0.2924845814704895, 0.28922295570373535, 0.2860094904899597, 0.28286004066467285, 0.2797587513923645, 0.2767098546028137, 0.2737385630607605, 0.27080076932907104, 0.2679141163825989, 0.26506155729293823, 0.2622568905353546, 0.2595070004463196, 0.2568051815032959, 0.2541542053222656, 0.25152865052223206, 0.24895484745502472, 0.24641123414039612, 0.24391046166419983, 0.2414507418870926, 0.2390289306640625, 0.23665452003479004, 0.23430708050727844, 0.23197956383228302, 0.22969894111156464, 0.227460578083992, 0.22524933516979218, 0.2230801284313202, 0.2209339439868927, 0.21881893277168274, 0.21674013137817383, 0.21469740569591522, 0.2126859724521637, 0.21070554852485657, 0.2087520807981491, 0.20682446658611298, 0.20492668449878693, 0.20305240154266357, 0.20120152831077576, 0.19937586784362793, 0.19757996499538422, 0.1958005726337433, 0.1940368413925171, 0.19231407344341278, 0.19061902165412903, 0.18893969058990479, 0.1872895061969757, 0.18565241992473602, 0.1840498149394989, 0.18246379494667053, 0.18089459836483002, 0.17934198677539825, 0.17782552540302277, 0.17632165551185608, 0.17483839392662048, 0.17338605225086212, 0.17194421589374542, 0.17051906883716583, 0.16910699009895325, 0.16772086918354034, 0.16634729504585266, 0.16498331725597382, 0.1636463850736618, 0.16232535243034363, 0.16101768612861633, 0.1597270667552948, 0.15844881534576416, 0.15719804167747498, 0.15595556795597076, 0.15472646057605743, 0.15352287888526917, 0.1523352563381195, 0.15115590393543243, 0.14999519288539886, 0.1488482803106308, 0.1477082222700119, 0.1465768814086914, 0.14546886086463928, 0.14437022805213928, 0.14328989386558533, 0.14222267270088196, 0.141170471906662, 0.14011983573436737, 0.13908807933330536, 0.13806290924549103, 0.13705086708068848, 0.13605013489723206, 0.13506439328193665, 0.13409146666526794, 0.13312678039073944, 0.13217437267303467, 0.13123594224452972, 0.13030728697776794, 0.12938432395458221, 0.12847375869750977, 0.1275712251663208, 0.12668029963970184, 0.12580180168151855, 0.12493100017309189, 0.12407156080007553, 0.12322518974542618, 0.12237908691167831, 0.12154728174209595, 0.12072807550430298, 0.11991532891988754, 0.11910790205001831, 0.11831189692020416, 0.11752013117074966, 0.11674074083566666, 0.11597245931625366, 0.11521516740322113, 0.11445997655391693, 0.11371024698019028, 0.11296971887350082, 0.11223815381526947, 0.11150906980037689, 0.11079331487417221, 0.11008753627538681, 0.10938193649053574, 0.10868635028600693, 0.10800071060657501, 0.10731854289770126, 0.10664273798465729, 0.10597457736730576, 0.10531286895275116, 0.10465427488088608, 0.10400016605854034, 0.10335743427276611, 0.10272375494241714, 0.10209675133228302, 0.10147456079721451, 0.10085970163345337, 0.10024996846914291, 0.09964331984519958, 0.09904656559228897, 0.09845862537622452, 0.09786780178546906, 0.09728493541479111, 0.09670822322368622, 0.09613455832004547, 0.09556926041841507]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Train Losses: {train_losses}\")\n",
    "print(f\"Test Losses: {test_losses}\")\n",
    "plt.plot(train_losses, 'r')\n",
    "plt.plot(test_losses, 'g')\n",
    "plt.legend(['Train Loss', 'Eval Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "PATH = \"state_dict_model.pt\"\n",
    "torch.save(second_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing params\n",
    "VOCAB_SIZE = int(test_inputs.max()) + 1\n",
    "TEST_VOCAB_SIZE = TRAIN_VOCAB_SIZE\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.088748, G-Mean=0.514\n",
      "Amount of test data: 540\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Accuracy Score: 0.4815517164145675\n",
      "\n",
      "Accuracy Score: 0.5166666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAHhCAYAAAAFwEUqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAowElEQVR4nO3dd5wV1d3H8c/PhoIiUgQUBUVKLLFhTWJMFEusxFgeSyRqNKIppqDPk9gToyYmdgWNoibRGI0lamJXULBrbFFARRFp0kQpop7nj5nFZdkDu7DL3cXP+/W6r7v3zJmZ312Y/d6ZOTM3UkpIkqSFrVDpAiRJaqoMSUmSMgxJSZIyDElJkjIMSUmSMgxJSZIyDElpCUXEFhHxYERMi4gUEWc00nr6l8vfuTGWvzwpf09DKl2Hlh+GpJqdiGgZET+JiGERMTUi5kXExIi4pwyUlZZBDSsBtwI9gFOBI4B/NPZ6KyUiupUBlCLirkyflSNictlnzFKsa//G+sAh1Vd4MwE1JxGxEXA30BN4ALgPeB9YG9i1fPwupTSwkevoCbwO/Cyl9IdGXteKwMrAxymlzxpzXYuooRvwFjCnrGW9lNL4Gn0OAG4p+0xMKXVbwnUNAY5MKcUSzLsq8GlKad6SrFuqqdE/cUsNJSJWA+4CNgQOSCnV3HM7LyK2AbZZBuV0Kp+nNvaKUkqfAp829nrq6C5gf4o95/NrTDsKeBFYEVh9WRVU/r+Yl1L6JKU0Z1mtV18MHm5Vc3IM0Au4oJaABCCl9HRK6fLqbeXhu8cj4qOI+LD8eb+a80bEmIh4JCJ6R8TdETEzImZExC0R0alav0eAR8uX11Y7DNltUecPy2WPqdG2Y0T8KyImRMSciBhXHjbevlqfWpcZEe0j4rKIGBsRH5fPl0VEuxr9qub/ZkT8PCLeiIi5ETEyIo6s7fe4CBOBe4Dv1VhHZ2B34NraZoqIbSNiSLnOWeXv9vGI6FfzdwQcWf6cqj36l21DytcdIuKaiJgIfAR0qTbPkGrLG1C2nVpjPeuUh4b/GxGt6vk70BeIe5JqTr5TPg+u6wwRMQC4DHgNOKts7g/cHhHHpZRqLmtd4BHgNuAXwObAcUBrYLeyz2+Ax4H/K2sZVrZPrvtbgYjoBdwPTAAuogigjsBXy/U+sYh51wSGAxsB1wDPAVsCxwPfjIhtU0oza8x2DrAaMAiYW/YdEhGjU0qP16P0ayh+fzuklEaUbUdS7O3+meLDTE39gN7AzcDbQLtynn9ExGEppb+W/X5D8eH9axR7q1WG11he1e/tbKAV8GFthaaULo+IXYDTI+LhlNJjEbEC8BdgDWDXlNJHdX/r+sJJKfnw0SwewBRgRj36r0Xxx3M00Lpae2vgDWAm0KZa+xggAQfVWM5lZXuvam07l239a/TtX7bvXEs9jwBjqr3+Udl328W8j4WWSREmCRhQo+8JZfvZtcz/PLBKtfZ1KcLyxjr8LruVy7iU4sP1BGBwtemvA7eUP79c/X2Wba1qWWbLcr5Xa7QPKf401VrHkLKOP2emJ2BILf8PxgDvlD+fWvY7sdL/p300/YeHW9WctKYItrrqS7GXcXFK6YOqxvLniynOm+1aY573Uko312h7qHzuUb9yF2tG+bxfOeCkPvpR7LnW3BMeVLb3W2gOuDyl9HHVi5TSOGAk9XxfKaVPgBuAgyNitYj4CsVAqmsWMc/8vbVydHI7ipB8CPhSRLSuTw3A7+tR7zTgUKAz8C/gdODOlNKl9VynvoAMSTUnH1AcIqurDcrnV2qZVtW2YY32N2vpO6V8blfLtKVxE8UI3f8DpkbEQxFxckR0rcO8GwCvl4E1X/l6JAu/L8i/tyV5X9dSfGg5gGLAznvAvbnOEbF2RAyudg7xfYow/0HZpU091z+yPp1TSsOB84DtyvUeVc/16QvKkFRz8jLQOiJqC4CGsqhRpHW5JGFR11QtMAYgpTQ3pdSX4g/3b8t1nwW8VnNASwPJvbd6X2qRUnoVeJLi8O5BwPWpGIW78MIjguJSnSOB64CDgT0o9vSrzkXW629RSmlWffpHxCoUA4sA2gLr12d+fXEZkmpObi2faxsYUpuqPadNapm2cY0+DaXqkpC2tUzboJY2UkpPpZTOLgNzI4o9rV8vZj1vAr1q3jihfN2Thn9ftbkG2J7isHX2UCvwZYqBSOemlAamlG5OKd2bUnqA4nKRmhrj4u3fAn2AgRRHJG5yVKvqwpBUc3I1xUCPn9d2CQdARGxdjmiFYgTkR8API2KNan3WAH5IMajn/gauseow4ALnOiPif4B1arS1r2X+dykOB9YWstXdDnRg4Q8M3y/bb6tbuUvlJuBM4McppVGL6Fe1h7nAHmtEbErt504/LKcv7ndQJxGxJ3AScF1K6XcUl6/0pBiEJC2Sl4Co2UgpzYqIvSnuuHN7RNxHEXJTKILhGxSH1M4v+0+PiIEUo1OfrHb9XH+KPbbjUkozaEAppdcj4gHguPIw4wvAFhRhMJribjVVfhURu1FcoP8WRYjsQ3GpRM0L9Ws6HzgQuCwitqIYubolcDTFB4nFzb/UygFQZ9Sh638pzgEPjIiqEa09KS6teQnYukb/J4ATgcsj4m5gHvBkSumt+tZYXr95HTCqXCYppbsi4iLgxxFxb0rppvouV18chqSalZTS6IjYkuIP7AHALykO900FnqE47/XXav0vj4jxFNc8nl42/wfol1K6vZHKPAK4BDis/HkYRYBfQXEpRZXbKUZcHkRxfeRsij/m3wf+tKgVpJRmlKNKzwT2pdg7mghcCZyeFr5GsmJSSp9GxF4UI1KPpBhx/HL58+YsHJI3UgT+IRQfBFageH/1CsnyesgbKK9xTSlVv5ZyILATMCgiliiA9cXgvVslScrwnKQkSRmGpCRJGYakJEkZhqQkSRmGpCRJGYakllhE7BERr0fE6Ig4pdL1SE1V+d2XkyLi5UrXovoxJLVEImJFiov096S4xdv/RMTGi55L+sIaQnG/WjUzhqSW1LbA6JTSm+XXL90E1HqrOOmLLqU0lM/v66tmxJDUkloXGFvt9btlmyQtNwxJSZIyDEktqXHAetVedynbJGm5YUhqST0N9IiIDcovtD0EuLPCNUlSgzIktURSSp9QfPXQvRRfhXRzSumVylYlNU0RcSMwguKLst+NiKMrXZPqxm8BkSQpwz1JSZIyDElJkjIMSUmSMgxJSZIyDElJkjIMSS21iDi20jVIzYHbSvNjSKohuOFLdeO20swYkpIkZTSrmwms2WattHandSpdhmqYMX0aa7ZZq9JlqIYVVlyp0iWohunTptJmrbaVLkM1jHzt1Q/Sp/PWrG1as9qK1u60DhcNvqnSZUjNQou2a1e6BKlZ2K3PRpNy0zzcKklShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVKGISlJUoYhKUlShiEpSVLGSpUuQE3D7FmzuPWmaxn12iuMfO0VPpgxjYOP+D7fPeaHC/WdPGkCf7n2Cl587kmmTZ3CWu3as2WfHTjku8fSYe1O2XXcd/dtXHT+6QDc+eBzrLiS//3UPM2e9RE3X3sZr7/8Aq+/8gIzpk3l0O//hKN+dEqt/d+fNIHrL/89Tw17kBnTprDmWu3o/eWt+MXZF9Jq9TUAePvNkdxwxQWMevVFpkyeyAorrEDn9bqxx/6HsPdB32XllVdZlm9RJf9KCYAPZkzjxusG0b5DR7r36M3zz4zI9JvOSccdyqeffsK39juItTt25p0xb/KvO//O008M5crrbqdlq9UXmm/mBzO4dtCFrLraasyZPbux347UqGZMm8INV/6BDh3XYaPem/HsiEezfd95cxQ//V4/WrZanb0PPIJ2HTsxfcoUXnnhKebOmT0/JCdPeI+ZM6az8x7706FjZz797FNeef5pLj/vVJ5/8jHOunjIMnp3qs6QFABt23Xg+lsfoF37tZk4fhxHHbJnrf2GPvRvpk19n9POuZjtvrLz/PaOndZh0CXn8dzTw/nqzrstNN+QwRexVtu2bLBRbx65/+7GehvSMtG2Q0duevAF2q/diQnj3uHwPbattV9Kid+ecgIdOq7DH4bcxmotW2WX2WfHnemz484LtO13yPdYo/Wa3HHTtYx9azTrbbBRQ74N1YHnJAXAyqusQrv2ay+236xZHwFFqFa3Vvm6xaqrLTTP6/99ifvu/gfH/egUVlxxxQaoVqqsVVZpQftFnFqo8vyTjzHqvy9y5Ak/Z7WWrZg7ZzafzJtXr3V1XHc9AD6c+cES1aqlU9GQjIg9IuL1iBgdEbUfzFeTsvlWxSfmKy/6La++/ALvT57I80+P4PqrL6H3xl9mqz47LND/s88+4/I//oYdd9qVzbfarhIlSxXzzPCHAVit5er8+Ih92GubDfhWn6787Khv8+bI/9Y6z5zZs5gxbQoT3xvLo/feyc3XXka7Dh3ZsOeXlmXpKlXscGtErAhcBvQF3gWejog7U0qvVqomLV6vL23GgJN+yfVXX8IvTvju/PZtd/w6J5923kKDce6542bGvv0mv/r1hcu4Uqny3h3zJgBn/ewYNttqe079/WCmTJ7IDVf+gZ99rx+Db32IDp3WWWCev117GTdcccH81z032ZyfnnFBrUdp1PgqeU5yW2B0SulNgIi4CdgPMCSbuPYdOtJ7k83ZYuvt6bxuF8a8MYpbbxrCWb/8MWf89lJWadECgOnTpnD91Zdw0OHHLHLUq7S8ml2entiw5yacceE189t7fGkzTuq/P3+/7koGnHzWAvP03ecgNt1yOz6YMZUXnnyct0b9lw9nzlimdetzlQzJdYGx1V6/C3g8rokb8djD/Pa0n3HJn26mazmIYPuvfIPuPb/EGSefwD133sz+Bx4BwDVX/IHWa7bhgIP7V7BiqXJarLoqAH33+c4C7ZttvT0d1+nCS88+sdA866zXlXXW6wrAN/bYn1uuH8Qpxx7CoFsfpOuGPRu/aC2gyQ/ciYhjI+KZiHhmxvRplS7nC++Ov/+ZdbqsPz8gq/TZ7qu0WHVVXn7hWQBGj3yVB+/9J/sfeARTp0xm4vhxTBw/jjmzZwEwaeJ4pk6ZvMzrl5aldh2KIyhtagx0A1ir/drM/GD6Ypexy7e+zSefzOPBu25t6PJUB5XckxwHrFftdZeybQEppcHAYIAevTdJy6Y05UydMqnW9s8++4z0WeKTTz8BYPLECQBcceE5XFFL/2MO3YuNem3MRYNvaqxSpYrrtekW3H3LDbw/cfxC096fMJ72HRd/GuLjj+cA1ClQ1fAqGZJPAz0iYgOKcDwEOLSC9agOuqy/AU8Nf5TXXn2R3ht/eX77Y4/cx8cfz6VHr40B6LXxZrUO1rnz1r/w4vNP839n/4HWrdsso6qlytjxG7tz2bm/4p5//IXd9z9k/iVQTzx6H+9PGs8e/Q6Z33falMnzL6Wq7p83Xw9A7023XDZFawEVC8mU0icRcSJwL7AicE1K6ZVK1SP45z9u5KMPZ/Lhh8X1WK++9Dw3XT8YgO2+sjMbdO/Jdw49imeffIxf/ew49tr/YDp17sKYN0fy73/eQtt2Hdhr/4OB4jrKHb72zYXWMeKxhwDYfsedvS2dmrXb//onPpz5AR+Vg2pefv5J/jzojwDsuPPubNhrY9q0bU//Ewcy6Pdn8vOjDuDru+/D+5MmcPtfr6bTuutzwBHHzV/ehWcN5IPp09h8mx3o0GldPpw5g2eHP8pzTwxlky22YZe9DqjI+/yiq+hfqZTSPcA9laxBn/vH365j0oT35r9+6YVneOmFZwBo16EjG3TvycabbsGFg2/ixuuu5NEH/8W0KZNZo3Ubvr7Lnhx+9Im0WatdpcqXlqm/X3cFE997d/7rF58ZwYvl7Rw7dOzMhuVRlQOPPJ7Wa67FrTcMZtAFZ9GyVSt26rsPR//kl6yxZpv58++8537cd/vf+NdtNzJj6hRWXmUV1uvWne+f9Cv6HXYMK6288jJ9fypESs3nNF+P3pskz2FJddOi7eLvoCQJduuz0ehP58zsUdu0Jj+6VZKkSjEkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScowJCVJyjAkJUnKMCQlScqoc0hGxLYR8f0abftFxEsRMS4izmn48iRJqpz67EmeDuxb9SIi1gduBDoBM4CTI+J7DVueJEmVU5+Q3Bx4rNrrQ4AAtkgpbQzcBxzbgLVJklRR9QnJdsDEaq93B4amlMaVr+8EejRUYZIkVVp9QnI60BEgIloA2wNDq01PwGoNVpkkSRW2Uj36vgAcExEPAP2AVYF7q03fgAX3NCVJatbqE5JnU5x3fIriXOT9KaVnqk3fG3iyAWuTJKmi6hySKaXhEbEVxbnIGcBNVdMioh1FgN7W4BVKklQh9dmTJKU0EhhZS/sU4KSGKkqSpKbAO+5IkpSR3ZOMiIeWYHkppbTLUtQjSVKTsajDrRtSXNYhSdIXUjYkU0rdlmEdkiQ1OZ6TlCQpw5CUJCmjXpeARMRawNHAdsBaLByyDtyRJC036hySEdEVeBxYh+JmAq2BqXwelu8DHzVCjZIkVUR9Drf+GmgD7ELxbR8BHEwRlr8FZgJfa+D6JEmqmPqE5C7AVSmlh/n80pBIKc1KKf0SeAk4r6ELlCSpUur7fZIvlz/PK5+rfzXW/UDfhihKkqSmoD4hORloW/48E5gDdKs2fRX8PklJ0nKkPiH5CrA5FENYKb4ya0BErB8R3YBjgdcavEJJkiqkPpeA3AH8LCJWSynNBs6i+NLlt8rpCfh2A9cnSVLF1Of7JC8HLq/2+qGI2AE4FPgUuC2lNLzhS5QkqTLqdTOBmlJKzwDPNFAtkiQ1Kd6WTpKkjPrcceeaOnRLKaWjl6IeSZKajPocbu1fhz6J4t6ukiQ1e3U+3JpSWqHmA1gZ6AVcBTxBcR9XSZKWC0t1TjKl9GlKaVRK6ThgCt6WTpK0HFmq0a01/Bs4HTi+AZe5gNarr0bfr2zaWIuXlisrRFS6BKlZWLPlytlpDTm6tS2wegMuT5KkilrqPcmIaAPsCpwEPLu0y5MkqamozyUgn/H5V2QtNJniC5h/2hBFSZLUFNRnT/J6Fg7JRBGOI4EbU0ozG6owSZIqrT73bu3fiHVIktTk1HngTkScFhHZoaURsUlEnNYwZUmSVHn1Gd16BvDlRUzflOISEEmSlgsNeQnIqsAnDbg8SZIqapHnJCOiNdCmWlO7iFi/lq5tgcOAsQ1XmiRJlbW4gTsnAVXnGRNwYfmoTQADG6QqSZKagMWF5CPlc1CE5W3AizX6JOBD4ImU0vAGrU6SpApaZEimlB4FHgWIiK7AlSmlJ5dFYZIkVVp9rpP8XmMWIklSU1Of6yRPiIgHFjH9vog4rmHKkiSp8upzCUh/YNQipo8EjlqqaiRJakLqE5I9gJcWMf2Vso8kScuF+oTkyhQ3DMhZdTHTJUlqVuoTkiOBvouYvhvwxtKVI0lS01GfkLwR2C0izo6IVaoaI2LliDiTIiT/2tAFSpJUKfX5Psk/AnsCvwSOj4jXyvbeFLelGwZc0LDlSZJUOXXek0wpzaPYWzwFeBfYsnyMpbgd3S4Ud+aRJGm5UK9vAUkpzUspnZ9S2iKl1Kp8bAk8DFwMvNcoVUqSVAH1Ody6gIhoCxxOcW3kZhR7kSMbqC5Jkiqu3t8nGRG7R8TfgHEU5ylbAGcCm6WUejdwfZIkVUyd9iQjohvFHuORQBfgfeAW4FDglymlfzRWgZIkVcoi9yQj4rCIeBAYDZwMPAP0A9YFzsCBOpKk5dji9iRvAN4EfgLcmFKaUjUhwnyUJC3fFndOci7QDdgP2CMiVmv0iiRJaiIWF5KdKfYi21HsVU6IiD9FxE54qFWStJxbZEimlKanlC5NKW0F9AH+THFO8mHgMSABazZ6lZIkVUB97rjzXErpBIq9yyMovhoL4OqIeCEifhURmzRGkZIkVUK9r5NMKc1NKf01pbQL0B34DbAWcBbwnwauT5Kkiql3SFaXUhqTUjqNYnDPtwCvl5QkLTeW+LZ01aWUEvDv8iFJ0nJhqfYkJUlanhmSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJJapLfffpvvHnE463RamzVarcbWW23B9dcNWajf2LFjOfb7R9Nzow1pvXpLevXozoDjj2Ps2LHLvmipAt5++22OOPwwOnXsQKuWq7LVlptz3ZAhC/QZM2YMK60YtT6O/f4xlSlci7RSpQtQ0zVu3Di+uuP2zJkzhwEnnEjnzp25+667OOboo5g+fTo/+vFPAJgyZQpf2WE75s2bx7HH/YCuXbvy31dfZfDgQfzrnnv4z0uv0Lp168q+GakRjRs3jh132I45c+Zwwok/pHPnztz1z39y9NHfY/qM6fy43Faq7LvvfhxwwHcWaOu+0UbLsGLVlSGprN+dfy6TJk3i0aGPsf0OOwDwg+MH8O1++3P6aady2OFH0K5dO/5+89+YMGEC/7jtDvbeZ5/583ft1o2fnvQT7r//voX+IEjLk/PPK7aVocMeZ4dyWzn++AH0238/Tjv1VxxebitVNtl0Uw47/PBKlat68HCrsoYNG0b37t3nB2SVQw87jI8++og777gdgA8++ACAzp07L9CvU/m65WotG79YqYKGDRtK9+7d5wdklcMOO5yPPvqIO26/faF5Zs+ezezZs5dRhVpSFQvJiLgmIiZFxMuVqkGLNnfuXFZruXDAtWrZCoBnn30GgG9845sA/OTHP2LE8OGMGzeOB+6/n9NO/RXbbbc9fXfbbdkVLVXA3LlzaVnLttKy1YLbSpVLLr6INVZvyRqrt6R3rx5cccXly6RO1V8l9ySHAHtUcP1ajF69ejPy9deZMGHCAu2PPvIwAOPGvQfANttuyyWXXsbIka/z9Z2+ygZd1+Nbe+5Or169+fd997PSSh7V1/KtV6/evF7LtvLI/G1lHAArrLAC39xlF8757bncdvudXH75lbRp04YfnngCAwf+YpnXrcWrWEimlIYCUyu1fi3e8ccPYO7cuRx84HcYMXw4b731FpdecjGDBw8CYPasWfP7rrtuF7bffgd+9/sLuPW22znjzLMYNvRRDui3P3PmzKnUW5CWieMHnMDcuXM58MADGF5uK5dccjGDB10JwKzZxbay/vrrc999DzBgwAnss88+HHvccTw+/Am+ttNOXPjHP/DGG29U8m2oFn7EV9auffsyaPBVnDzwF3x9p68C0KZNGy6+9DKO6n8kq6+xBgB33nkHhxx0IE8/+zybbLIJAPvssy9bbrkV++27N4MHXTl/JKy0POrbty+DB1/NwIE/Z6evfQUotpVLL72c/v2/yxqrr5Gdd8UVV+SnP/05w4YO5aEHH6R79+7LqmzVQZMfuBMRx0bEMxHxzPuTJ1e6nC+c7x11NGPHjWf4iCcZOuxx3h47jj59tgGgR48eAFxy0UVs1KPH/ICssseee9KyZUuGDh26zOuWlrWjjj6ace9NYMQTTzHsseGMffc9+mxTbis9ey5y3q5duwLw/pT3G71O1U+T35NMKQ0GBgNs3adPqnA5X0gtWrSYv7EDPHD/fQD07VsMyHlv/Hu1zvfZZ5/x2Wef8cm8eY1fpNQEtGjRgm2qbSv319hWct4YPRqAtTus3XjFaYk0+T1JNS3jx4/nd+efx1Zbb803vlmMau3VqzejR43iqSefXKDvLbf8nTlz5rDV1ltXolSposaPH8/5553L1ltvzTfLbWXSpEkL9ZszZw7nnnsOK620kiPBm6CK7UlGxI3AzkD7iHgXOD2l9KdK1aOFTZgwgX32/hb77rsfXbp04Z133uHqqwaTUmLIdTcQEQD84hcDufff/2LPPXbjuB8cz4YbbshLL73I1VddRefOnfnB8QMq/E6kxjVhwgT23mtP9t1vf7qs24V3xr7DVYMHkVLiuuv/PH9bOeXkgbw+8nV23bUv63VZjwkTJ/CXP9/AqFGjOOvsX7P++utX+J2opoqFZErpfyq1btXN6quvzgYbbMg1f7qaSZMm0b59e761116cetoZdOnSZX6/HXbckRFPPs1vfn02N//tJsaPH0+7du04+JD/4Ywzz2LttT2EpOVb1bbyp6uvmr+t7LXX3px2+oLbym677c7b77zN1VcNZurUqbRs2ZItttySc845l37f/nYF34FyIqXmc5pv6z590hNPPl3pMqRmYYVy70XSonVo33b01KlTe9Q2zXOSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZRiSkiRlGJKSJGUYkpIkZURKqdI11FlETAbernQdWkh74P1KFyE1A24rTVPXlFKH2iY0q5BU0xQRz6SU+lS6Dqmpc1tpfjzcKklShiEpSVKGIamGMLjSBSzPIqJbRKSIOGNRbY21LjUot5VmxpDUUkspLZcbfkTsXAZG9ceHEfFsRPw4IlasdI1LogzCMyJii0rX8kWzvG4ry7OVKl2A1AzcCNwDBLAO0B+4ENgEOLZCNb0NrAZ8sgTzdgNOB8YALzTgcqXljiEpLd5zKaU/V72IiCuA/wLHRMSpKaWJNWeIiDVSSjMbq6BUDEuf01yWKzVXHm6V6iml9AEwgmLPcsOIGBMRj0TElhFxb0TMAF6s6h8RPSLihogYHxEfl/1/FxGtai47Ir4aEY9HxOyImBgRlwKr19Ive+4wIg4o65keEbMi4vWIuDgiVomI/sDDZddrqx1GfmRRy42IlSLi5Ih4NSLmRMSUiLgtIjbL1RURe0fE02X/8eV7XqlG/00i4u8RMS4i5kbEhIh4OCL2qsM/hdTo3JOU6ikiAtiofFl1Yfj6wEPA34FbKYMtIrYu26cDg4BxwObAj4CvRMTXU0rzyr7bAQ8AM4HzynkOAa6vR22/Af4PeBX4IzAe6A4cAJwGDAXOKfsMBoaVsy60N1zDX4CDgPuBK4BOwAnAiIj4Wkrp+Rr9vwUMAK4ErgH2A34OTCvXT0S0o/jdUPZ7m+Ji+z7AdsDddX3fUqNJKfnw4aOWB7AzkCjCpT3QAfgycFXZPqLsN6Z8fUwty/gP8BqwRo32fuU8/au1DQc+BnpWa1sFeKrse0a19m61tG1btj0ErFpjfcHnNw/Zuea6F7PcvmXb36qWUbZvTnHuclgt838EdKux/peB8dXa9i37HlTpf2sfPnIPD7dKi3cmMBmYRBF6RwF3AvtX6zMVuLb6TOWhyC8DfwVaRET7qgfwGEWQ7Fb2XRvYAbgjpTSyahkppY8p9gjr4rDy+X9TSgucV0ylOi6npn7l82+qLyOl9B/gn8BXI6LmLb1uTymNqb5+isO8nSKi6vDxjPJ5z4hovYS1SY3KkJQWbzDF3tSuFEHWIaW0X1pwwM4bKaVPa8z3pfK5KmSrPyYBrYCOZZ8Ny+fXaln/q3WsswfFntl/6ti/rjYAPqMYrFTTK9X6VPdmLX2nlM/tAFJKj1IcSu4PvF+eiz0zIjZe6oqlBuI5SWnxRqWUHlhMn1m1tEX5fAHw78x805a4qtql8lFpNT8wVFf1eyGldGRE/A7YE/ga8DPglxHxk5TSpY1co7RYhqTUeEaVz5/WIWTfKp971zKtrntWIynCZnOK85g59Q3RNymOOn2JaqN2a9T2FksopfQyxfnK30VEG+BJ4NyIuGwpDhFLDcLDrVLjeZ7ij/8PImLDmhPLyyraApSHbp8A9ouIntX6rAKcVMf1/bV8Pqecr+b6qvbgPiyf29ZxubeXz/9bbRlExKYUg28eSylNruOyqtfTNiIW+BuUUppOEbgtgVXru0ypobknKTWSlFKKiCMoRpu+GBHXUJzDa0lxCcm3gf8FhpSz/BR4BHg8Ii7j80tA6rSdppSeiojzgJOB5yLib8AEivOF36EY/Tqd4hznTGBARMwq2yallB7KLPf+iLi5rGWtiLiLzy8BmUNxOcuS+C5wUkTcBowG5gFfB3YHbk4pzV7C5UoNxpCUGlFK6YWI2JIiDPcFfkARUGMowvHBan1HRERf4FzgFIrRn7dQXJf4Uh3Xd0pE/Ac4ERhIcbRoLMVt9WaVfWZHxCHArylur9cCeJTPr1mszWHAcxSDbC6gGJn7KHBqSqlOtdXiEWBLYG+gM8V5zLcorqf0fKSaBL90WZKkDM9JSpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKUYUhKkpRhSEqSlGFISpKU8f8+jEGLC+i5GwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 540x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix: \n",
      "[[184 163]\n",
      " [ 98  95]]\n",
      "\n",
      "TN: 184\t FP: 163\n",
      "FN: 98\t TP: 95\n",
      "\n",
      "Precision: 0.3682170542635659\n",
      "Recall:0.49222797927461137\n",
      "F1 Score: 0.4212860310421286\n"
     ]
    }
   ],
   "source": [
    "# Load Second Model\n",
    "PATH = \"state_dict_model.pt\"\n",
    "load_model = GRU(vocab_size=TEST_VOCAB_SIZE, \n",
    "                hidden_dim=HIDDEN_DIM, \n",
    "                embedding_dim=EMBEDDING_DIM, \n",
    "                dropout=DROPOUT)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "\n",
    "# Test Model\n",
    "from sklearn.metrics import f1_score\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(second_model.parameters(), lr=lr)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "    eval_losses = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "        output, _ = second_model(inputs.to(torch.long), h)\n",
    "        # print(f\"Output Shape:{output.shape}\")\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        eval_losses.append(loss)\n",
    "        preds = output.squeeze()\n",
    "        if len(labels) > 1:\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.numpy().astype(int))\n",
    "\n",
    "\n",
    "roc_acc_score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels_list, test_preds)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# Index of largest G-means\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "threshold = thresholds[ix]\n",
    "\n",
    "# Print how many data is being tested\n",
    "print(f\"Amount of test data: {len(test_labels_list)}\")\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "print(f\"ROC Accuracy Score: {roc_acc_score}\")\n",
    "\n",
    "# Normalize probability with threshold\n",
    "test_preds_thresholded = np.where(test_preds > threshold, 1, 0)\n",
    "for i in range(len(test_preds)-1140):\n",
    "    print(\"Test Preds Prob: {}    \\\n",
    "    Test Preds Label: {}  \\\n",
    "    True Label: {}  \\\n",
    "    \".format(test_preds[i], test_preds_thresholded[i], test_labels_list[i]))\n",
    "\n",
    "acc_score = accuracy_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"\\nAccuracy Score: {acc_score}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_labels_list, test_preds_thresholded)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix: \\n{cm}\")\n",
    "\n",
    "tn = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tp = cm[1][1]\n",
    "print(f\"\\nTN: {tn}\\t FP: {fp}\\nFN: {fn}\\t TP: {tp}\\n\")\n",
    "\n",
    "# Precision\n",
    "precision = tp/(tp+fp)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# Recall\n",
    "recall = tp/(tp+fn)\n",
    "print(f\"Recall:{recall}\")\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_score = f1_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: selamat no terdaftar meraih cek juta pinwha info klik bitlywhatsappjkt\n",
      "\n",
      "Words: ['selamat', 'no', 'terdaftar', 'meraih', 'cek', 'juta', 'pinwha', 'info', 'klik', 'bitlywhatsappjkt']\n",
      "\n",
      "Tokenization Result: [933, 3772, 823, 4197, 2982, 408, 3728, 1319, 2912, 1373]\n",
      "('Messages: [[933, 3772, 823, 4197, 2982, 408, 3728, 1319, 2912, 1373]]\\n'\n",
      " 'Features: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]')\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0  933 3772  823 4197 2982  408 3728 1319\n",
      "  2912 1373]]\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  933, 3772,  823, 4197,\n",
      "         2982,  408, 3728, 1319, 2912, 1373]], dtype=torch.int32)\n",
      "Infer Sentence: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  933, 3772,  823, 4197,\n",
      "         2982,  408, 3728, 1319, 2912, 1373]], dtype=torch.int32)\n",
      "\n",
      "Output: 0.11551926285028458\n",
      "Sentence is a Scam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_threshold = threshold\n",
    "\n",
    "# Load Second Model\n",
    "infer_model = GRU(vocab_size=4281, \n",
    "                hidden_dim=10, \n",
    "                embedding_dim=50, \n",
    "                dropout=0.2)\n",
    "infer_model.load_state_dict(torch.load(\"state_dict_model.pt\"))\n",
    "infer_model.eval()\n",
    "\n",
    "sentence = input()\n",
    "\n",
    "def pre_process(text):\n",
    "    text = clean_text(text)\n",
    "    print(f\"Text: {text}\\n\")\n",
    "\n",
    "    words = text.split()\n",
    "    print(f\"Words: {words}\\n\")\n",
    "\n",
    "    token = lambda x: tokenize (x, word_to_idx)\n",
    "    token_result = token(text)\n",
    "    print(f\"Tokenization Result: {token_result}\")\n",
    "\n",
    "    arr = []\n",
    "    arr.append(token_result)\n",
    "    inputs = pad_and_truncate(arr)\n",
    "    print(inputs)\n",
    "\n",
    "    input = torch.tensor(inputs)\n",
    "    print(input)\n",
    "    return input\n",
    "\n",
    "with torch.no_grad():\n",
    "    infer_sentence = pre_process(sentence)\n",
    "    print(f\"Infer Sentence: {infer_sentence}\")\n",
    "    h = torch.Tensor(np.zeros((1, 10)))\n",
    "\n",
    "    output, _ = infer_model(infer_sentence.to(torch.long), h)\n",
    "    print(f\"\\nOutput: {output.squeeze()}\")\n",
    "\n",
    "    if output > model_threshold:\n",
    "        print(\"Sentence is a Scam\")\n",
    "    else:\n",
    "        print(\"Sentence is NOT a Scam\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "27726a6b9d0b3aecf19ddb8fb5d165165384e9a9dccccc704489801e8c9c2418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('spam_classifier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.160008Z",
     "start_time": "2019-06-03T19:33:39.344527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:cuda\n",
      "Torch Ver: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import sqrt, argmax\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "# import opacus\n",
    "# from opacus import PrivacyEngine\n",
    "\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# from syft.frameworks.torch.nn import GRU\n",
    "from handcrafted_GRU import GRU\n",
    "# from opacus.layers import DPGRU\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device:{device}\")\n",
    "\n",
    "print(f\"Torch Ver: {torch.__version__}\")\n",
    "# print(f\"Opacus Ver: {opacus.__version__}\")\n",
    "# print(f\"Syft Ver: {sy.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('indonesian'))\n",
    "# print(f\"STOPWORDS:\\n {STOPWORDS}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # print(f\"\\n\\nOriginal Text: {text}\")\n",
    "    text = text.lower()\n",
    "#     print(f\"\\nCase Lowered Text: {text}\")\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "#     print(f\"\\nRegexed Text: {text}\")\n",
    "    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n",
    "#     print(f\"\\nStopwords Removed Text: {text}\")\n",
    "    return text\n",
    "\n",
    "def tokenize(text, word_to_idx):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        if word in word_to_idx:\n",
    "            tokens.append(word_to_idx[word])\n",
    "        else:\n",
    "            tokens.append(0)\n",
    "    return tokens\n",
    "\n",
    "def pad_and_truncate(messages, max_length=30):\n",
    "    features = np.zeros((len(messages), max_length), dtype=int)\n",
    "#     pprint(f\"Messages: {messages}\\nFeatures: {features}\")\n",
    "    for i, sms in enumerate(messages):\n",
    "        # print(f\"\\ni: {i}\\nSMS:{sms}\")\n",
    "        if len(sms):\n",
    "            features[i, -len(sms):] = sms[:max_length]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('dataset_sms_spam_v1.csv', sep=',', names=['Teks', 'Label'], encoding= 'unicode_escape')\n",
    "    data = data.sample(frac = 1)\n",
    "    #Lowercase, remove unnecessary char with regex, remove stop words\n",
    "    data.Teks = data.Teks.apply(clean_text)\n",
    "    # print(data.Teks)\n",
    "    words = set((' '.join(data.Teks)).split())\n",
    "#     print(words)\n",
    "    word_to_idx = {word: i for i, word in enumerate(words, start=1)}\n",
    "    # pprint(word_to_idx)\n",
    "    tokens = data.Teks.apply(lambda x: tokenize(x, word_to_idx))\n",
    "    # print(tokens)\n",
    "    inputs = pad_and_truncate(tokens)\n",
    "    # pprint(inputs)\n",
    "    labels = np.array((data.Label == '1').astype(int))\n",
    "\n",
    "    np.save('labels.npy', labels)\n",
    "    np.save('inputs.npy', inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.172746Z",
     "start_time": "2019-06-03T19:33:40.163793Z"
    }
   },
   "outputs": [],
   "source": [
    "original_inputs = np.load('inputs.npy')\n",
    "original_labels = np.load('labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.207763Z",
     "start_time": "2019-06-03T19:33:40.201292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training params\n",
    "TRAIN_VOCAB_SIZE = int(inputs.max()) + 1\n",
    "EPOCHS = 30\n",
    "CLIP = 5 # gradient clipping - to avoid gradient explosion (frequent in RNNs)\n",
    "lr = 0.1\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# # Privacy Engine Hyperparameters\n",
    "MAX_GRAD_NORM = 1.2\n",
    "NOISE_MULTIPLIER = 1.3\n",
    "EPSILON = 50.0\n",
    "# Delta value must be less than inverse of data amount. e.g: 100 data require leak probability value (delta) < 1/100\n",
    "DELTA = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.197935Z",
     "start_time": "2019-06-03T19:33:40.186270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels: [:-356]\n",
      "Train Inputs: [:-356]\n",
      "Test Labels: [-356:]\n",
      "Test Inputs: [-356:]\n",
      "Length Labels: 1781\n",
      "20% of Length Labels: 356.20000000000005\n",
      "1781\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(original_inputs)\n",
    "labels = torch.tensor(original_labels)\n",
    "\n",
    "# splitting training and test data\n",
    "# 20% of the data will be for Tests.\n",
    "pct_test = 0.2\n",
    "\n",
    "#20% of total data\n",
    "pct_test_count = -int(len(labels)*pct_test)\n",
    "\n",
    "# Get 80% of Train LABELS from left.\n",
    "train_labels = labels[:pct_test_count]\n",
    "print(f\"Train Labels: [:{pct_test_count}]\")\n",
    "\n",
    "# Get 80% of Train INPUTS from left.\n",
    "train_inputs = inputs[:pct_test_count]\n",
    "print(f\"Train Inputs: [:{pct_test_count}]\")\n",
    "\n",
    "# Get the rest of the LABEL data for test on the right (20%) \n",
    "test_labels = labels[pct_test_count:]\n",
    "print(f\"Test Labels: [{pct_test_count}:]\")\n",
    "\n",
    "# Get the rest of the INPUT data for test on the right (20%)\n",
    "test_inputs = inputs[pct_test_count:]\n",
    "print(f\"Test Inputs: [{pct_test_count}:]\")\n",
    "\n",
    "print(f\"Length Labels: {len(labels)}\")\n",
    "print(f\"20% of Length Labels: {len(labels)*pct_test}\")\n",
    "\n",
    "SAMPLE_SIZE = len(labels)\n",
    "print(SAMPLE_SIZE)\n",
    "\n",
    "# For Local Model Evaluation\n",
    "original_test_inputs = original_inputs[pct_test_count:]\n",
    "original_test_labels = original_labels[pct_test_count:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VirtualWorkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.591430Z",
     "start_time": "2019-06-03T19:33:41.969220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hook that extends the Pytorch library to enable all computations with pointers of tensors sent to other workers\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Creating 2 virtual workers Syft v0.2.9\n",
    "anne = sy.VirtualWorker(hook, id=\"anne\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "\n",
    "workers = [anne, bob]\n",
    "\n",
    "# this is done to have the local worker (you on your notebook!) have a registry\n",
    "# of objects like every other workers, which is disabled by default but needed here\n",
    "# sy.local_worker.is_client_worker = False\n",
    "\n",
    "\n",
    "# threshold indexes for dataset split (one half for Bob, other half for Anne)\n",
    "train_idx = int(len(train_labels)/2)\n",
    "test_idx = int(len(test_labels)/2)\n",
    "\n",
    "\n",
    "# Sending toy datasets to virtual workers\n",
    "bob_train_dataset = sy.BaseDataset(train_inputs[:train_idx], train_labels[:train_idx]).send(bob)\n",
    "anne_train_dataset = sy.BaseDataset(train_inputs[train_idx:], train_labels[train_idx:]).send(anne)\n",
    "bob_test_dataset = sy.BaseDataset(test_inputs[:test_idx], test_labels[:test_idx]).send(bob)\n",
    "anne_test_dataset = sy.BaseDataset(test_inputs[test_idx:], test_labels[test_idx:]).send(anne)\n",
    "\n",
    "\n",
    "# Creating federated datasets, an extension of Pytorch TensorDataset class for TRAINING METHOD #1\n",
    "bob_federated_train_dataset = sy.FederatedDataset([bob_train_dataset])\n",
    "anne_federated_train_dataset = sy.FederatedDataset([anne_train_dataset])\n",
    "bob_federated_test_dataset = sy.FederatedDataset([bob_test_dataset])\n",
    "anne_federated_test_dataset = sy.FederatedDataset([anne_test_dataset])\n",
    "\n",
    "\n",
    "merged_test_dataset = list(zip(original_test_inputs, original_test_labels))\n",
    "# print(f\"Input:{original_test_inputs[1]}\\t Label:{original_test_labels[1]}\")\n",
    "# print(merged_test_dataset[0])\n",
    "\n",
    "def collate_batch(batch):\n",
    "        label_list, text_list = [], []\n",
    "        for (_label, _text) in batch:\n",
    "                label_list.append(_label)\n",
    "                text_list.append(_text)\n",
    "        return label_list, text_list\n",
    "\n",
    "original_test_dataloader = DataLoader(merged_test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.638046Z",
     "start_time": "2019-06-03T19:33:42.617601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiating the model\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# model = GRU(vocab_size=TRAIN_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "# torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "def make_model():\n",
    "    model = GRU(vocab_size=TRAIN_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "    # model = DPGRU(hidden_size=HIDDEN_DIM, input_size=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "    return model\n",
    "    \n",
    "local_model = make_model()\n",
    "\n",
    "models, train_dataloaders, test_dataloaders, optimizers, privacy_engines = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching model, dataloaders, optimizers, and privacy engine to each worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in workers:\n",
    "    model = make_model()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    model.send(worker)\n",
    "    if(worker == anne):\n",
    "        train_dataset = anne_federated_train_dataset\n",
    "        test_dataset = anne_federated_test_dataset\n",
    "    elif(worker == bob):\n",
    "        train_dataset = bob_federated_train_dataset\n",
    "        test_dataset = bob_federated_test_dataset\n",
    "\n",
    "\n",
    "    train_dataloader = sy.FederatedDataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dataloader = sy.FederatedDataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    models.append(model)\n",
    "    train_dataloaders.append(train_dataloader)\n",
    "    test_dataloaders.append(test_dataloader)\n",
    "    optimizers.append(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to aggregate remote models and to send new updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_new_models(local_model, models):\n",
    "    with torch.no_grad():\n",
    "        for remote_model in models:\n",
    "            for new_param, remote_param in zip(local_model.parameters(), remote_model.parameters()):\n",
    "                worker = remote_param.location\n",
    "                remote_value = new_param.send(worker)\n",
    "                remote_param.set_(remote_value)\n",
    "\n",
    "def federated_aggregation(local_model, models):\n",
    "    with torch.no_grad():\n",
    "        for local_param, *remote_params in zip(*([local_model.parameters()] + [model.parameters() for model in models])):\n",
    "            param_stack = torch.zeros(*remote_params[0].shape)\n",
    "            for remote_param in remote_params:\n",
    "                param_stack += remote_param.copy().get()\n",
    "            param_stack /= len(remote_params)\n",
    "            local_param.set_(param_stack)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [[], []]\n",
    "test_losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    # 1. Send new version of the model\n",
    "    send_new_models(local_model, models)\n",
    "\n",
    "    # 2. Train remotely the models\n",
    "    for i, worker in enumerate(workers):\n",
    "        train_dataloader = train_dataloaders[i]\n",
    "        model = models[i]\n",
    "        optimizer = optimizers[i]\n",
    "        \n",
    "        model.train()\n",
    "        criterion = nn.BCELoss() # for two class classification\n",
    "        losses = []   \n",
    "    \n",
    "        for data, target in train_dataloader:            \n",
    "            data = data.to(torch.long)\n",
    "            h = torch.Tensor(torch.zeros(BATCH_SIZE, HIDDEN_DIM)).send(worker)  \n",
    "            \n",
    "            # Call zero grad to clear previous gradient before every training passses.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print(f\"Data:{data}\\nTarget: {target}\\n\")\n",
    "\n",
    "            # print(f\"Worker: {worker}\\nWorker Objects: {worker.object_store._objects}\")\n",
    "\n",
    "            output, _ = model(data.to(torch.long), h)\n",
    "            loss = criterion(output.squeeze(), target.float())\n",
    "            loss.backward()\n",
    "\n",
    "            # # Clipping the gradient to avoid explosion\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            losses.append(loss.get()) \n",
    "            optimizer.step()\n",
    "\n",
    "        sy.local_worker.clear_objects()\n",
    "        \n",
    "\n",
    "        train_loss = sum(losses) / len(losses)\n",
    "        train_losses[i].append(train_loss.item())\n",
    "\n",
    "        print(\n",
    "            f\"[{worker.id}]\\t\"\n",
    "            f\"Train Epoch: {epoch} \\t\"\n",
    "            f\"Train Loss: {train_loss:.4f} \")\n",
    "\n",
    "    # 3. Federated aggregation of the updated models\n",
    "    federated_aggregation(local_model, models)\n",
    "\n",
    "\n",
    "def eval(epoch, last_loss, trigger_times, patience):\n",
    "    # 4. Evaluate the model\n",
    "    local_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        test_labels_list = []\n",
    "        eval_losses = []\n",
    "\n",
    "        for inputs, labels in original_test_dataloader:\n",
    "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "            output, _ = local_model(torch.LongTensor(inputs), h)\n",
    "            criterion = nn.BCELoss()\n",
    "            labels = torch.LongTensor(labels)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            eval_losses.append(loss)\n",
    "            preds = output.squeeze()\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.numpy().astype(int))\n",
    "    \n",
    "    score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "    eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    test_losses.append(eval_loss.item())\n",
    "\n",
    "            \n",
    "    # Early Stopping\n",
    "    if eval_loss > last_loss:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times: {trigger_times}\")\n",
    "        \n",
    "        if trigger_times >= patience:\n",
    "            print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Trigger Times: 0\")\n",
    "        trigger_times = 0\n",
    "    \n",
    "    last_loss = eval_loss\n",
    "\n",
    "    print(\n",
    "        f\"Eval Epoch: {epoch} \\t\"\n",
    "        f\"AUC: {score:.3%} \\t\"\n",
    "        f\"Eval Loss: {eval_loss:.4f} \\n\\n\")\n",
    "\n",
    "    return last_loss, trigger_times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[anne]\tTrain Epoch: 0 \tTrain Loss: 0.5591 \n",
      "[bob]\tTrain Epoch: 0 \tTrain Loss: 0.5425 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 0 \tAUC: 50.329% \tEval Loss: 0.5845 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 1 \tTrain Loss: 0.5500 \n",
      "[bob]\tTrain Epoch: 1 \tTrain Loss: 0.5336 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 1 \tAUC: 50.333% \tEval Loss: 0.5842 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 2 \tTrain Loss: 0.5398 \n",
      "[bob]\tTrain Epoch: 2 \tTrain Loss: 0.5230 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 2 \tAUC: 50.792% \tEval Loss: 0.5840 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 3 \tTrain Loss: 0.5320 \n",
      "[bob]\tTrain Epoch: 3 \tTrain Loss: 0.5204 \n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'trigger_times' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-127013b5c278>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mlast_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrigger_times\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrigger_times\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"EARLY STOPPING! STARTING TEST PROCESS...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-b25436390f89>\u001b[0m in \u001b[0;36meval\u001b[1;34m(epoch, last_loss)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;31m# Early Stopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meval_loss\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlast_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mtrigger_times\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Trigger Times: {trigger_times}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'trigger_times' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# For Early Stopping\n",
    "last_loss = 100\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    last_loss, trigger_times = eval(epoch, last_loss, trigger_times, patience)\n",
    "    if trigger_times >= patience:\n",
    "        print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses[0], 'r')\n",
    "plt.plot(train_losses[1], 'g')\n",
    "plt.plot(test_losses, 'b')\n",
    "plt.legend(['Training Loss Anne', 'Training Loss Bob' , 'Eval Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "PATH = \"local_state_dict_model.pt\"\n",
    "torch.save(local_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('manually_collected_sms_600.csv', sep=',', names=['Teks', 'Label'])\n",
    "data = data.sample(frac = 1)\n",
    "# Lowercase, remove unnecessary char with regex, remove stop words\n",
    "data.Teks = data.Teks.apply(clean_text)\n",
    "#     print(data.Teks)\n",
    "words = set((' '.join(data.Teks)).split())\n",
    "#     print(words)\n",
    "word_to_idx = {word: i for i, word in enumerate(words, start=1)}\n",
    "#     pprint(word_to_idx)\n",
    "tokens = data.Teks.apply(lambda x: tokenize(x, word_to_idx))\n",
    "#     print(tokens)\n",
    "inputs = pad_and_truncate(tokens)\n",
    "#     pprint(inputs)\n",
    "labels = np.array((data.Label == '1').astype(int))\n",
    "\n",
    "np.save('test_labels.npy', labels)\n",
    "np.save('test_inputs.npy', inputs)\n",
    "\n",
    "test_inputs = torch.tensor(np.load('test_inputs.npy'))\n",
    "test_labels = torch.tensor(np.load('test_labels.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing params\n",
    "VOCAB_SIZE = int(test_inputs.max()) + 1\n",
    "TEST_VOCAB_SIZE = TRAIN_VOCAB_SIZE\n",
    "lr = 0.001\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"local_state_dict_model.pt\"\n",
    "load_model = GRU(vocab_size=TEST_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(load_model.parameters(), lr=lr)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "    eval_losses = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "\n",
    "        output, _ = load_model(inputs.to(torch.long), h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        eval_losses.append(loss)\n",
    "        preds = output.squeeze()\n",
    "        test_preds += list(preds.numpy())\n",
    "        test_labels_list += list(labels.numpy().astype(int))\n",
    "\n",
    "\n",
    "roc_acc_score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels_list, test_preds)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# Index of largest G-means\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "threshold = thresholds[ix]\n",
    "\n",
    "# Print how many data is being tested\n",
    "print(f\"Amount of test data: {len(test_labels_list)}\")\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "print(f\"ROC Accuracy Score: {roc_acc_score}\")\n",
    "\n",
    "# Normalize probability with threshold\n",
    "test_preds_thresholded = np.where(test_preds > threshold, 1, 0)\n",
    "for i in range(len(test_preds)-1140):\n",
    "    print(\"Test Preds Prob: {}    \\\n",
    "    Test Preds Label: {}  \\\n",
    "    True Label: {}  \\\n",
    "    \".format(test_preds[i], test_preds_thresholded[i], test_labels_list[i]))\n",
    "\n",
    "acc_score = accuracy_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"\\nAccuracy Score: {acc_score}\")\n",
    "\n",
    "# Calculate F1 Score\n",
    "# f1_score = f1_score(test_labels_list, test_preds_thresholded)\n",
    "# print(f\"F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, PySyft does not support optimizers with momentum. Therefore, we are going to stick with the classical [Stochastic Gradient Descent](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) optimizer.\n",
    "\n",
    "As our task consists of a binary classification, we are going to use the [Binary Cross Entropy Loss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T20:00:23.084933Z",
     "start_time": "2019-06-03T20:00:23.078688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining loss and optimizer\n",
    "second_model = make_model()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(second_model.parameters(), lr=lr)\n",
    "\n",
    "# Create data\n",
    "# # Creating federated datasets, an extension of Pytorch TensorDataset class\n",
    "federated_train_dataset = sy.FederatedDataset([bob_train_dataset, anne_train_dataset])\n",
    "federated_test_dataset = sy.FederatedDataset([bob_test_dataset, anne_test_dataset])\n",
    "\n",
    "# Creating federated dataloaders, an extension of Pytorch DataLoader class for TRAINIG METHOD #2\n",
    "federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "federated_test_loader = sy.FederatedDataLoader(federated_test_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:56:01.459697Z",
     "start_time": "2019-06-03T19:33:42.666174Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# For Early Stopping\n",
    "last_loss = 100\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "SECOND_EPOCHS = 100\n",
    "\n",
    "for e in range(SECOND_EPOCHS):\n",
    "    \n",
    "    ######### Training ##########\n",
    "\n",
    "    losses = []\n",
    "    # Batch loop\n",
    "    for inputs, labels in federated_train_loader:\n",
    "        # Location of current batch\n",
    "        worker = inputs.location\n",
    "        # Initialize hidden state and send it to worker\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
    "        # Send model to current worker\n",
    "        second_model.send(worker)\n",
    "        # Setting accumulated gradients to zero before backward step\n",
    "        optimizer.zero_grad()\n",
    "        # Output from the model\n",
    "        output, _ = second_model(inputs.to(torch.long), h)\n",
    "        # print(f\"Output:{output}\")\n",
    "        # Calculate the loss and perform backprop\n",
    "        # print(f\"Output Shape: {output.shape} Labels Shape: {labels.shape}\")\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # # Clipping the gradient to avoid explosion\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        # Backpropagation step\n",
    "        optimizer.step() \n",
    "        # Get the model back to the local worker\n",
    "        second_model.get()\n",
    "        losses.append(loss.get())\n",
    "    \n",
    "    \n",
    "    ######## Evaluation ##########\n",
    "    \n",
    "    # Model in evaluation mode\n",
    "    second_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        test_labels_list = []\n",
    "        eval_losses = []\n",
    "\n",
    "        for inputs, labels in federated_test_loader:\n",
    "            # get current location\n",
    "            worker = inputs.location\n",
    "            # Initialize hidden state and send it to worker\n",
    "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)    \n",
    "            # Send model to worker\n",
    "            second_model.send(worker)\n",
    "            output, _ = second_model(inputs.to(torch.long), h)\n",
    "            # loss = criterion(output.squeeze(), labels.float())\n",
    "            loss = criterion(output, labels.float())\n",
    "            eval_losses.append(loss.get())\n",
    "            preds = output.squeeze().get()\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.get().numpy().astype(int))\n",
    "            # Get the model back to the local worker\n",
    "            second_model.get()\n",
    "\n",
    "    # Check test preds\n",
    "    score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "    train_loss = sum(losses)/len(losses)\n",
    "    eval_loss = sum(eval_losses)/len(eval_losses)\n",
    "    \n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(eval_loss.item())\n",
    "    \n",
    "    print(\"Epoch {}/{}...  \\\n",
    "    AUC: {:.3%}...  \\\n",
    "    Training loss: {:.5f}...  \\\n",
    "    Validation loss: {:.5f}\".format(e+1, SECOND_EPOCHS, score, train_loss, eval_loss))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if eval_loss > last_loss:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times: {trigger_times}\")\n",
    "        \n",
    "        if trigger_times >= patience:\n",
    "            print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Trigger Times: 0\")\n",
    "        trigger_times = 0\n",
    "    \n",
    "    last_loss = eval_loss\n",
    "    \n",
    "    second_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Method #2 Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Losses: {train_losses}\")\n",
    "plt.plot(train_losses, 'r')\n",
    "plt.plot(test_losses, 'b')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "PATH = \"state_dict_model.pt\"\n",
    "torch.save(second_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask for input and pre-process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('manually_collected_sms_600.csv', sep=',', names=['Teks', 'Label'])\n",
    "data = data.sample(frac = 1)\n",
    "# Lowercase, remove unnecessary char with regex, remove stop words\n",
    "data.Teks = data.Teks.apply(clean_text)\n",
    "#     print(data.Teks)\n",
    "words = set((' '.join(data.Teks)).split())\n",
    "#     print(words)\n",
    "word_to_idx = {word: i for i, word in enumerate(words, start=1)}\n",
    "#     pprint(word_to_idx)\n",
    "tokens = data.Teks.apply(lambda x: tokenize(x, word_to_idx))\n",
    "#     print(tokens)\n",
    "inputs = pad_and_truncate(tokens)\n",
    "#     pprint(inputs)\n",
    "labels = np.array((data.Label == '1').astype(int))\n",
    "\n",
    "np.save('test_labels.npy', labels)\n",
    "np.save('test_inputs.npy', inputs)\n",
    "\n",
    "test_inputs = torch.tensor(np.load('test_inputs.npy'))\n",
    "test_labels = torch.tensor(np.load('test_labels.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing params\n",
    "VOCAB_SIZE = int(test_inputs.max()) + 1\n",
    "TEST_VOCAB_SIZE = TRAIN_VOCAB_SIZE\n",
    "lr = 0.001\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"state_dict_model.pt\"\n",
    "load_model = GRU(vocab_size=TEST_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(second_model.parameters(), lr=lr)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "    eval_losses = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "\n",
    "        output, _ = second_model(inputs.to(torch.long), h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        eval_losses.append(loss)\n",
    "        preds = output.squeeze()\n",
    "        test_preds += list(preds.numpy())\n",
    "        test_labels_list += list(labels.numpy().astype(int))\n",
    "\n",
    "\n",
    "roc_acc_score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels_list, test_preds)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# Index of largest G-means\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "threshold = thresholds[ix]\n",
    "\n",
    "# Print how many data is being tested\n",
    "print(f\"Amount of test data: {len(test_labels_list)}\")\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "print(f\"ROC Accuracy Score: {roc_acc_score}\")\n",
    "\n",
    "# Normalize probability with threshold\n",
    "test_preds_thresholded = np.where(test_preds > threshold, 1, 0)\n",
    "for i in range(len(test_preds)-1140):\n",
    "    print(\"Test Preds Prob: {}    \\\n",
    "    Test Preds Label: {}  \\\n",
    "    True Label: {}  \\\n",
    "    \".format(test_preds[i], test_preds_thresholded[i], test_labels_list[i]))\n",
    "\n",
    "acc_score = accuracy_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"\\nAccuracy Score: {acc_score}\")\n",
    "\n",
    "# Calculate F1 Score\n",
    "# f1_score = f1_score(test_labels_list, test_preds_thresholded)\n",
    "# print(f\"F1 Score: {f1_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "27726a6b9d0b3aecf19ddb8fb5d165165384e9a9dccccc704489801e8c9c2418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('spam_classifier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

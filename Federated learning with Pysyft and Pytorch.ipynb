{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.160008Z",
     "start_time": "2019-06-03T19:33:39.344527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:cuda\n",
      "Torch Ver: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import sqrt, argmax\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "# import opacus\n",
    "# from opacus import PrivacyEngine\n",
    "\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# from syft.frameworks.torch.nn import GRU\n",
    "from handcrafted_GRU import GRU\n",
    "# from opacus.layers import DPGRU\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device:{device}\")\n",
    "\n",
    "print(f\"Torch Ver: {torch.__version__}\")\n",
    "# print(f\"Opacus Ver: {opacus.__version__}\")\n",
    "# print(f\"Syft Ver: {sy.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('indonesian'))\n",
    "# print(f\"STOPWORDS:\\n {STOPWORDS}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # print(f\"\\n\\nOriginal Text: {text}\")\n",
    "    text = text.lower()\n",
    "    # print(f\"\\nCase Lowered Text: {text}\")\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # print(f\"\\nRegexed Text: {text}\")\n",
    "    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n",
    "    # print(f\"\\nStopwords Removed Text: {text}\")\n",
    "    return text\n",
    "\n",
    "def tokenize(text, word_to_idx):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        if word in word_to_idx:\n",
    "            tokens.append(word_to_idx[word])\n",
    "        else:\n",
    "            tokens.append(0)\n",
    "    return tokens\n",
    "\n",
    "def pad_and_truncate(messages, max_length=30):\n",
    "    features = np.zeros((len(messages), max_length), dtype=int)\n",
    "    # pprint(f\"Messages: {messages}\\nFeatures: {features}\")\n",
    "    for i, sms in enumerate(messages):\n",
    "        # print(f\"\\ni: {i}\\nSMS:{sms}\")\n",
    "        if len(sms):\n",
    "            features[i, -len(sms):] = sms[:max_length]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('train_sms_1240.csv', sep=',', names=['Teks', 'Label'], encoding= 'unicode_escape')\n",
    "    data = data.sample(frac = 1)\n",
    "    # Lowercase, remove unnecessary char with regex, remove stop words\n",
    "    data.Teks = data.Teks.apply(clean_text)\n",
    "    # print(data.Teks)\n",
    "    words = set((' '.join(data.Teks)).split())\n",
    "    # print(words)\n",
    "    word_to_idx = {word: i for i, word in enumerate(words, start=1)}\n",
    "    # pprint(word_to_idx)\n",
    "    tokens = data.Teks.apply(lambda x: tokenize(x, word_to_idx))\n",
    "    # print(tokens)\n",
    "    inputs = pad_and_truncate(tokens)\n",
    "    # pprint(inputs)\n",
    "    labels = np.array((data.Label == '1').astype(int))\n",
    "\n",
    "    np.save('labels.npy', labels)\n",
    "    np.save('inputs.npy', inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.207763Z",
     "start_time": "2019-06-03T19:33:40.201292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training params\n",
    "TRAIN_VOCAB_SIZE = int(inputs.max()) + 1\n",
    "EPOCHS = 200\n",
    "CLIP = 5 # gradient clipping - to avoid gradient explosion (frequent in RNNs)\n",
    "lr = 0.01\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.197935Z",
     "start_time": "2019-06-03T19:33:40.186270Z"
    }
   },
   "outputs": [],
   "source": [
    "original_inputs = np.load('inputs.npy')\n",
    "original_labels = np.load('labels.npy')\n",
    "\n",
    "inputs = torch.tensor(original_inputs)\n",
    "labels = torch.tensor(original_labels)\n",
    "\n",
    "# splitting training and test data\n",
    "# 20% of the data will be for Tests.\n",
    "pct_test = 0.2\n",
    "\n",
    "#20% of total data\n",
    "pct_test_count = -int(len(labels)*pct_test)\n",
    "\n",
    "# Get 80% of Train LABELS from left.\n",
    "train_labels = labels[:pct_test_count]\n",
    "# print(f\"Train Labels: [:{pct_test_count}]\")\n",
    "\n",
    "# Get 80% of Train INPUTS from left.\n",
    "train_inputs = inputs[:pct_test_count]\n",
    "# print(f\"Train Inputs: [:{pct_test_count}]\")\n",
    "\n",
    "# Get the rest of the LABEL data for test on the right (20%) \n",
    "test_labels = labels[pct_test_count:]\n",
    "# print(f\"Test Labels: [{pct_test_count}:]\")\n",
    "\n",
    "# Get the rest of the INPUT data for test on the right (20%)\n",
    "test_inputs = inputs[pct_test_count:]\n",
    "# print(f\"Test Inputs: [{pct_test_count}:]\")\n",
    "\n",
    "\n",
    "SAMPLE_SIZE = len(labels)\n",
    "# print(f\"Sample Size: {SAMPLE_SIZE}\")\n",
    "\n",
    "# print(f\"20% of Sample Size: {SAMPLE_SIZE*pct_test}\")\n",
    "\n",
    "# For Local Model Evaluation\n",
    "original_test_inputs = original_inputs[pct_test_count:]\n",
    "original_test_labels = original_labels[pct_test_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VirtualWorkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.591430Z",
     "start_time": "2019-06-03T19:33:41.969220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hook that extends the Pytorch library \n",
    "# to enable all computations with pointers of tensors sent to other workers\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Creating 2 virtual workers Syft v0.2.9\n",
    "anne = sy.VirtualWorker(hook, id=\"anne\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "\n",
    "workers = [anne, bob]\n",
    "\n",
    "# this is done to have the local worker (you on your notebook!) have a registry\n",
    "# of objects like every other workers, which is disabled by default but needed here\n",
    "# sy.local_worker.is_client_worker = False\n",
    "\n",
    "\n",
    "# threshold indexes for dataset split (one half for Bob, other half for Anne)\n",
    "train_idx = int(len(train_labels)/2)\n",
    "test_idx = int(len(test_labels)/2)\n",
    "\n",
    "# Sending toy datasets to virtual workers\n",
    "bob_train_dataset = sy.BaseDataset(train_inputs[:train_idx], train_labels[:train_idx]).send(bob)\n",
    "anne_train_dataset = sy.BaseDataset(train_inputs[train_idx:], train_labels[train_idx:]).send(anne)\n",
    "bob_test_dataset = sy.BaseDataset(test_inputs[:test_idx], test_labels[:test_idx]).send(bob)\n",
    "anne_test_dataset = sy.BaseDataset(test_inputs[test_idx:], test_labels[test_idx:]).send(anne)\n",
    "\n",
    "# print(f\"Train Index: {train_idx}\")\n",
    "# print(f\"Test Index: {test_idx}\")\n",
    "\n",
    "# print(f\"Anne's Data Amount: {len(train_inputs[train_idx:])}\")\n",
    "# print(f\"Bob's Data Amount: {len(train_inputs[:train_idx])}\")\n",
    "\n",
    "\n",
    "# Creating federated datasets, an extension of Pytorch TensorDataset class \n",
    "# for TRAINING METHOD #1 (with aggregation)\n",
    "bob_federated_train_dataset = sy.FederatedDataset([bob_train_dataset])\n",
    "anne_federated_train_dataset = sy.FederatedDataset([anne_train_dataset])\n",
    "bob_federated_test_dataset = sy.FederatedDataset([bob_test_dataset])\n",
    "anne_federated_test_dataset = sy.FederatedDataset([anne_test_dataset])\n",
    "\n",
    "\n",
    "merged_test_dataset = list(zip(original_test_inputs, original_test_labels))\n",
    "\n",
    "# print(f\"Input:{original_test_inputs[1]}\\t Label:{original_test_labels[1]}\")\n",
    "# print(merged_test_dataset[0])\n",
    "\n",
    "def collate_batch(batch):\n",
    "        label_list, text_list = [], []\n",
    "        for (_label, _text) in batch:\n",
    "                label_list.append(_label)\n",
    "                text_list.append(_text)\n",
    "        return label_list, text_list\n",
    "\n",
    "original_test_dataloader = DataLoader(merged_test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.638046Z",
     "start_time": "2019-06-03T19:33:42.617601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiating the model\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# model = GRU(vocab_size=TRAIN_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "# torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "def make_model():\n",
    "    model = GRU(vocab_size=TRAIN_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "    return model\n",
    "    \n",
    "local_model = make_model()\n",
    "\n",
    "models, train_dataloaders, test_dataloaders, optimizers, privacy_engines = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching model, dataloaders, and optimizers to each worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in workers:\n",
    "    model = make_model()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    model.send(worker)\n",
    "    if(worker == anne):\n",
    "        train_dataset = anne_federated_train_dataset\n",
    "        test_dataset = anne_federated_test_dataset\n",
    "    elif(worker == bob):\n",
    "        train_dataset = bob_federated_train_dataset\n",
    "        test_dataset = bob_federated_test_dataset\n",
    "\n",
    "\n",
    "    train_dataloader = sy.FederatedDataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "    test_dataloader = sy.FederatedDataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    models.append(model)\n",
    "    train_dataloaders.append(train_dataloader)\n",
    "    test_dataloaders.append(test_dataloader)\n",
    "    optimizers.append(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to aggregate remote models and to send new updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_aggregation(local_model, models):\n",
    "    with torch.no_grad():\n",
    "        for local_param, *remote_params in zip(*([local_model.parameters()] + [model.parameters() for model in models])):\n",
    "            param_stack = torch.zeros(*remote_params[0].shape)\n",
    "            for remote_param in remote_params:\n",
    "                param_stack += remote_param.copy().get()\n",
    "                # print(f\"Param Stack Sum: {param_stack}\")\n",
    "            param_stack /= len(remote_params)\n",
    "            # print(f\"Param Stack Division: {param_stack}\")\n",
    "            local_param.set_(param_stack)\n",
    "\n",
    "def send_new_models(local_model, models):\n",
    "    with torch.no_grad():\n",
    "        for remote_model in models:\n",
    "            for new_param, remote_param in zip(local_model.parameters(), remote_model.parameters()):\n",
    "                worker = remote_param.location\n",
    "                remote_value = new_param.send(worker)\n",
    "                remote_param.set_(remote_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [[], []]\n",
    "test_losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    # 1. Send new version of the model\n",
    "    send_new_models(local_model, models)\n",
    "\n",
    "    # 2. Train remotely the models\n",
    "    for i, worker in enumerate(workers):\n",
    "        train_dataloader = train_dataloaders[i]\n",
    "        model = models[i]\n",
    "        optimizer = optimizers[i]\n",
    "        \n",
    "        model.train()\n",
    "        criterion = nn.BCELoss() # for two class classification\n",
    "        losses = []   \n",
    "    \n",
    "        for data, target in train_dataloader:            \n",
    "            data = data.to(torch.long)\n",
    "            h = torch.Tensor(torch.zeros(BATCH_SIZE, HIDDEN_DIM)).send(worker)  \n",
    "            \n",
    "            # Call zero grad to clear previous gradient before every training passses.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print(f\"Data:{data}\\nTarget: {target}\\n\")\n",
    "\n",
    "            # print(f\"Worker: {worker}\\nWorker Objects: {worker.object_store._objects}\")\n",
    "\n",
    "            output, _ = model(data.to(torch.long), h)\n",
    "            # print(f\"Output: {output}\")\n",
    "            loss = criterion(output.squeeze(), target.float())\n",
    "            loss.backward()\n",
    "\n",
    "            # # Clipping the gradient to avoid explosion\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            losses.append(loss.get()) \n",
    "            optimizer.step()\n",
    "\n",
    "        sy.local_worker.clear_objects()\n",
    "        \n",
    "\n",
    "        train_loss = sum(losses) / len(losses)\n",
    "        train_losses[i].append(train_loss.item())\n",
    "\n",
    "        print(\n",
    "            f\"[{worker.id}]\\t\"\n",
    "            f\"Train Epoch: {epoch} \\t\"\n",
    "            f\"Train Loss: {train_loss:.4f} \")\n",
    "\n",
    "    # 3. Federated aggregation of the updated models\n",
    "    federated_aggregation(local_model, models)\n",
    "\n",
    "\n",
    "def eval(epoch, last_loss, trigger_times, patience):\n",
    "    # 4. Evaluate the model\n",
    "    local_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        test_labels_list = []\n",
    "        eval_losses = []\n",
    "\n",
    "        for inputs, labels in original_test_dataloader:\n",
    "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "            output, _ = local_model(torch.LongTensor(inputs), h)\n",
    "            criterion = nn.BCELoss()\n",
    "            labels = torch.LongTensor(labels)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            eval_losses.append(loss)\n",
    "            preds = output.squeeze()\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.numpy().astype(int))\n",
    "    \n",
    "    # score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "    eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    test_losses.append(eval_loss.item())\n",
    "\n",
    "            \n",
    "    # Early Stopping\n",
    "    if eval_loss > last_loss:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times: {trigger_times}\")\n",
    "        \n",
    "        if trigger_times >= patience:\n",
    "            print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Trigger Times: 0\")\n",
    "        trigger_times = 0\n",
    "    \n",
    "    last_loss = eval_loss\n",
    "\n",
    "    print(\n",
    "        f\"Eval Epoch: {epoch} \\t\"\n",
    "        f\"Eval Loss: {eval_loss:.4f} \\n\\n\")\n",
    "        # f\"AUC: {score:.3%} \\t\"\n",
    "\n",
    "    return last_loss, trigger_times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[anne]\tTrain Epoch: 0 \tTrain Loss: 0.7684 \n",
      "[bob]\tTrain Epoch: 0 \tTrain Loss: 0.9545 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 0 \tEval Loss: 0.7625 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 1 \tTrain Loss: 0.7120 \n",
      "[bob]\tTrain Epoch: 1 \tTrain Loss: 0.8565 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 1 \tEval Loss: 0.7086 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 2 \tTrain Loss: 0.6593 \n",
      "[bob]\tTrain Epoch: 2 \tTrain Loss: 0.7968 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 2 \tEval Loss: 0.6597 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 3 \tTrain Loss: 0.6068 \n",
      "[bob]\tTrain Epoch: 3 \tTrain Loss: 0.7276 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 3 \tEval Loss: 0.6161 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 4 \tTrain Loss: 0.5627 \n",
      "[bob]\tTrain Epoch: 4 \tTrain Loss: 0.6738 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 4 \tEval Loss: 0.5768 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 5 \tTrain Loss: 0.5258 \n",
      "[bob]\tTrain Epoch: 5 \tTrain Loss: 0.6244 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 5 \tEval Loss: 0.5412 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 6 \tTrain Loss: 0.4889 \n",
      "[bob]\tTrain Epoch: 6 \tTrain Loss: 0.5830 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 6 \tEval Loss: 0.5089 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 7 \tTrain Loss: 0.4511 \n",
      "[bob]\tTrain Epoch: 7 \tTrain Loss: 0.5384 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 7 \tEval Loss: 0.4797 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 8 \tTrain Loss: 0.4255 \n",
      "[bob]\tTrain Epoch: 8 \tTrain Loss: 0.5028 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 8 \tEval Loss: 0.4531 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 9 \tTrain Loss: 0.3993 \n",
      "[bob]\tTrain Epoch: 9 \tTrain Loss: 0.4749 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 9 \tEval Loss: 0.4288 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 10 \tTrain Loss: 0.3730 \n",
      "[bob]\tTrain Epoch: 10 \tTrain Loss: 0.4427 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 10 \tEval Loss: 0.4066 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 11 \tTrain Loss: 0.3517 \n",
      "[bob]\tTrain Epoch: 11 \tTrain Loss: 0.4171 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 11 \tEval Loss: 0.3862 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 12 \tTrain Loss: 0.3347 \n",
      "[bob]\tTrain Epoch: 12 \tTrain Loss: 0.3941 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 12 \tEval Loss: 0.3674 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 13 \tTrain Loss: 0.3079 \n",
      "[bob]\tTrain Epoch: 13 \tTrain Loss: 0.3723 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 13 \tEval Loss: 0.3503 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 14 \tTrain Loss: 0.2958 \n",
      "[bob]\tTrain Epoch: 14 \tTrain Loss: 0.3508 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 14 \tEval Loss: 0.3344 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 15 \tTrain Loss: 0.2808 \n",
      "[bob]\tTrain Epoch: 15 \tTrain Loss: 0.3343 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 15 \tEval Loss: 0.3197 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 16 \tTrain Loss: 0.2623 \n",
      "[bob]\tTrain Epoch: 16 \tTrain Loss: 0.3199 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 16 \tEval Loss: 0.3061 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 17 \tTrain Loss: 0.2511 \n",
      "[bob]\tTrain Epoch: 17 \tTrain Loss: 0.3041 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 17 \tEval Loss: 0.2934 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 18 \tTrain Loss: 0.2410 \n",
      "[bob]\tTrain Epoch: 18 \tTrain Loss: 0.2887 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 18 \tEval Loss: 0.2816 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 19 \tTrain Loss: 0.2228 \n",
      "[bob]\tTrain Epoch: 19 \tTrain Loss: 0.2756 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 19 \tEval Loss: 0.2707 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 20 \tTrain Loss: 0.2140 \n",
      "[bob]\tTrain Epoch: 20 \tTrain Loss: 0.2651 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 20 \tEval Loss: 0.2605 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 21 \tTrain Loss: 0.2045 \n",
      "[bob]\tTrain Epoch: 21 \tTrain Loss: 0.2508 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 21 \tEval Loss: 0.2510 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 22 \tTrain Loss: 0.1913 \n",
      "[bob]\tTrain Epoch: 22 \tTrain Loss: 0.2428 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 22 \tEval Loss: 0.2421 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 23 \tTrain Loss: 0.1840 \n",
      "[bob]\tTrain Epoch: 23 \tTrain Loss: 0.2287 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 23 \tEval Loss: 0.2339 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 24 \tTrain Loss: 0.1822 \n",
      "[bob]\tTrain Epoch: 24 \tTrain Loss: 0.2239 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 24 \tEval Loss: 0.2260 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 25 \tTrain Loss: 0.1725 \n",
      "[bob]\tTrain Epoch: 25 \tTrain Loss: 0.2139 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 25 \tEval Loss: 0.2186 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 26 \tTrain Loss: 0.1618 \n",
      "[bob]\tTrain Epoch: 26 \tTrain Loss: 0.2078 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 26 \tEval Loss: 0.2117 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 27 \tTrain Loss: 0.1571 \n",
      "[bob]\tTrain Epoch: 27 \tTrain Loss: 0.1960 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 27 \tEval Loss: 0.2051 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 28 \tTrain Loss: 0.1508 \n",
      "[bob]\tTrain Epoch: 28 \tTrain Loss: 0.1942 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 28 \tEval Loss: 0.1989 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 29 \tTrain Loss: 0.1427 \n",
      "[bob]\tTrain Epoch: 29 \tTrain Loss: 0.1842 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 29 \tEval Loss: 0.1931 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 30 \tTrain Loss: 0.1401 \n",
      "[bob]\tTrain Epoch: 30 \tTrain Loss: 0.1780 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 30 \tEval Loss: 0.1876 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 31 \tTrain Loss: 0.1324 \n",
      "[bob]\tTrain Epoch: 31 \tTrain Loss: 0.1715 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 31 \tEval Loss: 0.1824 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 32 \tTrain Loss: 0.1283 \n",
      "[bob]\tTrain Epoch: 32 \tTrain Loss: 0.1644 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 32 \tEval Loss: 0.1774 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 33 \tTrain Loss: 0.1232 \n",
      "[bob]\tTrain Epoch: 33 \tTrain Loss: 0.1615 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 33 \tEval Loss: 0.1728 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 34 \tTrain Loss: 0.1167 \n",
      "[bob]\tTrain Epoch: 34 \tTrain Loss: 0.1529 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 34 \tEval Loss: 0.1684 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 35 \tTrain Loss: 0.1147 \n",
      "[bob]\tTrain Epoch: 35 \tTrain Loss: 0.1487 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 35 \tEval Loss: 0.1641 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 36 \tTrain Loss: 0.1081 \n",
      "[bob]\tTrain Epoch: 36 \tTrain Loss: 0.1465 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 36 \tEval Loss: 0.1601 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 37 \tTrain Loss: 0.1110 \n",
      "[bob]\tTrain Epoch: 37 \tTrain Loss: 0.1392 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 37 \tEval Loss: 0.1562 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 38 \tTrain Loss: 0.1045 \n",
      "[bob]\tTrain Epoch: 38 \tTrain Loss: 0.1355 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 38 \tEval Loss: 0.1526 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 39 \tTrain Loss: 0.1001 \n",
      "[bob]\tTrain Epoch: 39 \tTrain Loss: 0.1306 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 39 \tEval Loss: 0.1491 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 40 \tTrain Loss: 0.0964 \n",
      "[bob]\tTrain Epoch: 40 \tTrain Loss: 0.1286 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 40 \tEval Loss: 0.1457 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 41 \tTrain Loss: 0.0927 \n",
      "[bob]\tTrain Epoch: 41 \tTrain Loss: 0.1268 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 41 \tEval Loss: 0.1425 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 42 \tTrain Loss: 0.0895 \n",
      "[bob]\tTrain Epoch: 42 \tTrain Loss: 0.1198 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 42 \tEval Loss: 0.1395 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 43 \tTrain Loss: 0.0880 \n",
      "[bob]\tTrain Epoch: 43 \tTrain Loss: 0.1168 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 43 \tEval Loss: 0.1366 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 44 \tTrain Loss: 0.0831 \n",
      "[bob]\tTrain Epoch: 44 \tTrain Loss: 0.1146 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 44 \tEval Loss: 0.1338 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 45 \tTrain Loss: 0.0822 \n",
      "[bob]\tTrain Epoch: 45 \tTrain Loss: 0.1121 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 45 \tEval Loss: 0.1311 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 46 \tTrain Loss: 0.0796 \n",
      "[bob]\tTrain Epoch: 46 \tTrain Loss: 0.1094 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 46 \tEval Loss: 0.1285 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 47 \tTrain Loss: 0.0775 \n",
      "[bob]\tTrain Epoch: 47 \tTrain Loss: 0.1044 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 47 \tEval Loss: 0.1260 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 48 \tTrain Loss: 0.0767 \n",
      "[bob]\tTrain Epoch: 48 \tTrain Loss: 0.1028 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 48 \tEval Loss: 0.1236 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 49 \tTrain Loss: 0.0730 \n",
      "[bob]\tTrain Epoch: 49 \tTrain Loss: 0.1007 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 49 \tEval Loss: 0.1213 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 50 \tTrain Loss: 0.0733 \n",
      "[bob]\tTrain Epoch: 50 \tTrain Loss: 0.0992 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 50 \tEval Loss: 0.1191 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 51 \tTrain Loss: 0.0694 \n",
      "[bob]\tTrain Epoch: 51 \tTrain Loss: 0.0974 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 51 \tEval Loss: 0.1169 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 52 \tTrain Loss: 0.0714 \n",
      "[bob]\tTrain Epoch: 52 \tTrain Loss: 0.0938 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 52 \tEval Loss: 0.1149 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 53 \tTrain Loss: 0.0642 \n",
      "[bob]\tTrain Epoch: 53 \tTrain Loss: 0.0915 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 53 \tEval Loss: 0.1129 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 54 \tTrain Loss: 0.0660 \n",
      "[bob]\tTrain Epoch: 54 \tTrain Loss: 0.0879 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 54 \tEval Loss: 0.1110 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 55 \tTrain Loss: 0.0655 \n",
      "[bob]\tTrain Epoch: 55 \tTrain Loss: 0.0867 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 55 \tEval Loss: 0.1091 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 56 \tTrain Loss: 0.0627 \n",
      "[bob]\tTrain Epoch: 56 \tTrain Loss: 0.0833 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 56 \tEval Loss: 0.1074 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 57 \tTrain Loss: 0.0610 \n",
      "[bob]\tTrain Epoch: 57 \tTrain Loss: 0.0825 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 57 \tEval Loss: 0.1056 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 58 \tTrain Loss: 0.0590 \n",
      "[bob]\tTrain Epoch: 58 \tTrain Loss: 0.0806 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 58 \tEval Loss: 0.1040 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 59 \tTrain Loss: 0.0575 \n",
      "[bob]\tTrain Epoch: 59 \tTrain Loss: 0.0786 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 59 \tEval Loss: 0.1024 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 60 \tTrain Loss: 0.0580 \n",
      "[bob]\tTrain Epoch: 60 \tTrain Loss: 0.0792 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 60 \tEval Loss: 0.1008 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 61 \tTrain Loss: 0.0557 \n",
      "[bob]\tTrain Epoch: 61 \tTrain Loss: 0.0753 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 61 \tEval Loss: 0.0993 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 62 \tTrain Loss: 0.0514 \n",
      "[bob]\tTrain Epoch: 62 \tTrain Loss: 0.0740 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 62 \tEval Loss: 0.0978 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 63 \tTrain Loss: 0.0531 \n",
      "[bob]\tTrain Epoch: 63 \tTrain Loss: 0.0732 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 63 \tEval Loss: 0.0964 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 64 \tTrain Loss: 0.0500 \n",
      "[bob]\tTrain Epoch: 64 \tTrain Loss: 0.0720 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 64 \tEval Loss: 0.0950 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 65 \tTrain Loss: 0.0502 \n",
      "[bob]\tTrain Epoch: 65 \tTrain Loss: 0.0719 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 65 \tEval Loss: 0.0937 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 66 \tTrain Loss: 0.0485 \n",
      "[bob]\tTrain Epoch: 66 \tTrain Loss: 0.0700 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 66 \tEval Loss: 0.0924 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 67 \tTrain Loss: 0.0493 \n",
      "[bob]\tTrain Epoch: 67 \tTrain Loss: 0.0682 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 67 \tEval Loss: 0.0911 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 68 \tTrain Loss: 0.0477 \n",
      "[bob]\tTrain Epoch: 68 \tTrain Loss: 0.0660 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 68 \tEval Loss: 0.0899 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 69 \tTrain Loss: 0.0448 \n",
      "[bob]\tTrain Epoch: 69 \tTrain Loss: 0.0654 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 69 \tEval Loss: 0.0887 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 70 \tTrain Loss: 0.0457 \n",
      "[bob]\tTrain Epoch: 70 \tTrain Loss: 0.0651 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 70 \tEval Loss: 0.0875 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 71 \tTrain Loss: 0.0447 \n",
      "[bob]\tTrain Epoch: 71 \tTrain Loss: 0.0638 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 71 \tEval Loss: 0.0864 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 72 \tTrain Loss: 0.0428 \n",
      "[bob]\tTrain Epoch: 72 \tTrain Loss: 0.0607 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 72 \tEval Loss: 0.0853 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 73 \tTrain Loss: 0.0423 \n",
      "[bob]\tTrain Epoch: 73 \tTrain Loss: 0.0608 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 73 \tEval Loss: 0.0843 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 74 \tTrain Loss: 0.0424 \n",
      "[bob]\tTrain Epoch: 74 \tTrain Loss: 0.0595 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 74 \tEval Loss: 0.0832 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 75 \tTrain Loss: 0.0412 \n",
      "[bob]\tTrain Epoch: 75 \tTrain Loss: 0.0579 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 75 \tEval Loss: 0.0822 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 76 \tTrain Loss: 0.0385 \n",
      "[bob]\tTrain Epoch: 76 \tTrain Loss: 0.0587 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 76 \tEval Loss: 0.0812 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 77 \tTrain Loss: 0.0408 \n",
      "[bob]\tTrain Epoch: 77 \tTrain Loss: 0.0568 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 77 \tEval Loss: 0.0803 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 78 \tTrain Loss: 0.0392 \n",
      "[bob]\tTrain Epoch: 78 \tTrain Loss: 0.0546 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 78 \tEval Loss: 0.0793 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 79 \tTrain Loss: 0.0387 \n",
      "[bob]\tTrain Epoch: 79 \tTrain Loss: 0.0547 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 79 \tEval Loss: 0.0784 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 80 \tTrain Loss: 0.0364 \n",
      "[bob]\tTrain Epoch: 80 \tTrain Loss: 0.0532 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 80 \tEval Loss: 0.0775 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 81 \tTrain Loss: 0.0368 \n",
      "[bob]\tTrain Epoch: 81 \tTrain Loss: 0.0525 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 81 \tEval Loss: 0.0767 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 82 \tTrain Loss: 0.0365 \n",
      "[bob]\tTrain Epoch: 82 \tTrain Loss: 0.0529 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 82 \tEval Loss: 0.0758 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 83 \tTrain Loss: 0.0332 \n",
      "[bob]\tTrain Epoch: 83 \tTrain Loss: 0.0510 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 83 \tEval Loss: 0.0750 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 84 \tTrain Loss: 0.0358 \n",
      "[bob]\tTrain Epoch: 84 \tTrain Loss: 0.0501 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 84 \tEval Loss: 0.0742 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 85 \tTrain Loss: 0.0331 \n",
      "[bob]\tTrain Epoch: 85 \tTrain Loss: 0.0505 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 85 \tEval Loss: 0.0734 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 86 \tTrain Loss: 0.0342 \n",
      "[bob]\tTrain Epoch: 86 \tTrain Loss: 0.0479 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 86 \tEval Loss: 0.0726 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 87 \tTrain Loss: 0.0329 \n",
      "[bob]\tTrain Epoch: 87 \tTrain Loss: 0.0475 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 87 \tEval Loss: 0.0719 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 88 \tTrain Loss: 0.0325 \n",
      "[bob]\tTrain Epoch: 88 \tTrain Loss: 0.0476 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 88 \tEval Loss: 0.0712 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 89 \tTrain Loss: 0.0316 \n",
      "[bob]\tTrain Epoch: 89 \tTrain Loss: 0.0464 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 89 \tEval Loss: 0.0704 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 90 \tTrain Loss: 0.0310 \n",
      "[bob]\tTrain Epoch: 90 \tTrain Loss: 0.0467 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 90 \tEval Loss: 0.0697 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 91 \tTrain Loss: 0.0305 \n",
      "[bob]\tTrain Epoch: 91 \tTrain Loss: 0.0439 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 91 \tEval Loss: 0.0691 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 92 \tTrain Loss: 0.0312 \n",
      "[bob]\tTrain Epoch: 92 \tTrain Loss: 0.0439 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 92 \tEval Loss: 0.0684 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 93 \tTrain Loss: 0.0301 \n",
      "[bob]\tTrain Epoch: 93 \tTrain Loss: 0.0448 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 93 \tEval Loss: 0.0677 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 94 \tTrain Loss: 0.0297 \n",
      "[bob]\tTrain Epoch: 94 \tTrain Loss: 0.0431 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 94 \tEval Loss: 0.0671 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 95 \tTrain Loss: 0.0300 \n",
      "[bob]\tTrain Epoch: 95 \tTrain Loss: 0.0427 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 95 \tEval Loss: 0.0664 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 96 \tTrain Loss: 0.0284 \n",
      "[bob]\tTrain Epoch: 96 \tTrain Loss: 0.0430 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 96 \tEval Loss: 0.0658 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 97 \tTrain Loss: 0.0276 \n",
      "[bob]\tTrain Epoch: 97 \tTrain Loss: 0.0411 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 97 \tEval Loss: 0.0652 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 98 \tTrain Loss: 0.0283 \n",
      "[bob]\tTrain Epoch: 98 \tTrain Loss: 0.0415 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 98 \tEval Loss: 0.0646 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 99 \tTrain Loss: 0.0275 \n",
      "[bob]\tTrain Epoch: 99 \tTrain Loss: 0.0409 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 99 \tEval Loss: 0.0641 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 100 \tTrain Loss: 0.0260 \n",
      "[bob]\tTrain Epoch: 100 \tTrain Loss: 0.0399 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 100 \tEval Loss: 0.0635 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 101 \tTrain Loss: 0.0272 \n",
      "[bob]\tTrain Epoch: 101 \tTrain Loss: 0.0380 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 101 \tEval Loss: 0.0629 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 102 \tTrain Loss: 0.0268 \n",
      "[bob]\tTrain Epoch: 102 \tTrain Loss: 0.0394 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 102 \tEval Loss: 0.0624 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 103 \tTrain Loss: 0.0264 \n",
      "[bob]\tTrain Epoch: 103 \tTrain Loss: 0.0389 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 103 \tEval Loss: 0.0619 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 104 \tTrain Loss: 0.0247 \n",
      "[bob]\tTrain Epoch: 104 \tTrain Loss: 0.0366 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 104 \tEval Loss: 0.0613 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 105 \tTrain Loss: 0.0262 \n",
      "[bob]\tTrain Epoch: 105 \tTrain Loss: 0.0370 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 105 \tEval Loss: 0.0608 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 106 \tTrain Loss: 0.0257 \n",
      "[bob]\tTrain Epoch: 106 \tTrain Loss: 0.0380 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 106 \tEval Loss: 0.0603 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 107 \tTrain Loss: 0.0242 \n",
      "[bob]\tTrain Epoch: 107 \tTrain Loss: 0.0356 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 107 \tEval Loss: 0.0598 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 108 \tTrain Loss: 0.0235 \n",
      "[bob]\tTrain Epoch: 108 \tTrain Loss: 0.0359 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 108 \tEval Loss: 0.0593 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 109 \tTrain Loss: 0.0243 \n",
      "[bob]\tTrain Epoch: 109 \tTrain Loss: 0.0340 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 109 \tEval Loss: 0.0589 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 110 \tTrain Loss: 0.0236 \n",
      "[bob]\tTrain Epoch: 110 \tTrain Loss: 0.0355 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 110 \tEval Loss: 0.0584 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 111 \tTrain Loss: 0.0229 \n",
      "[bob]\tTrain Epoch: 111 \tTrain Loss: 0.0341 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 111 \tEval Loss: 0.0579 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 112 \tTrain Loss: 0.0240 \n",
      "[bob]\tTrain Epoch: 112 \tTrain Loss: 0.0344 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 112 \tEval Loss: 0.0575 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 113 \tTrain Loss: 0.0219 \n",
      "[bob]\tTrain Epoch: 113 \tTrain Loss: 0.0349 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 113 \tEval Loss: 0.0570 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 114 \tTrain Loss: 0.0222 \n",
      "[bob]\tTrain Epoch: 114 \tTrain Loss: 0.0324 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 114 \tEval Loss: 0.0566 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 115 \tTrain Loss: 0.0212 \n",
      "[bob]\tTrain Epoch: 115 \tTrain Loss: 0.0326 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 115 \tEval Loss: 0.0562 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 116 \tTrain Loss: 0.0217 \n",
      "[bob]\tTrain Epoch: 116 \tTrain Loss: 0.0319 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 116 \tEval Loss: 0.0558 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 117 \tTrain Loss: 0.0213 \n",
      "[bob]\tTrain Epoch: 117 \tTrain Loss: 0.0327 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 117 \tEval Loss: 0.0553 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 118 \tTrain Loss: 0.0220 \n",
      "[bob]\tTrain Epoch: 118 \tTrain Loss: 0.0326 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 118 \tEval Loss: 0.0549 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 119 \tTrain Loss: 0.0217 \n",
      "[bob]\tTrain Epoch: 119 \tTrain Loss: 0.0308 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 119 \tEval Loss: 0.0545 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 120 \tTrain Loss: 0.0215 \n",
      "[bob]\tTrain Epoch: 120 \tTrain Loss: 0.0305 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 120 \tEval Loss: 0.0541 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 121 \tTrain Loss: 0.0196 \n",
      "[bob]\tTrain Epoch: 121 \tTrain Loss: 0.0311 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 121 \tEval Loss: 0.0537 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 122 \tTrain Loss: 0.0196 \n",
      "[bob]\tTrain Epoch: 122 \tTrain Loss: 0.0317 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 122 \tEval Loss: 0.0534 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 123 \tTrain Loss: 0.0200 \n",
      "[bob]\tTrain Epoch: 123 \tTrain Loss: 0.0317 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 123 \tEval Loss: 0.0530 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 124 \tTrain Loss: 0.0204 \n",
      "[bob]\tTrain Epoch: 124 \tTrain Loss: 0.0289 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 124 \tEval Loss: 0.0526 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 125 \tTrain Loss: 0.0202 \n",
      "[bob]\tTrain Epoch: 125 \tTrain Loss: 0.0288 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 125 \tEval Loss: 0.0522 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 126 \tTrain Loss: 0.0202 \n",
      "[bob]\tTrain Epoch: 126 \tTrain Loss: 0.0290 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 126 \tEval Loss: 0.0519 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 127 \tTrain Loss: 0.0193 \n",
      "[bob]\tTrain Epoch: 127 \tTrain Loss: 0.0286 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 127 \tEval Loss: 0.0515 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 128 \tTrain Loss: 0.0195 \n",
      "[bob]\tTrain Epoch: 128 \tTrain Loss: 0.0290 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 128 \tEval Loss: 0.0512 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 129 \tTrain Loss: 0.0199 \n",
      "[bob]\tTrain Epoch: 129 \tTrain Loss: 0.0282 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 129 \tEval Loss: 0.0508 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 130 \tTrain Loss: 0.0186 \n",
      "[bob]\tTrain Epoch: 130 \tTrain Loss: 0.0280 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 130 \tEval Loss: 0.0505 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 131 \tTrain Loss: 0.0183 \n",
      "[bob]\tTrain Epoch: 131 \tTrain Loss: 0.0279 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 131 \tEval Loss: 0.0502 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 132 \tTrain Loss: 0.0182 \n",
      "[bob]\tTrain Epoch: 132 \tTrain Loss: 0.0275 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 132 \tEval Loss: 0.0498 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 133 \tTrain Loss: 0.0192 \n",
      "[bob]\tTrain Epoch: 133 \tTrain Loss: 0.0268 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 133 \tEval Loss: 0.0495 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 134 \tTrain Loss: 0.0178 \n",
      "[bob]\tTrain Epoch: 134 \tTrain Loss: 0.0268 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 134 \tEval Loss: 0.0492 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 135 \tTrain Loss: 0.0178 \n",
      "[bob]\tTrain Epoch: 135 \tTrain Loss: 0.0267 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 135 \tEval Loss: 0.0489 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 136 \tTrain Loss: 0.0173 \n",
      "[bob]\tTrain Epoch: 136 \tTrain Loss: 0.0255 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 136 \tEval Loss: 0.0486 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 137 \tTrain Loss: 0.0180 \n",
      "[bob]\tTrain Epoch: 137 \tTrain Loss: 0.0263 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 137 \tEval Loss: 0.0483 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 138 \tTrain Loss: 0.0181 \n",
      "[bob]\tTrain Epoch: 138 \tTrain Loss: 0.0258 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 138 \tEval Loss: 0.0480 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 139 \tTrain Loss: 0.0177 \n",
      "[bob]\tTrain Epoch: 139 \tTrain Loss: 0.0254 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 139 \tEval Loss: 0.0477 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 140 \tTrain Loss: 0.0171 \n",
      "[bob]\tTrain Epoch: 140 \tTrain Loss: 0.0258 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 140 \tEval Loss: 0.0474 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 141 \tTrain Loss: 0.0173 \n",
      "[bob]\tTrain Epoch: 141 \tTrain Loss: 0.0257 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 141 \tEval Loss: 0.0471 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 142 \tTrain Loss: 0.0167 \n",
      "[bob]\tTrain Epoch: 142 \tTrain Loss: 0.0236 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 142 \tEval Loss: 0.0468 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 143 \tTrain Loss: 0.0165 \n",
      "[bob]\tTrain Epoch: 143 \tTrain Loss: 0.0237 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 143 \tEval Loss: 0.0465 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 144 \tTrain Loss: 0.0170 \n",
      "[bob]\tTrain Epoch: 144 \tTrain Loss: 0.0242 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 144 \tEval Loss: 0.0463 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 145 \tTrain Loss: 0.0163 \n",
      "[bob]\tTrain Epoch: 145 \tTrain Loss: 0.0241 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 145 \tEval Loss: 0.0460 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 146 \tTrain Loss: 0.0160 \n",
      "[bob]\tTrain Epoch: 146 \tTrain Loss: 0.0231 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 146 \tEval Loss: 0.0457 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 147 \tTrain Loss: 0.0173 \n",
      "[bob]\tTrain Epoch: 147 \tTrain Loss: 0.0237 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 147 \tEval Loss: 0.0455 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 148 \tTrain Loss: 0.0158 \n",
      "[bob]\tTrain Epoch: 148 \tTrain Loss: 0.0237 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 148 \tEval Loss: 0.0452 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 149 \tTrain Loss: 0.0162 \n",
      "[bob]\tTrain Epoch: 149 \tTrain Loss: 0.0239 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 149 \tEval Loss: 0.0449 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 150 \tTrain Loss: 0.0159 \n",
      "[bob]\tTrain Epoch: 150 \tTrain Loss: 0.0226 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 150 \tEval Loss: 0.0447 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 151 \tTrain Loss: 0.0151 \n",
      "[bob]\tTrain Epoch: 151 \tTrain Loss: 0.0228 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 151 \tEval Loss: 0.0444 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 152 \tTrain Loss: 0.0145 \n",
      "[bob]\tTrain Epoch: 152 \tTrain Loss: 0.0225 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 152 \tEval Loss: 0.0442 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 153 \tTrain Loss: 0.0155 \n",
      "[bob]\tTrain Epoch: 153 \tTrain Loss: 0.0220 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 153 \tEval Loss: 0.0440 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 154 \tTrain Loss: 0.0154 \n",
      "[bob]\tTrain Epoch: 154 \tTrain Loss: 0.0228 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 154 \tEval Loss: 0.0437 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 155 \tTrain Loss: 0.0152 \n",
      "[bob]\tTrain Epoch: 155 \tTrain Loss: 0.0231 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 155 \tEval Loss: 0.0435 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 156 \tTrain Loss: 0.0144 \n",
      "[bob]\tTrain Epoch: 156 \tTrain Loss: 0.0215 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 156 \tEval Loss: 0.0432 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 157 \tTrain Loss: 0.0143 \n",
      "[bob]\tTrain Epoch: 157 \tTrain Loss: 0.0216 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 157 \tEval Loss: 0.0430 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 158 \tTrain Loss: 0.0150 \n",
      "[bob]\tTrain Epoch: 158 \tTrain Loss: 0.0212 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 158 \tEval Loss: 0.0428 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 159 \tTrain Loss: 0.0142 \n",
      "[bob]\tTrain Epoch: 159 \tTrain Loss: 0.0212 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 159 \tEval Loss: 0.0425 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 160 \tTrain Loss: 0.0148 \n",
      "[bob]\tTrain Epoch: 160 \tTrain Loss: 0.0221 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 160 \tEval Loss: 0.0423 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 161 \tTrain Loss: 0.0138 \n",
      "[bob]\tTrain Epoch: 161 \tTrain Loss: 0.0211 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 161 \tEval Loss: 0.0421 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 162 \tTrain Loss: 0.0137 \n",
      "[bob]\tTrain Epoch: 162 \tTrain Loss: 0.0209 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 162 \tEval Loss: 0.0419 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 163 \tTrain Loss: 0.0144 \n",
      "[bob]\tTrain Epoch: 163 \tTrain Loss: 0.0196 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 163 \tEval Loss: 0.0417 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 164 \tTrain Loss: 0.0145 \n",
      "[bob]\tTrain Epoch: 164 \tTrain Loss: 0.0211 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 164 \tEval Loss: 0.0414 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 165 \tTrain Loss: 0.0144 \n",
      "[bob]\tTrain Epoch: 165 \tTrain Loss: 0.0199 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 165 \tEval Loss: 0.0412 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 166 \tTrain Loss: 0.0143 \n",
      "[bob]\tTrain Epoch: 166 \tTrain Loss: 0.0202 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 166 \tEval Loss: 0.0410 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 167 \tTrain Loss: 0.0143 \n",
      "[bob]\tTrain Epoch: 167 \tTrain Loss: 0.0209 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 167 \tEval Loss: 0.0408 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 168 \tTrain Loss: 0.0137 \n",
      "[bob]\tTrain Epoch: 168 \tTrain Loss: 0.0205 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 168 \tEval Loss: 0.0406 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 169 \tTrain Loss: 0.0131 \n",
      "[bob]\tTrain Epoch: 169 \tTrain Loss: 0.0197 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 169 \tEval Loss: 0.0404 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 170 \tTrain Loss: 0.0127 \n",
      "[bob]\tTrain Epoch: 170 \tTrain Loss: 0.0191 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 170 \tEval Loss: 0.0402 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 171 \tTrain Loss: 0.0128 \n",
      "[bob]\tTrain Epoch: 171 \tTrain Loss: 0.0190 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 171 \tEval Loss: 0.0400 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 172 \tTrain Loss: 0.0130 \n",
      "[bob]\tTrain Epoch: 172 \tTrain Loss: 0.0189 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 172 \tEval Loss: 0.0398 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 173 \tTrain Loss: 0.0127 \n",
      "[bob]\tTrain Epoch: 173 \tTrain Loss: 0.0189 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 173 \tEval Loss: 0.0396 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 174 \tTrain Loss: 0.0124 \n",
      "[bob]\tTrain Epoch: 174 \tTrain Loss: 0.0187 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 174 \tEval Loss: 0.0394 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 175 \tTrain Loss: 0.0123 \n",
      "[bob]\tTrain Epoch: 175 \tTrain Loss: 0.0179 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 175 \tEval Loss: 0.0392 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 176 \tTrain Loss: 0.0126 \n",
      "[bob]\tTrain Epoch: 176 \tTrain Loss: 0.0183 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 176 \tEval Loss: 0.0391 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 177 \tTrain Loss: 0.0124 \n",
      "[bob]\tTrain Epoch: 177 \tTrain Loss: 0.0189 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 177 \tEval Loss: 0.0389 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 178 \tTrain Loss: 0.0121 \n",
      "[bob]\tTrain Epoch: 178 \tTrain Loss: 0.0182 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 178 \tEval Loss: 0.0387 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 179 \tTrain Loss: 0.0121 \n",
      "[bob]\tTrain Epoch: 179 \tTrain Loss: 0.0178 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 179 \tEval Loss: 0.0385 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 180 \tTrain Loss: 0.0124 \n",
      "[bob]\tTrain Epoch: 180 \tTrain Loss: 0.0179 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 180 \tEval Loss: 0.0383 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 181 \tTrain Loss: 0.0121 \n",
      "[bob]\tTrain Epoch: 181 \tTrain Loss: 0.0175 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 181 \tEval Loss: 0.0382 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 182 \tTrain Loss: 0.0115 \n",
      "[bob]\tTrain Epoch: 182 \tTrain Loss: 0.0186 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 182 \tEval Loss: 0.0380 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 183 \tTrain Loss: 0.0121 \n",
      "[bob]\tTrain Epoch: 183 \tTrain Loss: 0.0175 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 183 \tEval Loss: 0.0378 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 184 \tTrain Loss: 0.0116 \n",
      "[bob]\tTrain Epoch: 184 \tTrain Loss: 0.0168 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 184 \tEval Loss: 0.0377 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 185 \tTrain Loss: 0.0120 \n",
      "[bob]\tTrain Epoch: 185 \tTrain Loss: 0.0176 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 185 \tEval Loss: 0.0375 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 186 \tTrain Loss: 0.0115 \n",
      "[bob]\tTrain Epoch: 186 \tTrain Loss: 0.0169 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 186 \tEval Loss: 0.0373 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 187 \tTrain Loss: 0.0113 \n",
      "[bob]\tTrain Epoch: 187 \tTrain Loss: 0.0168 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 187 \tEval Loss: 0.0372 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 188 \tTrain Loss: 0.0119 \n",
      "[bob]\tTrain Epoch: 188 \tTrain Loss: 0.0170 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 188 \tEval Loss: 0.0370 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 189 \tTrain Loss: 0.0117 \n",
      "[bob]\tTrain Epoch: 189 \tTrain Loss: 0.0161 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 189 \tEval Loss: 0.0368 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 190 \tTrain Loss: 0.0124 \n",
      "[bob]\tTrain Epoch: 190 \tTrain Loss: 0.0167 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 190 \tEval Loss: 0.0367 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 191 \tTrain Loss: 0.0115 \n",
      "[bob]\tTrain Epoch: 191 \tTrain Loss: 0.0165 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 191 \tEval Loss: 0.0365 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 192 \tTrain Loss: 0.0109 \n",
      "[bob]\tTrain Epoch: 192 \tTrain Loss: 0.0158 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 192 \tEval Loss: 0.0364 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 193 \tTrain Loss: 0.0119 \n",
      "[bob]\tTrain Epoch: 193 \tTrain Loss: 0.0157 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 193 \tEval Loss: 0.0362 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 194 \tTrain Loss: 0.0112 \n",
      "[bob]\tTrain Epoch: 194 \tTrain Loss: 0.0163 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 194 \tEval Loss: 0.0360 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 195 \tTrain Loss: 0.0114 \n",
      "[bob]\tTrain Epoch: 195 \tTrain Loss: 0.0158 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 195 \tEval Loss: 0.0359 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 196 \tTrain Loss: 0.0112 \n",
      "[bob]\tTrain Epoch: 196 \tTrain Loss: 0.0159 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 196 \tEval Loss: 0.0357 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 197 \tTrain Loss: 0.0110 \n",
      "[bob]\tTrain Epoch: 197 \tTrain Loss: 0.0156 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 197 \tEval Loss: 0.0356 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 198 \tTrain Loss: 0.0104 \n",
      "[bob]\tTrain Epoch: 198 \tTrain Loss: 0.0156 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 198 \tEval Loss: 0.0354 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 199 \tTrain Loss: 0.0101 \n",
      "[bob]\tTrain Epoch: 199 \tTrain Loss: 0.0162 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 199 \tEval Loss: 0.0353 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Early Stopping\n",
    "last_loss = 100\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    last_loss, trigger_times = eval(epoch, last_loss, trigger_times, patience)\n",
    "    if trigger_times >= patience:\n",
    "        print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses[0], 'r')\n",
    "plt.plot(train_losses[1], 'g')\n",
    "plt.plot(test_losses, 'b')\n",
    "plt.legend(['Training Loss Anne', 'Training Loss Bob' , 'Eval Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "PATH = \"local_state_dict_model.pt\"\n",
    "torch.save(local_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test_sms_540.csv', sep=',', encoding= 'unicode_escape', names=['Teks', 'Label'])\n",
    "# data = data.sample(frac = 1)\n",
    "# Lowercase, remove unnecessary char with regex, remove stop words\n",
    "data.Teks = data.Teks.apply(clean_text)\n",
    "#     print(data.Teks)\n",
    "words = set((' '.join(data.Teks)).split())\n",
    "#     print(words)\n",
    "word_to_idx = {word: i for i, word in enumerate(words, start=1)}\n",
    "#     pprint(word_to_idx)\n",
    "tokens = data.Teks.apply(lambda x: tokenize(x, word_to_idx))\n",
    "#     print(tokens)\n",
    "inputs = pad_and_truncate(tokens)\n",
    "#     pprint(inputs)\n",
    "labels = np.array((data.Label == '1').astype(int))\n",
    "\n",
    "np.save('test_labels.npy', labels)\n",
    "np.save('test_inputs.npy', inputs)\n",
    "\n",
    "test_inputs = torch.tensor(np.load('test_inputs.npy'))\n",
    "test_labels = torch.tensor(np.load('test_labels.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing params\n",
    "# VOCAB_SIZE = int(test_inputs.max()) + 1\n",
    "TEST_VOCAB_SIZE = TRAIN_VOCAB_SIZE \n",
    "lr = 0.01\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Test First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.033911, G-Mean=0.546\n",
      "Amount of test data: 540\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Accuracy Score: 0.5458929984620209\n",
      "\n",
      "Accuracy Score: 0.5481481481481482\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 540x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix: \n",
      "[[194 153]\n",
      " [ 91 102]]\n",
      "\n",
      "TN: 194\t FP: 153\n",
      "FN: 91\t TP: 102\n",
      "\n",
      "Precision: 0.4\n",
      "Recall:0.5284974093264249\n",
      "F1 Score: 0.4553571428571429\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "PATH = \"local_state_dict_model.pt\"\n",
    "load_model = GRU(vocab_size=TEST_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "\n",
    "# Test Model\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(load_model.parameters(), lr=lr)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "    eval_losses = []\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        # print(f\"i: {i}\\nInputs: {inputs} Labels: {labels}\")\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "\n",
    "        output, _ = load_model(inputs.to(torch.long), h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        eval_losses.append(loss)\n",
    "        preds = output.squeeze()\n",
    "        if len(labels) > 1:\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.numpy().astype(int))\n",
    "        # print(f\"Preds: {preds}\\n Preds Type: {type(preds)}\")\n",
    "\n",
    "\n",
    "roc_acc_score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels_list, test_preds)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# Index of largest G-means\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "threshold = thresholds[ix]\n",
    "\n",
    "# Print how many data is being tested\n",
    "print(f\"Amount of test data: {len(test_labels_list)}\")\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "print(f\"ROC Accuracy Score: {roc_acc_score}\")\n",
    "\n",
    "# Normalize probability with threshold\n",
    "test_preds_thresholded = np.where(test_preds > threshold, 1, 0)\n",
    "for i in range(len(test_preds)-1140):\n",
    "    print(\"Test Preds Prob: {}    \\\n",
    "    Test Preds Label: {}  \\\n",
    "    True Label: {}  \\\n",
    "    \".format(test_preds[i], test_preds_thresholded[i], test_labels_list[i]))\n",
    "\n",
    "acc_score = accuracy_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"\\nAccuracy Score: {acc_score}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cm = confusion_matrix(test_labels_list, test_preds_thresholded)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix: \\n{cm}\")\n",
    "\n",
    "tn = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tp = cm[1][1]\n",
    "print(f\"\\nTN: {tn}\\t FP: {fp}\\nFN: {fn}\\t TP: {tp}\\n\")\n",
    "\n",
    "# Precision\n",
    "precision = tp/(tp+fp)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# Recall\n",
    "recall = tp/(tp+fn)\n",
    "print(f\"Recall:{recall}\")\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_score = f1_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, PySyft does not support optimizers with momentum. Therefore, we are going to stick with the classical [Stochastic Gradient Descent](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) optimizer.\n",
    "\n",
    "As our task consists of a binary classification, we are going to use the [Binary Cross Entropy Loss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T20:00:23.084933Z",
     "start_time": "2019-06-03T20:00:23.078688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining loss and optimizer\n",
    "second_model = make_model()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(second_model.parameters(), lr=lr)\n",
    "\n",
    "# Create data\n",
    "# # Creating federated datasets, an extension of Pytorch TensorDataset class\n",
    "federated_train_dataset = sy.FederatedDataset([bob_train_dataset, anne_train_dataset])\n",
    "federated_test_dataset = sy.FederatedDataset([bob_test_dataset, anne_test_dataset])\n",
    "\n",
    "# Creating federated dataloaders, an extension of Pytorch DataLoader class for TRAINIG METHOD #2\n",
    "federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, batch_size=BATCH_SIZE)\n",
    "federated_test_loader = sy.FederatedDataLoader(federated_test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:56:01.459697Z",
     "start_time": "2019-06-03T19:33:42.666174Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200...      Training loss: 0.82086...      Validation loss: 0.75774\n",
      "Trigger Times: 0\n",
      "Epoch 2/200...      Training loss: 0.69971...      Validation loss: 0.64751\n",
      "Trigger Times: 0\n",
      "Epoch 3/200...      Training loss: 0.60100...      Validation loss: 0.55898\n",
      "Trigger Times: 0\n",
      "Epoch 4/200...      Training loss: 0.52153...      Validation loss: 0.48675\n",
      "Trigger Times: 0\n",
      "Epoch 5/200...      Training loss: 0.45695...      Validation loss: 0.42727\n",
      "Trigger Times: 0\n",
      "Epoch 6/200...      Training loss: 0.40357...      Validation loss: 0.37765\n",
      "Trigger Times: 0\n",
      "Epoch 7/200...      Training loss: 0.35959...      Validation loss: 0.33596\n",
      "Trigger Times: 0\n",
      "Epoch 8/200...      Training loss: 0.31979...      Validation loss: 0.30075\n",
      "Trigger Times: 0\n",
      "Epoch 9/200...      Training loss: 0.28753...      Validation loss: 0.27060\n",
      "Trigger Times: 0\n",
      "Epoch 10/200...      Training loss: 0.25871...      Validation loss: 0.24481\n",
      "Trigger Times: 0\n",
      "Epoch 11/200...      Training loss: 0.23535...      Validation loss: 0.22245\n",
      "Trigger Times: 0\n",
      "Epoch 12/200...      Training loss: 0.21482...      Validation loss: 0.20299\n",
      "Trigger Times: 0\n",
      "Epoch 13/200...      Training loss: 0.19884...      Validation loss: 0.18593\n",
      "Trigger Times: 0\n",
      "Epoch 14/200...      Training loss: 0.18304...      Validation loss: 0.17098\n",
      "Trigger Times: 0\n",
      "Epoch 15/200...      Training loss: 0.16720...      Validation loss: 0.15779\n",
      "Trigger Times: 0\n",
      "Epoch 16/200...      Training loss: 0.15736...      Validation loss: 0.14604\n",
      "Trigger Times: 0\n",
      "Epoch 17/200...      Training loss: 0.14441...      Validation loss: 0.13564\n",
      "Trigger Times: 0\n",
      "Epoch 18/200...      Training loss: 0.13379...      Validation loss: 0.12630\n",
      "Trigger Times: 0\n",
      "Epoch 19/200...      Training loss: 0.12672...      Validation loss: 0.11789\n",
      "Trigger Times: 0\n",
      "Epoch 20/200...      Training loss: 0.11529...      Validation loss: 0.11044\n",
      "Trigger Times: 0\n",
      "Epoch 21/200...      Training loss: 0.11018...      Validation loss: 0.10355\n",
      "Trigger Times: 0\n",
      "Epoch 22/200...      Training loss: 0.10400...      Validation loss: 0.09735\n",
      "Trigger Times: 0\n",
      "Epoch 23/200...      Training loss: 0.09844...      Validation loss: 0.09173\n",
      "Trigger Times: 0\n",
      "Epoch 24/200...      Training loss: 0.09333...      Validation loss: 0.08661\n",
      "Trigger Times: 0\n",
      "Epoch 25/200...      Training loss: 0.08709...      Validation loss: 0.08194\n",
      "Trigger Times: 0\n",
      "Epoch 26/200...      Training loss: 0.08386...      Validation loss: 0.07757\n",
      "Trigger Times: 0\n",
      "Epoch 27/200...      Training loss: 0.08022...      Validation loss: 0.07360\n",
      "Trigger Times: 0\n",
      "Epoch 28/200...      Training loss: 0.07644...      Validation loss: 0.06994\n",
      "Trigger Times: 0\n",
      "Epoch 29/200...      Training loss: 0.07141...      Validation loss: 0.06658\n",
      "Trigger Times: 0\n",
      "Epoch 30/200...      Training loss: 0.06886...      Validation loss: 0.06345\n",
      "Trigger Times: 0\n",
      "Epoch 31/200...      Training loss: 0.06448...      Validation loss: 0.06058\n",
      "Trigger Times: 0\n",
      "Epoch 32/200...      Training loss: 0.06278...      Validation loss: 0.05788\n",
      "Trigger Times: 0\n",
      "Epoch 33/200...      Training loss: 0.06011...      Validation loss: 0.05539\n",
      "Trigger Times: 0\n",
      "Epoch 34/200...      Training loss: 0.05755...      Validation loss: 0.05306\n",
      "Trigger Times: 0\n",
      "Epoch 35/200...      Training loss: 0.05482...      Validation loss: 0.05091\n",
      "Trigger Times: 0\n",
      "Epoch 36/200...      Training loss: 0.05329...      Validation loss: 0.04887\n",
      "Trigger Times: 0\n",
      "Epoch 37/200...      Training loss: 0.05179...      Validation loss: 0.04696\n",
      "Trigger Times: 0\n",
      "Epoch 38/200...      Training loss: 0.04848...      Validation loss: 0.04518\n",
      "Trigger Times: 0\n",
      "Epoch 39/200...      Training loss: 0.04614...      Validation loss: 0.04354\n",
      "Trigger Times: 0\n",
      "Epoch 40/200...      Training loss: 0.04496...      Validation loss: 0.04198\n",
      "Trigger Times: 0\n",
      "Epoch 41/200...      Training loss: 0.04329...      Validation loss: 0.04050\n",
      "Trigger Times: 0\n",
      "Epoch 42/200...      Training loss: 0.04350...      Validation loss: 0.03908\n",
      "Trigger Times: 0\n",
      "Epoch 43/200...      Training loss: 0.04011...      Validation loss: 0.03778\n",
      "Trigger Times: 0\n",
      "Epoch 44/200...      Training loss: 0.04034...      Validation loss: 0.03652\n",
      "Trigger Times: 0\n",
      "Epoch 45/200...      Training loss: 0.03820...      Validation loss: 0.03535\n",
      "Trigger Times: 0\n",
      "Epoch 46/200...      Training loss: 0.03733...      Validation loss: 0.03423\n",
      "Trigger Times: 0\n",
      "Epoch 47/200...      Training loss: 0.03674...      Validation loss: 0.03316\n",
      "Trigger Times: 0\n",
      "Epoch 48/200...      Training loss: 0.03621...      Validation loss: 0.03213\n",
      "Trigger Times: 0\n",
      "Epoch 49/200...      Training loss: 0.03430...      Validation loss: 0.03117\n",
      "Trigger Times: 0\n",
      "Epoch 50/200...      Training loss: 0.03315...      Validation loss: 0.03026\n",
      "Trigger Times: 0\n",
      "Epoch 51/200...      Training loss: 0.03227...      Validation loss: 0.02939\n",
      "Trigger Times: 0\n",
      "Epoch 52/200...      Training loss: 0.03139...      Validation loss: 0.02856\n",
      "Trigger Times: 0\n",
      "Epoch 53/200...      Training loss: 0.03123...      Validation loss: 0.02776\n",
      "Trigger Times: 0\n",
      "Epoch 54/200...      Training loss: 0.02934...      Validation loss: 0.02701\n",
      "Trigger Times: 0\n",
      "Epoch 55/200...      Training loss: 0.02988...      Validation loss: 0.02629\n",
      "Trigger Times: 0\n",
      "Epoch 56/200...      Training loss: 0.02858...      Validation loss: 0.02559\n",
      "Trigger Times: 0\n",
      "Epoch 57/200...      Training loss: 0.02674...      Validation loss: 0.02495\n",
      "Trigger Times: 0\n",
      "Epoch 58/200...      Training loss: 0.02678...      Validation loss: 0.02432\n",
      "Trigger Times: 0\n",
      "Epoch 59/200...      Training loss: 0.02642...      Validation loss: 0.02371\n",
      "Trigger Times: 0\n",
      "Epoch 60/200...      Training loss: 0.02584...      Validation loss: 0.02313\n",
      "Trigger Times: 0\n",
      "Epoch 61/200...      Training loss: 0.02475...      Validation loss: 0.02257\n",
      "Trigger Times: 0\n",
      "Epoch 62/200...      Training loss: 0.02451...      Validation loss: 0.02204\n",
      "Trigger Times: 0\n",
      "Epoch 63/200...      Training loss: 0.02356...      Validation loss: 0.02153\n",
      "Trigger Times: 0\n",
      "Epoch 64/200...      Training loss: 0.02371...      Validation loss: 0.02103\n",
      "Trigger Times: 0\n",
      "Epoch 65/200...      Training loss: 0.02257...      Validation loss: 0.02056\n",
      "Trigger Times: 0\n",
      "Epoch 66/200...      Training loss: 0.02190...      Validation loss: 0.02011\n",
      "Trigger Times: 0\n",
      "Epoch 67/200...      Training loss: 0.02194...      Validation loss: 0.01967\n",
      "Trigger Times: 0\n",
      "Epoch 68/200...      Training loss: 0.02197...      Validation loss: 0.01925\n",
      "Trigger Times: 0\n",
      "Epoch 69/200...      Training loss: 0.02080...      Validation loss: 0.01884\n",
      "Trigger Times: 0\n",
      "Epoch 70/200...      Training loss: 0.02081...      Validation loss: 0.01844\n",
      "Trigger Times: 0\n",
      "Epoch 71/200...      Training loss: 0.02058...      Validation loss: 0.01806\n",
      "Trigger Times: 0\n",
      "Epoch 72/200...      Training loss: 0.01939...      Validation loss: 0.01770\n",
      "Trigger Times: 0\n",
      "Epoch 73/200...      Training loss: 0.01930...      Validation loss: 0.01734\n",
      "Trigger Times: 0\n",
      "Epoch 74/200...      Training loss: 0.01908...      Validation loss: 0.01700\n",
      "Trigger Times: 0\n",
      "Epoch 75/200...      Training loss: 0.01857...      Validation loss: 0.01667\n",
      "Trigger Times: 0\n",
      "Epoch 76/200...      Training loss: 0.01861...      Validation loss: 0.01635\n",
      "Trigger Times: 0\n",
      "Epoch 77/200...      Training loss: 0.01782...      Validation loss: 0.01604\n",
      "Trigger Times: 0\n",
      "Epoch 78/200...      Training loss: 0.01791...      Validation loss: 0.01574\n",
      "Trigger Times: 0\n",
      "Epoch 79/200...      Training loss: 0.01779...      Validation loss: 0.01544\n",
      "Trigger Times: 0\n",
      "Epoch 80/200...      Training loss: 0.01727...      Validation loss: 0.01516\n",
      "Trigger Times: 0\n",
      "Epoch 81/200...      Training loss: 0.01656...      Validation loss: 0.01489\n",
      "Trigger Times: 0\n",
      "Epoch 82/200...      Training loss: 0.01668...      Validation loss: 0.01462\n",
      "Trigger Times: 0\n",
      "Epoch 83/200...      Training loss: 0.01612...      Validation loss: 0.01437\n",
      "Trigger Times: 0\n",
      "Epoch 84/200...      Training loss: 0.01620...      Validation loss: 0.01412\n",
      "Trigger Times: 0\n",
      "Epoch 85/200...      Training loss: 0.01538...      Validation loss: 0.01388\n",
      "Trigger Times: 0\n",
      "Epoch 86/200...      Training loss: 0.01584...      Validation loss: 0.01364\n",
      "Trigger Times: 0\n",
      "Epoch 87/200...      Training loss: 0.01558...      Validation loss: 0.01341\n",
      "Trigger Times: 0\n",
      "Epoch 88/200...      Training loss: 0.01528...      Validation loss: 0.01318\n",
      "Trigger Times: 0\n",
      "Epoch 89/200...      Training loss: 0.01457...      Validation loss: 0.01297\n",
      "Trigger Times: 0\n",
      "Epoch 90/200...      Training loss: 0.01419...      Validation loss: 0.01277\n",
      "Trigger Times: 0\n",
      "Epoch 91/200...      Training loss: 0.01436...      Validation loss: 0.01256\n",
      "Trigger Times: 0\n",
      "Epoch 92/200...      Training loss: 0.01444...      Validation loss: 0.01236\n",
      "Trigger Times: 0\n",
      "Epoch 93/200...      Training loss: 0.01416...      Validation loss: 0.01216\n",
      "Trigger Times: 0\n",
      "Epoch 94/200...      Training loss: 0.01370...      Validation loss: 0.01197\n",
      "Trigger Times: 0\n",
      "Epoch 95/200...      Training loss: 0.01343...      Validation loss: 0.01179\n",
      "Trigger Times: 0\n",
      "Epoch 96/200...      Training loss: 0.01315...      Validation loss: 0.01161\n",
      "Trigger Times: 0\n",
      "Epoch 97/200...      Training loss: 0.01313...      Validation loss: 0.01144\n",
      "Trigger Times: 0\n",
      "Epoch 98/200...      Training loss: 0.01286...      Validation loss: 0.01127\n",
      "Trigger Times: 0\n",
      "Epoch 99/200...      Training loss: 0.01299...      Validation loss: 0.01110\n",
      "Trigger Times: 0\n",
      "Epoch 100/200...      Training loss: 0.01251...      Validation loss: 0.01094\n",
      "Trigger Times: 0\n",
      "Epoch 101/200...      Training loss: 0.01266...      Validation loss: 0.01078\n",
      "Trigger Times: 0\n",
      "Epoch 102/200...      Training loss: 0.01268...      Validation loss: 0.01063\n",
      "Trigger Times: 0\n",
      "Epoch 103/200...      Training loss: 0.01198...      Validation loss: 0.01048\n",
      "Trigger Times: 0\n",
      "Epoch 104/200...      Training loss: 0.01170...      Validation loss: 0.01033\n",
      "Trigger Times: 0\n",
      "Epoch 105/200...      Training loss: 0.01172...      Validation loss: 0.01019\n",
      "Trigger Times: 0\n",
      "Epoch 106/200...      Training loss: 0.01148...      Validation loss: 0.01005\n",
      "Trigger Times: 0\n",
      "Epoch 107/200...      Training loss: 0.01155...      Validation loss: 0.00992\n",
      "Trigger Times: 0\n",
      "Epoch 108/200...      Training loss: 0.01094...      Validation loss: 0.00979\n",
      "Trigger Times: 0\n",
      "Epoch 109/200...      Training loss: 0.01097...      Validation loss: 0.00966\n",
      "Trigger Times: 0\n",
      "Epoch 110/200...      Training loss: 0.01169...      Validation loss: 0.00953\n",
      "Trigger Times: 0\n",
      "Epoch 111/200...      Training loss: 0.01109...      Validation loss: 0.00940\n",
      "Trigger Times: 0\n",
      "Epoch 112/200...      Training loss: 0.01050...      Validation loss: 0.00928\n",
      "Trigger Times: 0\n",
      "Epoch 113/200...      Training loss: 0.01053...      Validation loss: 0.00917\n",
      "Trigger Times: 0\n",
      "Epoch 114/200...      Training loss: 0.01054...      Validation loss: 0.00905\n",
      "Trigger Times: 0\n",
      "Epoch 115/200...      Training loss: 0.01033...      Validation loss: 0.00894\n",
      "Trigger Times: 0\n",
      "Epoch 116/200...      Training loss: 0.01019...      Validation loss: 0.00883\n",
      "Trigger Times: 0\n",
      "Epoch 117/200...      Training loss: 0.01032...      Validation loss: 0.00871\n",
      "Trigger Times: 0\n",
      "Epoch 118/200...      Training loss: 0.01020...      Validation loss: 0.00861\n",
      "Trigger Times: 0\n",
      "Epoch 119/200...      Training loss: 0.01032...      Validation loss: 0.00850\n",
      "Trigger Times: 0\n",
      "Epoch 120/200...      Training loss: 0.01000...      Validation loss: 0.00840\n",
      "Trigger Times: 0\n",
      "Epoch 121/200...      Training loss: 0.00922...      Validation loss: 0.00830\n",
      "Trigger Times: 0\n",
      "Epoch 122/200...      Training loss: 0.00977...      Validation loss: 0.00820\n",
      "Trigger Times: 0\n",
      "Epoch 123/200...      Training loss: 0.00984...      Validation loss: 0.00810\n",
      "Trigger Times: 0\n",
      "Epoch 124/200...      Training loss: 0.00972...      Validation loss: 0.00801\n",
      "Trigger Times: 0\n",
      "Epoch 125/200...      Training loss: 0.00906...      Validation loss: 0.00792\n",
      "Trigger Times: 0\n",
      "Epoch 126/200...      Training loss: 0.00855...      Validation loss: 0.00783\n",
      "Trigger Times: 0\n",
      "Epoch 127/200...      Training loss: 0.00916...      Validation loss: 0.00774\n",
      "Trigger Times: 0\n",
      "Epoch 128/200...      Training loss: 0.00894...      Validation loss: 0.00765\n",
      "Trigger Times: 0\n",
      "Epoch 129/200...      Training loss: 0.00884...      Validation loss: 0.00757\n",
      "Trigger Times: 0\n",
      "Epoch 130/200...      Training loss: 0.00897...      Validation loss: 0.00748\n",
      "Trigger Times: 0\n",
      "Epoch 131/200...      Training loss: 0.00885...      Validation loss: 0.00740\n",
      "Trigger Times: 0\n",
      "Epoch 132/200...      Training loss: 0.00922...      Validation loss: 0.00732\n",
      "Trigger Times: 0\n",
      "Epoch 133/200...      Training loss: 0.00841...      Validation loss: 0.00724\n",
      "Trigger Times: 0\n",
      "Epoch 134/200...      Training loss: 0.00865...      Validation loss: 0.00716\n",
      "Trigger Times: 0\n",
      "Epoch 135/200...      Training loss: 0.00830...      Validation loss: 0.00709\n",
      "Trigger Times: 0\n",
      "Epoch 136/200...      Training loss: 0.00836...      Validation loss: 0.00701\n",
      "Trigger Times: 0\n",
      "Epoch 137/200...      Training loss: 0.00838...      Validation loss: 0.00694\n",
      "Trigger Times: 0\n",
      "Epoch 138/200...      Training loss: 0.00856...      Validation loss: 0.00686\n",
      "Trigger Times: 0\n",
      "Epoch 139/200...      Training loss: 0.00789...      Validation loss: 0.00679\n",
      "Trigger Times: 0\n",
      "Epoch 140/200...      Training loss: 0.00777...      Validation loss: 0.00672\n",
      "Trigger Times: 0\n",
      "Epoch 141/200...      Training loss: 0.00783...      Validation loss: 0.00666\n",
      "Trigger Times: 0\n",
      "Epoch 142/200...      Training loss: 0.00784...      Validation loss: 0.00659\n",
      "Trigger Times: 0\n",
      "Epoch 143/200...      Training loss: 0.00790...      Validation loss: 0.00652\n",
      "Trigger Times: 0\n",
      "Epoch 144/200...      Training loss: 0.00774...      Validation loss: 0.00646\n",
      "Trigger Times: 0\n",
      "Epoch 145/200...      Training loss: 0.00766...      Validation loss: 0.00639\n",
      "Trigger Times: 0\n",
      "Epoch 146/200...      Training loss: 0.00766...      Validation loss: 0.00633\n",
      "Trigger Times: 0\n",
      "Epoch 147/200...      Training loss: 0.00768...      Validation loss: 0.00627\n",
      "Trigger Times: 0\n",
      "Epoch 148/200...      Training loss: 0.00745...      Validation loss: 0.00621\n",
      "Trigger Times: 0\n",
      "Epoch 149/200...      Training loss: 0.00770...      Validation loss: 0.00615\n",
      "Trigger Times: 0\n",
      "Epoch 150/200...      Training loss: 0.00732...      Validation loss: 0.00609\n",
      "Trigger Times: 0\n",
      "Epoch 151/200...      Training loss: 0.00770...      Validation loss: 0.00603\n",
      "Trigger Times: 0\n",
      "Epoch 152/200...      Training loss: 0.00764...      Validation loss: 0.00597\n",
      "Trigger Times: 0\n",
      "Epoch 153/200...      Training loss: 0.00694...      Validation loss: 0.00591\n",
      "Trigger Times: 0\n",
      "Epoch 154/200...      Training loss: 0.00695...      Validation loss: 0.00586\n",
      "Trigger Times: 0\n",
      "Epoch 155/200...      Training loss: 0.00699...      Validation loss: 0.00581\n",
      "Trigger Times: 0\n",
      "Epoch 156/200...      Training loss: 0.00696...      Validation loss: 0.00575\n",
      "Trigger Times: 0\n",
      "Epoch 157/200...      Training loss: 0.00666...      Validation loss: 0.00570\n",
      "Trigger Times: 0\n",
      "Epoch 158/200...      Training loss: 0.00666...      Validation loss: 0.00565\n",
      "Trigger Times: 0\n",
      "Epoch 159/200...      Training loss: 0.00675...      Validation loss: 0.00560\n",
      "Trigger Times: 0\n",
      "Epoch 160/200...      Training loss: 0.00640...      Validation loss: 0.00555\n",
      "Trigger Times: 0\n",
      "Epoch 161/200...      Training loss: 0.00720...      Validation loss: 0.00550\n",
      "Trigger Times: 0\n",
      "Epoch 162/200...      Training loss: 0.00662...      Validation loss: 0.00545\n",
      "Trigger Times: 0\n",
      "Epoch 163/200...      Training loss: 0.00665...      Validation loss: 0.00540\n",
      "Trigger Times: 0\n",
      "Epoch 164/200...      Training loss: 0.00646...      Validation loss: 0.00535\n",
      "Trigger Times: 0\n",
      "Epoch 165/200...      Training loss: 0.00640...      Validation loss: 0.00531\n",
      "Trigger Times: 0\n",
      "Epoch 166/200...      Training loss: 0.00640...      Validation loss: 0.00526\n",
      "Trigger Times: 0\n",
      "Epoch 167/200...      Training loss: 0.00629...      Validation loss: 0.00522\n",
      "Trigger Times: 0\n",
      "Epoch 168/200...      Training loss: 0.00627...      Validation loss: 0.00517\n",
      "Trigger Times: 0\n",
      "Epoch 169/200...      Training loss: 0.00652...      Validation loss: 0.00513\n",
      "Trigger Times: 0\n",
      "Epoch 170/200...      Training loss: 0.00651...      Validation loss: 0.00508\n",
      "Trigger Times: 0\n",
      "Epoch 171/200...      Training loss: 0.00661...      Validation loss: 0.00504\n",
      "Trigger Times: 0\n",
      "Epoch 172/200...      Training loss: 0.00601...      Validation loss: 0.00500\n",
      "Trigger Times: 0\n",
      "Epoch 173/200...      Training loss: 0.00635...      Validation loss: 0.00495\n",
      "Trigger Times: 0\n",
      "Epoch 174/200...      Training loss: 0.00611...      Validation loss: 0.00491\n",
      "Trigger Times: 0\n",
      "Epoch 175/200...      Training loss: 0.00621...      Validation loss: 0.00487\n",
      "Trigger Times: 0\n",
      "Epoch 176/200...      Training loss: 0.00595...      Validation loss: 0.00483\n",
      "Trigger Times: 0\n",
      "Epoch 177/200...      Training loss: 0.00582...      Validation loss: 0.00479\n",
      "Trigger Times: 0\n",
      "Epoch 178/200...      Training loss: 0.00557...      Validation loss: 0.00476\n",
      "Trigger Times: 0\n",
      "Epoch 179/200...      Training loss: 0.00587...      Validation loss: 0.00472\n",
      "Trigger Times: 0\n",
      "Epoch 180/200...      Training loss: 0.00565...      Validation loss: 0.00468\n",
      "Trigger Times: 0\n",
      "Epoch 181/200...      Training loss: 0.00570...      Validation loss: 0.00465\n",
      "Trigger Times: 0\n",
      "Epoch 182/200...      Training loss: 0.00570...      Validation loss: 0.00461\n",
      "Trigger Times: 0\n",
      "Epoch 183/200...      Training loss: 0.00570...      Validation loss: 0.00457\n",
      "Trigger Times: 0\n",
      "Epoch 184/200...      Training loss: 0.00546...      Validation loss: 0.00454\n",
      "Trigger Times: 0\n",
      "Epoch 185/200...      Training loss: 0.00560...      Validation loss: 0.00450\n",
      "Trigger Times: 0\n",
      "Epoch 186/200...      Training loss: 0.00552...      Validation loss: 0.00447\n",
      "Trigger Times: 0\n",
      "Epoch 187/200...      Training loss: 0.00546...      Validation loss: 0.00443\n",
      "Trigger Times: 0\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# For Early Stopping\n",
    "last_loss = 100\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "SECOND_EPOCHS = 200\n",
    "\n",
    "for e in range(SECOND_EPOCHS):\n",
    "    \n",
    "    ######### Training ##########\n",
    "\n",
    "    losses = []\n",
    "    # Batch loop\n",
    "    for inputs, labels in federated_train_loader:\n",
    "        # Location of current batch\n",
    "        worker = inputs.location\n",
    "        # Initialize hidden state and send it to worker\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
    "        # Send model to current worker\n",
    "        second_model.send(worker)\n",
    "        # Setting accumulated gradients to zero before backward step\n",
    "        optimizer.zero_grad()\n",
    "        # Output from the model\n",
    "        output, _ = second_model(inputs.to(torch.long), h)\n",
    "        # print(f\"Output:{output}\")\n",
    "        # Calculate the loss and perform backprop\n",
    "        # print(f\"Output Shape: {output.shape} Labels Shape: {labels.shape}\")\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # # Clipping the gradient to avoid explosion\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        # Backpropagation step\n",
    "        optimizer.step() \n",
    "        # Get the model back to the local worker\n",
    "        second_model.get()\n",
    "        losses.append(loss.get())\n",
    "    \n",
    "    \n",
    "    ######## Evaluation ##########\n",
    "    \n",
    "    # Model in evaluation mode\n",
    "    second_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        test_labels_list = []\n",
    "        eval_losses = []\n",
    "\n",
    "        for inputs, labels in federated_test_loader:\n",
    "            # get current location\n",
    "            worker = inputs.location\n",
    "            # Initialize hidden state and send it to worker\n",
    "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)    \n",
    "            # Send model to worker\n",
    "            second_model.send(worker)\n",
    "            output, _ = second_model(inputs.to(torch.long), h)\n",
    "            # loss = criterion(output.squeeze(), labels.float())\n",
    "            loss = criterion(output, labels.float())\n",
    "            eval_losses.append(loss.get())\n",
    "            preds = output.squeeze().get()\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.get().numpy().astype(int))\n",
    "            # Get the model back to the local worker\n",
    "            second_model.get()\n",
    "\n",
    "    # score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "    # Check test preds\n",
    "\n",
    "    train_loss = sum(losses)/len(losses)\n",
    "    eval_loss = sum(eval_losses)/len(eval_losses)\n",
    "    \n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(eval_loss.item())\n",
    "    \n",
    "    print(\"Epoch {}/{}...  \\\n",
    "    Training loss: {:.5f}...  \\\n",
    "    Validation loss: {:.5f}\".format(e+1, SECOND_EPOCHS, train_loss, eval_loss))\n",
    "        \n",
    "    # Early Stopping\n",
    "    if eval_loss > last_loss:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times: {trigger_times}\")\n",
    "        \n",
    "        if trigger_times >= patience:\n",
    "            print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Trigger Times: 0\")\n",
    "        trigger_times = 0\n",
    "    \n",
    "    last_loss = eval_loss\n",
    "    \n",
    "    second_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Method #2 Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Losses: {train_losses}\")\n",
    "print(f\"Test Losses: {test_losses}\")\n",
    "plt.plot(train_losses, 'r')\n",
    "plt.plot(test_losses, 'g')\n",
    "plt.legend(['Train Loss', 'Eval Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "PATH = \"state_dict_model.pt\"\n",
    "torch.save(second_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing params\n",
    "VOCAB_SIZE = int(test_inputs.max()) + 1\n",
    "TEST_VOCAB_SIZE = TRAIN_VOCAB_SIZE\n",
    "lr = 0.01\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "PATH = \"state_dict_model.pt\"\n",
    "load_model = GRU(vocab_size=TEST_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "\n",
    "# Test Model\n",
    "from sklearn.metrics import f1_score\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(second_model.parameters(), lr=lr)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "    eval_losses = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "        output, _ = second_model(inputs.to(torch.long), h)\n",
    "        # print(f\"Output Shape:{output.shape}\")\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        eval_losses.append(loss)\n",
    "        preds = output.squeeze()\n",
    "        if len(labels) > 1:\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.numpy().astype(int))\n",
    "\n",
    "\n",
    "roc_acc_score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels_list, test_preds)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# Index of largest G-means\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "threshold = thresholds[ix]\n",
    "\n",
    "# Print how many data is being tested\n",
    "print(f\"Amount of test data: {len(test_labels_list)}\")\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "print(f\"ROC Accuracy Score: {roc_acc_score}\")\n",
    "\n",
    "# Normalize probability with threshold\n",
    "test_preds_thresholded = np.where(test_preds > threshold, 1, 0)\n",
    "for i in range(len(test_preds)-1140):\n",
    "    print(\"Test Preds Prob: {}    \\\n",
    "    Test Preds Label: {}  \\\n",
    "    True Label: {}  \\\n",
    "    \".format(test_preds[i], test_preds_thresholded[i], test_labels_list[i]))\n",
    "\n",
    "acc_score = accuracy_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"\\nAccuracy Score: {acc_score}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_labels_list, test_preds_thresholded)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix: \\n{cm}\")\n",
    "\n",
    "tn = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tp = cm[1][1]\n",
    "print(f\"\\nTN: {tn}\\t FP: {fp}\\nFN: {fn}\\t TP: {tp}\\n\")\n",
    "\n",
    "# Precision\n",
    "precision = tp/(tp+fp)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# Recall\n",
    "recall = tp/(tp+fn)\n",
    "print(f\"Recall:{recall}\")\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_score = f1_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "27726a6b9d0b3aecf19ddb8fb5d165165384e9a9dccccc704489801e8c9c2418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('spam_classifier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

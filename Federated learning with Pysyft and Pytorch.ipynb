{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.160008Z",
     "start_time": "2019-06-03T19:33:39.344527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:cuda\n",
      "Torch Ver: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import sqrt, argmax\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "# import opacus\n",
    "# from opacus import PrivacyEngine\n",
    "\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# from syft.frameworks.torch.nn import GRU\n",
    "from handcrafted_GRU import GRU\n",
    "# from opacus.layers import DPGRU\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device:{device}\")\n",
    "\n",
    "print(f\"Torch Ver: {torch.__version__}\")\n",
    "# print(f\"Opacus Ver: {opacus.__version__}\")\n",
    "# print(f\"Syft Ver: {sy.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('indonesian'))\n",
    "# print(f\"STOPWORDS:\\n {STOPWORDS}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # print(f\"\\n\\nOriginal Text: {text}\")\n",
    "    text = text.lower()\n",
    "    # print(f\"\\nCase Lowered Text: {text}\")\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # print(f\"\\nRegexed Text: {text}\")\n",
    "    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n",
    "    # print(f\"\\nStopwords Removed Text: {text}\")\n",
    "    return text\n",
    "\n",
    "def tokenize(text, word_to_idx):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        if word in word_to_idx:\n",
    "            tokens.append(word_to_idx[word])\n",
    "        else:\n",
    "            tokens.append(0)\n",
    "    return tokens\n",
    "\n",
    "def pad_and_truncate(messages, max_length=30):\n",
    "    features = np.zeros((len(messages), max_length), dtype=int)\n",
    "    # pprint(f\"Messages: {messages}\\nFeatures: {features}\")\n",
    "    for i, sms in enumerate(messages):\n",
    "        # print(f\"\\ni: {i}\\nSMS:{sms}\")\n",
    "        if len(sms):\n",
    "            features[i, -len(sms):] = sms[:max_length]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('train_sms_1780.csv', sep=',', names=['Teks', 'Label'], encoding= 'unicode_escape')\n",
    "    data = data.sample(frac = 1)\n",
    "    # Lowercase, remove unnecessary char with regex, remove stop words\n",
    "    data.Teks = data.Teks.apply(clean_text)\n",
    "    # print(data.Teks)\n",
    "    words = set((' '.join(data.Teks)).split())\n",
    "    # print(words)\n",
    "    word_to_idx = {word: i for i, word in enumerate(words, start=1)}\n",
    "    # pprint(word_to_idx)\n",
    "    tokens = data.Teks.apply(lambda x: tokenize(x, word_to_idx))\n",
    "    # print(tokens)\n",
    "    inputs = pad_and_truncate(tokens)\n",
    "    # pprint(inputs)\n",
    "    labels = np.array((data.Label == '1').astype(int))\n",
    "\n",
    "    np.save('labels.npy', labels)\n",
    "    np.save('inputs.npy', inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.207763Z",
     "start_time": "2019-06-03T19:33:40.201292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training params\n",
    "TRAIN_VOCAB_SIZE = int(inputs.max()) + 1\n",
    "EPOCHS = 100\n",
    "CLIP = 5 # gradient clipping - to avoid gradient explosion (frequent in RNNs)\n",
    "lr = 0.01\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.197935Z",
     "start_time": "2019-06-03T19:33:40.186270Z"
    }
   },
   "outputs": [],
   "source": [
    "original_inputs = np.load('inputs.npy')\n",
    "original_labels = np.load('labels.npy')\n",
    "\n",
    "inputs = torch.tensor(original_inputs)\n",
    "labels = torch.tensor(original_labels)\n",
    "\n",
    "# splitting training and test data\n",
    "# 20% of the data will be for Tests.\n",
    "pct_test = 0.2\n",
    "\n",
    "#20% of total data\n",
    "pct_test_count = -int(len(labels)*pct_test)\n",
    "\n",
    "# Get 80% of Train LABELS from left.\n",
    "train_labels = labels[:pct_test_count]\n",
    "# print(f\"Train Labels: [:{pct_test_count}]\")\n",
    "\n",
    "# Get 80% of Train INPUTS from left.\n",
    "train_inputs = inputs[:pct_test_count]\n",
    "# print(f\"Train Inputs: [:{pct_test_count}]\")\n",
    "\n",
    "# Get the rest of the LABEL data for test on the right (20%) \n",
    "test_labels = labels[pct_test_count:]\n",
    "# print(f\"Test Labels: [{pct_test_count}:]\")\n",
    "\n",
    "# Get the rest of the INPUT data for test on the right (20%)\n",
    "test_inputs = inputs[pct_test_count:]\n",
    "# print(f\"Test Inputs: [{pct_test_count}:]\")\n",
    "\n",
    "\n",
    "SAMPLE_SIZE = len(labels)\n",
    "# print(f\"Sample Size: {SAMPLE_SIZE}\")\n",
    "\n",
    "# print(f\"20% of Sample Size: {SAMPLE_SIZE*pct_test}\")\n",
    "\n",
    "# For Local Model Evaluation\n",
    "original_test_inputs = original_inputs[pct_test_count:]\n",
    "original_test_labels = original_labels[pct_test_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VirtualWorkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.591430Z",
     "start_time": "2019-06-03T19:33:41.969220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hook that extends the Pytorch library \n",
    "# to enable all computations with pointers of tensors sent to other workers\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Creating 2 virtual workers Syft v0.2.9\n",
    "anne = sy.VirtualWorker(hook, id=\"anne\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "\n",
    "workers = [anne, bob]\n",
    "\n",
    "# this is done to have the local worker (you on your notebook!) have a registry\n",
    "# of objects like every other workers, which is disabled by default but needed here\n",
    "# sy.local_worker.is_client_worker = False\n",
    "\n",
    "\n",
    "# threshold indexes for dataset split (one half for Bob, other half for Anne)\n",
    "train_idx = int(len(train_labels)/2)\n",
    "test_idx = int(len(test_labels)/2)\n",
    "\n",
    "# Sending toy datasets to virtual workers\n",
    "bob_train_dataset = sy.BaseDataset(train_inputs[:train_idx], train_labels[:train_idx]).send(bob)\n",
    "anne_train_dataset = sy.BaseDataset(train_inputs[train_idx:], train_labels[train_idx:]).send(anne)\n",
    "bob_test_dataset = sy.BaseDataset(test_inputs[:test_idx], test_labels[:test_idx]).send(bob)\n",
    "anne_test_dataset = sy.BaseDataset(test_inputs[test_idx:], test_labels[test_idx:]).send(anne)\n",
    "\n",
    "# print(f\"Train Index: {train_idx}\")\n",
    "# print(f\"Test Index: {test_idx}\")\n",
    "\n",
    "# print(f\"Anne's Data Amount: {len(train_inputs[train_idx:])}\")\n",
    "# print(f\"Bob's Data Amount: {len(train_inputs[:train_idx])}\")\n",
    "\n",
    "\n",
    "# Creating federated datasets, an extension of Pytorch TensorDataset class \n",
    "# for TRAINING METHOD #1 (with aggregation)\n",
    "bob_federated_train_dataset = sy.FederatedDataset([bob_train_dataset])\n",
    "anne_federated_train_dataset = sy.FederatedDataset([anne_train_dataset])\n",
    "bob_federated_test_dataset = sy.FederatedDataset([bob_test_dataset])\n",
    "anne_federated_test_dataset = sy.FederatedDataset([anne_test_dataset])\n",
    "\n",
    "\n",
    "merged_test_dataset = list(zip(original_test_inputs, original_test_labels))\n",
    "\n",
    "# print(f\"Input:{original_test_inputs[1]}\\t Label:{original_test_labels[1]}\")\n",
    "# print(merged_test_dataset[0])\n",
    "\n",
    "def collate_batch(batch):\n",
    "        label_list, text_list = [], []\n",
    "        for (_label, _text) in batch:\n",
    "                label_list.append(_label)\n",
    "                text_list.append(_text)\n",
    "        return label_list, text_list\n",
    "\n",
    "original_test_dataloader = DataLoader(merged_test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.638046Z",
     "start_time": "2019-06-03T19:33:42.617601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiating the model\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# model = GRU(vocab_size=TRAIN_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "# torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "def make_model():\n",
    "    model = GRU(vocab_size=TRAIN_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "    return model\n",
    "    \n",
    "local_model = make_model()\n",
    "\n",
    "models, train_dataloaders, test_dataloaders, optimizers, privacy_engines = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching model, dataloaders, and optimizers to each worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in workers:\n",
    "    model = make_model()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    model.send(worker)\n",
    "    if(worker == anne):\n",
    "        train_dataset = anne_federated_train_dataset\n",
    "        test_dataset = anne_federated_test_dataset\n",
    "    elif(worker == bob):\n",
    "        train_dataset = bob_federated_train_dataset\n",
    "        test_dataset = bob_federated_test_dataset\n",
    "\n",
    "\n",
    "    train_dataloader = sy.FederatedDataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dataloader = sy.FederatedDataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    models.append(model)\n",
    "    train_dataloaders.append(train_dataloader)\n",
    "    test_dataloaders.append(test_dataloader)\n",
    "    optimizers.append(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to aggregate remote models and to send new updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_aggregation(local_model, models):\n",
    "    with torch.no_grad():\n",
    "        for local_param, *remote_params in zip(*([local_model.parameters()] + [model.parameters() for model in models])):\n",
    "            param_stack = torch.zeros(*remote_params[0].shape)\n",
    "            for remote_param in remote_params:\n",
    "                param_stack += remote_param.copy().get()\n",
    "                # print(f\"Param Stack Sum: {param_stack}\")\n",
    "            param_stack /= len(remote_params)\n",
    "            # print(f\"Param Stack Division: {param_stack}\")\n",
    "            local_param.set_(param_stack)\n",
    "\n",
    "def send_new_models(local_model, models):\n",
    "    with torch.no_grad():\n",
    "        for remote_model in models:\n",
    "            for new_param, remote_param in zip(local_model.parameters(), remote_model.parameters()):\n",
    "                worker = remote_param.location\n",
    "                remote_value = new_param.send(worker)\n",
    "                remote_param.set_(remote_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [[], []]\n",
    "test_losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    # 1. Send new version of the model\n",
    "    send_new_models(local_model, models)\n",
    "\n",
    "    # 2. Train remotely the models\n",
    "    for i, worker in enumerate(workers):\n",
    "        train_dataloader = train_dataloaders[i]\n",
    "        model = models[i]\n",
    "        optimizer = optimizers[i]\n",
    "        \n",
    "        model.train()\n",
    "        criterion = nn.BCELoss() # for two class classification\n",
    "        losses = []   \n",
    "    \n",
    "        for data, target in train_dataloader:            \n",
    "            data = data.to(torch.long)\n",
    "            h = torch.Tensor(torch.zeros(BATCH_SIZE, HIDDEN_DIM)).send(worker)  \n",
    "            \n",
    "            # Call zero grad to clear previous gradient before every training passses.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print(f\"Data:{data}\\nTarget: {target}\\n\")\n",
    "\n",
    "            # print(f\"Worker: {worker}\\nWorker Objects: {worker.object_store._objects}\")\n",
    "\n",
    "            output, _ = model(data.to(torch.long), h)\n",
    "            # print(f\"Output: {output}\")\n",
    "            loss = criterion(output.squeeze(), target.float())\n",
    "            loss.backward()\n",
    "\n",
    "            # # Clipping the gradient to avoid explosion\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            losses.append(loss.get()) \n",
    "            optimizer.step()\n",
    "\n",
    "        sy.local_worker.clear_objects()\n",
    "        \n",
    "\n",
    "        train_loss = sum(losses) / len(losses)\n",
    "        train_losses[i].append(train_loss.item())\n",
    "\n",
    "        print(\n",
    "            f\"[{worker.id}]\\t\"\n",
    "            f\"Train Epoch: {epoch} \\t\"\n",
    "            f\"Train Loss: {train_loss:.4f} \")\n",
    "\n",
    "    # 3. Federated aggregation of the updated models\n",
    "    federated_aggregation(local_model, models)\n",
    "\n",
    "\n",
    "def eval(epoch, last_loss, trigger_times, patience):\n",
    "    # 4. Evaluate the model\n",
    "    local_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        test_labels_list = []\n",
    "        eval_losses = []\n",
    "\n",
    "        for inputs, labels in original_test_dataloader:\n",
    "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "            output, _ = local_model(torch.LongTensor(inputs), h)\n",
    "            criterion = nn.BCELoss()\n",
    "            labels = torch.LongTensor(labels)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            eval_losses.append(loss)\n",
    "            preds = output.squeeze()\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.numpy().astype(int))\n",
    "    \n",
    "    score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "    eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    test_losses.append(eval_loss.item())\n",
    "\n",
    "            \n",
    "    # Early Stopping\n",
    "    if eval_loss > last_loss:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times: {trigger_times}\")\n",
    "        \n",
    "        if trigger_times >= patience:\n",
    "            print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Trigger Times: 0\")\n",
    "        trigger_times = 0\n",
    "    \n",
    "    last_loss = eval_loss\n",
    "\n",
    "    print(\n",
    "        f\"Eval Epoch: {epoch} \\t\"\n",
    "        f\"AUC: {score:.3%} \\t\"\n",
    "        f\"Eval Loss: {eval_loss:.4f} \\n\\n\")\n",
    "\n",
    "    return last_loss, trigger_times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[anne]\tTrain Epoch: 0 \tTrain Loss: 0.7053 \n",
      "[bob]\tTrain Epoch: 0 \tTrain Loss: 0.7120 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 0 \tAUC: 55.166% \tEval Loss: 0.6800 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 1 \tTrain Loss: 0.6937 \n",
      "[bob]\tTrain Epoch: 1 \tTrain Loss: 0.7001 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 1 \tAUC: 56.054% \tEval Loss: 0.6691 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 2 \tTrain Loss: 0.6783 \n",
      "[bob]\tTrain Epoch: 2 \tTrain Loss: 0.6909 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 2 \tAUC: 57.042% \tEval Loss: 0.6591 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 3 \tTrain Loss: 0.6712 \n",
      "[bob]\tTrain Epoch: 3 \tTrain Loss: 0.6768 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 3 \tAUC: 57.631% \tEval Loss: 0.6504 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 4 \tTrain Loss: 0.6611 \n",
      "[bob]\tTrain Epoch: 4 \tTrain Loss: 0.6668 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 4 \tAUC: 58.393% \tEval Loss: 0.6428 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 5 \tTrain Loss: 0.6554 \n",
      "[bob]\tTrain Epoch: 5 \tTrain Loss: 0.6590 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 5 \tAUC: 59.385% \tEval Loss: 0.6364 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 6 \tTrain Loss: 0.6472 \n",
      "[bob]\tTrain Epoch: 6 \tTrain Loss: 0.6520 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 6 \tAUC: 60.135% \tEval Loss: 0.6303 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 7 \tTrain Loss: 0.6337 \n",
      "[bob]\tTrain Epoch: 7 \tTrain Loss: 0.6505 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 7 \tAUC: 60.841% \tEval Loss: 0.6236 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 8 \tTrain Loss: 0.6282 \n",
      "[bob]\tTrain Epoch: 8 \tTrain Loss: 0.6406 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 8 \tAUC: 61.514% \tEval Loss: 0.6190 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 9 \tTrain Loss: 0.6249 \n",
      "[bob]\tTrain Epoch: 9 \tTrain Loss: 0.6377 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 9 \tAUC: 62.309% \tEval Loss: 0.6138 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 10 \tTrain Loss: 0.6206 \n",
      "[bob]\tTrain Epoch: 10 \tTrain Loss: 0.6365 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 10 \tAUC: 62.930% \tEval Loss: 0.6104 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 11 \tTrain Loss: 0.6145 \n",
      "[bob]\tTrain Epoch: 11 \tTrain Loss: 0.6286 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 11 \tAUC: 63.358% \tEval Loss: 0.6063 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 12 \tTrain Loss: 0.6075 \n",
      "[bob]\tTrain Epoch: 12 \tTrain Loss: 0.6255 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 12 \tAUC: 64.003% \tEval Loss: 0.6018 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 13 \tTrain Loss: 0.6032 \n",
      "[bob]\tTrain Epoch: 13 \tTrain Loss: 0.6254 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 13 \tAUC: 64.499% \tEval Loss: 0.5991 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 14 \tTrain Loss: 0.6064 \n",
      "[bob]\tTrain Epoch: 14 \tTrain Loss: 0.6214 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 14 \tAUC: 65.144% \tEval Loss: 0.5967 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 15 \tTrain Loss: 0.6019 \n",
      "[bob]\tTrain Epoch: 15 \tTrain Loss: 0.6180 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 15 \tAUC: 65.410% \tEval Loss: 0.5954 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 16 \tTrain Loss: 0.5917 \n",
      "[bob]\tTrain Epoch: 16 \tTrain Loss: 0.6160 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 16 \tAUC: 65.826% \tEval Loss: 0.5917 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 17 \tTrain Loss: 0.5907 \n",
      "[bob]\tTrain Epoch: 17 \tTrain Loss: 0.6117 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 17 \tAUC: 66.403% \tEval Loss: 0.5890 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 18 \tTrain Loss: 0.5836 \n",
      "[bob]\tTrain Epoch: 18 \tTrain Loss: 0.6130 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 18 \tAUC: 66.866% \tEval Loss: 0.5876 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 19 \tTrain Loss: 0.5805 \n",
      "[bob]\tTrain Epoch: 19 \tTrain Loss: 0.6094 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 19 \tAUC: 67.262% \tEval Loss: 0.5850 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 20 \tTrain Loss: 0.5754 \n",
      "[bob]\tTrain Epoch: 20 \tTrain Loss: 0.6061 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 20 \tAUC: 67.717% \tEval Loss: 0.5830 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 21 \tTrain Loss: 0.5728 \n",
      "[bob]\tTrain Epoch: 21 \tTrain Loss: 0.6088 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 21 \tAUC: 68.024% \tEval Loss: 0.5816 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 22 \tTrain Loss: 0.5799 \n",
      "[bob]\tTrain Epoch: 22 \tTrain Loss: 0.6039 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 22 \tAUC: 68.363% \tEval Loss: 0.5791 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 23 \tTrain Loss: 0.5695 \n",
      "[bob]\tTrain Epoch: 23 \tTrain Loss: 0.6038 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 23 \tAUC: 68.685% \tEval Loss: 0.5775 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 24 \tTrain Loss: 0.5662 \n",
      "[bob]\tTrain Epoch: 24 \tTrain Loss: 0.6006 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 24 \tAUC: 68.972% \tEval Loss: 0.5766 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 25 \tTrain Loss: 0.5658 \n",
      "[bob]\tTrain Epoch: 25 \tTrain Loss: 0.6014 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 25 \tAUC: 69.185% \tEval Loss: 0.5750 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 26 \tTrain Loss: 0.5583 \n",
      "[bob]\tTrain Epoch: 26 \tTrain Loss: 0.5975 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 26 \tAUC: 69.403% \tEval Loss: 0.5733 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 27 \tTrain Loss: 0.5597 \n",
      "[bob]\tTrain Epoch: 27 \tTrain Loss: 0.5933 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 27 \tAUC: 69.605% \tEval Loss: 0.5726 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 28 \tTrain Loss: 0.5491 \n",
      "[bob]\tTrain Epoch: 28 \tTrain Loss: 0.5943 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 28 \tAUC: 69.790% \tEval Loss: 0.5709 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 29 \tTrain Loss: 0.5468 \n",
      "[bob]\tTrain Epoch: 29 \tTrain Loss: 0.5962 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 29 \tAUC: 69.948% \tEval Loss: 0.5697 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 30 \tTrain Loss: 0.5487 \n",
      "[bob]\tTrain Epoch: 30 \tTrain Loss: 0.5865 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 30 \tAUC: 70.173% \tEval Loss: 0.5687 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 31 \tTrain Loss: 0.5446 \n",
      "[bob]\tTrain Epoch: 31 \tTrain Loss: 0.5875 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 31 \tAUC: 70.423% \tEval Loss: 0.5677 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 32 \tTrain Loss: 0.5429 \n",
      "[bob]\tTrain Epoch: 32 \tTrain Loss: 0.5877 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 32 \tAUC: 70.593% \tEval Loss: 0.5657 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 33 \tTrain Loss: 0.5317 \n",
      "[bob]\tTrain Epoch: 33 \tTrain Loss: 0.5846 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 33 \tAUC: 70.819% \tEval Loss: 0.5645 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 34 \tTrain Loss: 0.5332 \n",
      "[bob]\tTrain Epoch: 34 \tTrain Loss: 0.5841 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 34 \tAUC: 71.109% \tEval Loss: 0.5645 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 35 \tTrain Loss: 0.5373 \n",
      "[bob]\tTrain Epoch: 35 \tTrain Loss: 0.5820 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 35 \tAUC: 71.206% \tEval Loss: 0.5619 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 36 \tTrain Loss: 0.5199 \n",
      "[bob]\tTrain Epoch: 36 \tTrain Loss: 0.5801 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 36 \tAUC: 71.307% \tEval Loss: 0.5603 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 37 \tTrain Loss: 0.5284 \n",
      "[bob]\tTrain Epoch: 37 \tTrain Loss: 0.5789 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 37 \tAUC: 71.545% \tEval Loss: 0.5586 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 38 \tTrain Loss: 0.5215 \n",
      "[bob]\tTrain Epoch: 38 \tTrain Loss: 0.5758 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 38 \tAUC: 71.742% \tEval Loss: 0.5588 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 39 \tTrain Loss: 0.5190 \n",
      "[bob]\tTrain Epoch: 39 \tTrain Loss: 0.5781 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 39 \tAUC: 71.887% \tEval Loss: 0.5577 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 40 \tTrain Loss: 0.5176 \n",
      "[bob]\tTrain Epoch: 40 \tTrain Loss: 0.5728 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 40 \tAUC: 72.113% \tEval Loss: 0.5569 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 41 \tTrain Loss: 0.5123 \n",
      "[bob]\tTrain Epoch: 41 \tTrain Loss: 0.5706 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 41 \tAUC: 72.279% \tEval Loss: 0.5553 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 42 \tTrain Loss: 0.5105 \n",
      "[bob]\tTrain Epoch: 42 \tTrain Loss: 0.5655 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 42 \tAUC: 72.529% \tEval Loss: 0.5535 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 43 \tTrain Loss: 0.5058 \n",
      "[bob]\tTrain Epoch: 43 \tTrain Loss: 0.5644 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 43 \tAUC: 72.698% \tEval Loss: 0.5529 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 44 \tTrain Loss: 0.5097 \n",
      "[bob]\tTrain Epoch: 44 \tTrain Loss: 0.5671 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 44 \tAUC: 72.863% \tEval Loss: 0.5530 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 45 \tTrain Loss: 0.5126 \n",
      "[bob]\tTrain Epoch: 45 \tTrain Loss: 0.5597 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 45 \tAUC: 73.077% \tEval Loss: 0.5506 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 46 \tTrain Loss: 0.4970 \n",
      "[bob]\tTrain Epoch: 46 \tTrain Loss: 0.5620 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 46 \tAUC: 73.194% \tEval Loss: 0.5499 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 47 \tTrain Loss: 0.4964 \n",
      "[bob]\tTrain Epoch: 47 \tTrain Loss: 0.5613 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 47 \tAUC: 73.376% \tEval Loss: 0.5493 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 48 \tTrain Loss: 0.4955 \n",
      "[bob]\tTrain Epoch: 48 \tTrain Loss: 0.5513 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 48 \tAUC: 73.521% \tEval Loss: 0.5474 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 49 \tTrain Loss: 0.4947 \n",
      "[bob]\tTrain Epoch: 49 \tTrain Loss: 0.5544 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 49 \tAUC: 73.638% \tEval Loss: 0.5468 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 50 \tTrain Loss: 0.4907 \n",
      "[bob]\tTrain Epoch: 50 \tTrain Loss: 0.5513 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 50 \tAUC: 73.888% \tEval Loss: 0.5473 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 51 \tTrain Loss: 0.4889 \n",
      "[bob]\tTrain Epoch: 51 \tTrain Loss: 0.5456 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 51 \tAUC: 74.033% \tEval Loss: 0.5450 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 52 \tTrain Loss: 0.4902 \n",
      "[bob]\tTrain Epoch: 52 \tTrain Loss: 0.5445 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 52 \tAUC: 74.190% \tEval Loss: 0.5431 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 53 \tTrain Loss: 0.4801 \n",
      "[bob]\tTrain Epoch: 53 \tTrain Loss: 0.5377 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 53 \tAUC: 74.307% \tEval Loss: 0.5433 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 54 \tTrain Loss: 0.4750 \n",
      "[bob]\tTrain Epoch: 54 \tTrain Loss: 0.5373 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 54 \tAUC: 74.448% \tEval Loss: 0.5422 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 55 \tTrain Loss: 0.4725 \n",
      "[bob]\tTrain Epoch: 55 \tTrain Loss: 0.5372 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 55 \tAUC: 74.586% \tEval Loss: 0.5419 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 56 \tTrain Loss: 0.4610 \n",
      "[bob]\tTrain Epoch: 56 \tTrain Loss: 0.5355 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 56 \tAUC: 74.723% \tEval Loss: 0.5406 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 57 \tTrain Loss: 0.4665 \n",
      "[bob]\tTrain Epoch: 57 \tTrain Loss: 0.5295 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 57 \tAUC: 74.787% \tEval Loss: 0.5385 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 58 \tTrain Loss: 0.4752 \n",
      "[bob]\tTrain Epoch: 58 \tTrain Loss: 0.5290 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 58 \tAUC: 74.876% \tEval Loss: 0.5372 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 59 \tTrain Loss: 0.4565 \n",
      "[bob]\tTrain Epoch: 59 \tTrain Loss: 0.5318 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 59 \tAUC: 74.949% \tEval Loss: 0.5370 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 60 \tTrain Loss: 0.4598 \n",
      "[bob]\tTrain Epoch: 60 \tTrain Loss: 0.5243 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 60 \tAUC: 75.037% \tEval Loss: 0.5368 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 61 \tTrain Loss: 0.4535 \n",
      "[bob]\tTrain Epoch: 61 \tTrain Loss: 0.5257 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 61 \tAUC: 75.102% \tEval Loss: 0.5343 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 62 \tTrain Loss: 0.4552 \n",
      "[bob]\tTrain Epoch: 62 \tTrain Loss: 0.5237 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 62 \tAUC: 75.199% \tEval Loss: 0.5342 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 63 \tTrain Loss: 0.4544 \n",
      "[bob]\tTrain Epoch: 63 \tTrain Loss: 0.5162 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 63 \tAUC: 75.324% \tEval Loss: 0.5342 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 64 \tTrain Loss: 0.4558 \n",
      "[bob]\tTrain Epoch: 64 \tTrain Loss: 0.5140 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 64 \tAUC: 75.433% \tEval Loss: 0.5332 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 65 \tTrain Loss: 0.4478 \n",
      "[bob]\tTrain Epoch: 65 \tTrain Loss: 0.5145 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 65 \tAUC: 75.497% \tEval Loss: 0.5311 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 66 \tTrain Loss: 0.4477 \n",
      "[bob]\tTrain Epoch: 66 \tTrain Loss: 0.5159 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 66 \tAUC: 75.570% \tEval Loss: 0.5305 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 67 \tTrain Loss: 0.4393 \n",
      "[bob]\tTrain Epoch: 67 \tTrain Loss: 0.5155 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 67 \tAUC: 75.650% \tEval Loss: 0.5297 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 68 \tTrain Loss: 0.4462 \n",
      "[bob]\tTrain Epoch: 68 \tTrain Loss: 0.5067 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 68 \tAUC: 75.747% \tEval Loss: 0.5294 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 69 \tTrain Loss: 0.4362 \n",
      "[bob]\tTrain Epoch: 69 \tTrain Loss: 0.5115 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 69 \tAUC: 75.917% \tEval Loss: 0.5307 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 70 \tTrain Loss: 0.4412 \n",
      "[bob]\tTrain Epoch: 70 \tTrain Loss: 0.5023 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 70 \tAUC: 76.017% \tEval Loss: 0.5265 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 71 \tTrain Loss: 0.4361 \n",
      "[bob]\tTrain Epoch: 71 \tTrain Loss: 0.5030 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 71 \tAUC: 76.110% \tEval Loss: 0.5265 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 72 \tTrain Loss: 0.4329 \n",
      "[bob]\tTrain Epoch: 72 \tTrain Loss: 0.4998 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 72 \tAUC: 76.171% \tEval Loss: 0.5253 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 73 \tTrain Loss: 0.4178 \n",
      "[bob]\tTrain Epoch: 73 \tTrain Loss: 0.4935 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 73 \tAUC: 76.215% \tEval Loss: 0.5244 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 74 \tTrain Loss: 0.4282 \n",
      "[bob]\tTrain Epoch: 74 \tTrain Loss: 0.4921 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 74 \tAUC: 76.275% \tEval Loss: 0.5237 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 75 \tTrain Loss: 0.4218 \n",
      "[bob]\tTrain Epoch: 75 \tTrain Loss: 0.4934 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 75 \tAUC: 76.332% \tEval Loss: 0.5224 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 76 \tTrain Loss: 0.4227 \n",
      "[bob]\tTrain Epoch: 76 \tTrain Loss: 0.4852 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 76 \tAUC: 76.413% \tEval Loss: 0.5212 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 77 \tTrain Loss: 0.4127 \n",
      "[bob]\tTrain Epoch: 77 \tTrain Loss: 0.4874 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 77 \tAUC: 76.493% \tEval Loss: 0.5213 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 78 \tTrain Loss: 0.4077 \n",
      "[bob]\tTrain Epoch: 78 \tTrain Loss: 0.4829 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 78 \tAUC: 76.554% \tEval Loss: 0.5197 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 79 \tTrain Loss: 0.4097 \n",
      "[bob]\tTrain Epoch: 79 \tTrain Loss: 0.4762 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 79 \tAUC: 76.695% \tEval Loss: 0.5189 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 80 \tTrain Loss: 0.4102 \n",
      "[bob]\tTrain Epoch: 80 \tTrain Loss: 0.4731 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 80 \tAUC: 76.812% \tEval Loss: 0.5180 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 81 \tTrain Loss: 0.4036 \n",
      "[bob]\tTrain Epoch: 81 \tTrain Loss: 0.4738 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 81 \tAUC: 76.836% \tEval Loss: 0.5185 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 82 \tTrain Loss: 0.4073 \n",
      "[bob]\tTrain Epoch: 82 \tTrain Loss: 0.4772 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 82 \tAUC: 76.965% \tEval Loss: 0.5154 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 83 \tTrain Loss: 0.4012 \n",
      "[bob]\tTrain Epoch: 83 \tTrain Loss: 0.4750 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 83 \tAUC: 77.026% \tEval Loss: 0.5157 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 84 \tTrain Loss: 0.4036 \n",
      "[bob]\tTrain Epoch: 84 \tTrain Loss: 0.4768 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 84 \tAUC: 77.070% \tEval Loss: 0.5148 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 85 \tTrain Loss: 0.4035 \n",
      "[bob]\tTrain Epoch: 85 \tTrain Loss: 0.4628 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 85 \tAUC: 77.114% \tEval Loss: 0.5145 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 86 \tTrain Loss: 0.3871 \n",
      "[bob]\tTrain Epoch: 86 \tTrain Loss: 0.4722 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 86 \tAUC: 77.143% \tEval Loss: 0.5141 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 87 \tTrain Loss: 0.3958 \n",
      "[bob]\tTrain Epoch: 87 \tTrain Loss: 0.4659 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 87 \tAUC: 77.215% \tEval Loss: 0.5127 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 88 \tTrain Loss: 0.3863 \n",
      "[bob]\tTrain Epoch: 88 \tTrain Loss: 0.4613 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 88 \tAUC: 77.260% \tEval Loss: 0.5120 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 89 \tTrain Loss: 0.3900 \n",
      "[bob]\tTrain Epoch: 89 \tTrain Loss: 0.4523 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 89 \tAUC: 77.336% \tEval Loss: 0.5106 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 90 \tTrain Loss: 0.4038 \n",
      "[bob]\tTrain Epoch: 90 \tTrain Loss: 0.4551 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 90 \tAUC: 77.425% \tEval Loss: 0.5094 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 91 \tTrain Loss: 0.3865 \n",
      "[bob]\tTrain Epoch: 91 \tTrain Loss: 0.4529 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 91 \tAUC: 77.506% \tEval Loss: 0.5090 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 92 \tTrain Loss: 0.3836 \n",
      "[bob]\tTrain Epoch: 92 \tTrain Loss: 0.4540 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 92 \tAUC: 77.538% \tEval Loss: 0.5084 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 93 \tTrain Loss: 0.3860 \n",
      "[bob]\tTrain Epoch: 93 \tTrain Loss: 0.4593 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 93 \tAUC: 77.582% \tEval Loss: 0.5085 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 94 \tTrain Loss: 0.3758 \n",
      "[bob]\tTrain Epoch: 94 \tTrain Loss: 0.4522 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 94 \tAUC: 77.562% \tEval Loss: 0.5069 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 95 \tTrain Loss: 0.3735 \n",
      "[bob]\tTrain Epoch: 95 \tTrain Loss: 0.4508 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 95 \tAUC: 77.635% \tEval Loss: 0.5064 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 96 \tTrain Loss: 0.3818 \n",
      "[bob]\tTrain Epoch: 96 \tTrain Loss: 0.4500 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 96 \tAUC: 77.711% \tEval Loss: 0.5055 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 97 \tTrain Loss: 0.3696 \n",
      "[bob]\tTrain Epoch: 97 \tTrain Loss: 0.4520 \n",
      "Trigger Times: 1\n",
      "Eval Epoch: 97 \tAUC: 77.743% \tEval Loss: 0.5056 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 98 \tTrain Loss: 0.3660 \n",
      "[bob]\tTrain Epoch: 98 \tTrain Loss: 0.4364 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 98 \tAUC: 77.772% \tEval Loss: 0.5034 \n",
      "\n",
      "\n",
      "[anne]\tTrain Epoch: 99 \tTrain Loss: 0.3751 \n",
      "[bob]\tTrain Epoch: 99 \tTrain Loss: 0.4377 \n",
      "Trigger Times: 0\n",
      "Eval Epoch: 99 \tAUC: 77.808% \tEval Loss: 0.5027 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Early Stopping\n",
    "last_loss = 100\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    last_loss, trigger_times = eval(epoch, last_loss, trigger_times, patience)\n",
    "    if trigger_times >= patience:\n",
    "        print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses[0], 'r')\n",
    "plt.plot(train_losses[1], 'g')\n",
    "plt.plot(test_losses, 'b')\n",
    "plt.legend(['Training Loss Anne', 'Training Loss Bob' , 'Eval Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "PATH = \"local_state_dict_model.pt\"\n",
    "torch.save(local_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test_sms_600.csv', sep=',', names=['Teks', 'Label'], encoding= 'unicode_escape')\n",
    "data = data.sample(frac = 1)\n",
    "# Lowercase, remove unnecessary char with regex, remove stop words\n",
    "data.Teks = data.Teks.apply(clean_text)\n",
    "#     print(data.Teks)\n",
    "words = set((' '.join(data.Teks)).split())\n",
    "#     print(words)\n",
    "word_to_idx = {word: i for i, word in enumerate(words, start=1)}\n",
    "#     pprint(word_to_idx)\n",
    "tokens = data.Teks.apply(lambda x: tokenize(x, word_to_idx))\n",
    "#     print(tokens)\n",
    "inputs = pad_and_truncate(tokens)\n",
    "#     pprint(inputs)\n",
    "labels = np.array((data.Label == '1').astype(int))\n",
    "\n",
    "np.save('test_labels.npy', labels)\n",
    "np.save('test_inputs.npy', inputs)\n",
    "\n",
    "test_inputs = torch.tensor(np.load('test_inputs.npy'))\n",
    "test_labels = torch.tensor(np.load('test_labels.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing params\n",
    "VOCAB_SIZE = int(test_inputs.max()) + 1\n",
    "TEST_VOCAB_SIZE = TRAIN_VOCAB_SIZE\n",
    "lr = 0.01\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (embedding): Embedding(4552, 50)\n",
       "  (gru_cell): GRU_Cell(\n",
       "    (fc_ir): Linear(in_features=50, out_features=10, bias=True)\n",
       "    (fc_hr): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (fc_iz): Linear(in_features=50, out_features=10, bias=True)\n",
       "    (fc_hz): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (fc_in): Linear(in_features=50, out_features=10, bias=True)\n",
       "    (fc_hn): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"local_state_dict_model.pt\"\n",
    "load_model = GRU(vocab_size=TEST_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1693,  301,\n",
      "          949, 2172,  885,  335, 1393,  545],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  843, 1350, 1804,  408,\n",
      "         1027, 1111, 1183, 1027,  680,  392],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  654,  139, 1788,  931, 1973, 1378, 2025,\n",
      "          588,  680, 1396,  272, 1996, 2094],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1402,  474,  672,  486, 1908, 1292,  904,  988, 1267,  996,\n",
      "          474, 1352,  951,  272, 2246, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1693,  388, 2322],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1438, 2211,  165,  946,  820,  578,  481,  821,\n",
      "          252, 1567, 1801,  923,  153, 2295],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  435,  844,  660, 1719, 1498, 1719,  931,\n",
      "          273, 2214,  277,  433, 2127, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          384,  464, 1506,  797, 1728,  502,  833, 1261, 1718, 2207,  857,   23,\n",
      "         1292, 1516, 1078,   80,  417, 1042],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1658, 1720,  716, 1524,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1156, 2350,\n",
      "         1752, 2350, 1873,  679, 2278, 2299],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2062,   45,\n",
      "          514, 2031, 2197, 1931, 1015,  432],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  850,  213,  672, 1715,  469,\n",
      "         1071, 1079, 2169,  272,  171,  553],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1680,  975, 1027, 1334, 1860, 2292, 2177,\n",
      "         1079, 1051, 1593, 1628,  269, 1709],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1047, 1540,  271,  384, 2384, 2117,\n",
      "          202,  423, 1103, 1364, 2147, 2210],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 2302,  186,  672,  680,\n",
      "          111,  380,  272, 2022, 1214, 1001],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2347,   88],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1563, 1498,\n",
      "         2086, 1492,  763, 1340,  660, 2316, 1852, 1178, 1567, 1730, 2202, 2167,\n",
      "         1380, 2183, 1236, 1481, 1443, 1447],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1547,  931,  256, 2391,  788, 1829,  374, 1367, 1860, 1402, 1573,\n",
      "         1562,  247,  680, 1645,  403,  241],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1960, 1895, 2392,   32],\n",
      "        [   0,    0,    0,    0,    0, 2349,  191,  243,  699, 1679,  745, 2000,\n",
      "          396,   89, 2085, 1823,  146, 1465,  891, 1971, 1692, 2085, 1823, 1897,\n",
      "          117, 1479,  185,  201, 1823, 1123],\n",
      "        [1579, 1010, 1864, 1714, 2063,  619,  870,  427, 1790, 2348,  293,  322,\n",
      "         2357, 1573,  460,  565,  948,  637,  789, 2185,  803, 1737,  292, 1292,\n",
      "         1588, 1075, 2063, 1804, 1573, 1280],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1127, 2307,  938, 1699, 1337, 1962,  673,  314,   92,\n",
      "          520, 2008,  544, 2089, 1380,  325],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1438, 2211, 1377, 2247, 1872,  820,  578,  481, 1765,  183,  765, 1089,\n",
      "          809, 1013, 1607,  152, 1393,  258],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  615,  425, 1159,  416,  369,  303, 1876,  289,\n",
      "         2310, 1554,  504, 1139,  854, 1398],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1402,  514,   23,  265,   41,  454, 2156,  368,  901,  341,\n",
      "         2355, 1115, 1276, 1402, 1140, 2291],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  503, 2225, 2064, 1716, 1563, 2214, 1837, 1768, 1852,\n",
      "          660, 1719, 1498, 1719,  272, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1658, 2233, 1939, 1211, 1080,  915,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1658, 1166,  512, 1259,   38,  512,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  829, 2057,\n",
      "         1967,  988, 1779,  656,  714, 1614]], dtype=torch.int32) Labels: tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "i: 1\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  556,  497, 1210, 1682, 1050, 2141, 1431,   99, 1456,\n",
      "         1050, 1431, 1498,   16, 2215,  272],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1983,  464,  897,\n",
      "           23, 1506, 1456,  642, 1292,  864,  885,  864,  938,  672, 2273, 2291,\n",
      "          857,  904, 1020, 1423, 2283, 1393],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  654,\n",
      "         1350,   23, 2355, 1707, 1779,  965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1494, 2143, 1421, 1533],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2123,  684],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2225,    5,\n",
      "           23, 1687,  833, 1698,  252, 2230],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1027, 2370,  282, 1893, 1634,  366, 1567,  904, 1027, 1375,\n",
      "          384, 1858, 1257, 1801, 1407, 1476],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1089,\n",
      "         2392,    6,  391, 1089, 2150,  279],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  710,  247, 1050,  672,  486, 1596,  672,  486,  518,\n",
      "         1306, 1826, 1539, 2207, 1057,  272],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  304,  684,  967],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1408,  186,  105,  672,\n",
      "          680,  111,  272, 1257, 1214, 2108],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  417, 1042, 1188,   41, 1531, 1188,  132, 1221,  154,\n",
      "         2355, 1573, 1172, 1027,  154,  566],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 2251,  741, 1644, 2008],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1674, 1116, 1745,    8, 1178,\n",
      "         1145,  660, 1568, 1852,  272, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  851,  132, 1531, 1122,  464,  958,  886,\n",
      "         1855, 1182,   39, 1297,  729, 2234],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1845,  845,  350,  717, 2288, 1313, 2244, 1420, 1489,   17,\n",
      "         2063,  983, 2288, 1313, 1733,  412],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          383,  298, 1265, 1353, 2020,  988],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  547,  108, 1502,  393,\n",
      "         1440,  302, 2020, 1021,  938, 1719],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1201, 1804, 1110,  675, 2391,  788,  374, 1367, 1573, 1562,  680, 1378,\n",
      "         2109,  433, 1027,  680, 1628, 1844],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  491,  227, 1081, 2163,\n",
      "         1817, 1954, 2382, 1361, 2379, 2127],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1831,   31,\n",
      "          119, 1267,  252, 1291, 1851,  743,  247,  514,   23,  265, 1286, 2273,\n",
      "         1582, 1670, 1172, 1027, 1050, 2122],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1557, 1350,   23, 2355, 1035, 2058,\n",
      "         1509, 1050, 2141, 1687,   23, 2396],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1221, 2374, 1188, 1402,  376, 1219,  642, 2314,  334, 2119, 1779,\n",
      "          247,   90,  329, 1933, 2207, 2253],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         2202, 1830,  849,  413, 1151,  464,  694, 1481, 2237,  335, 1779,  247,\n",
      "         1586, 2059, 2059, 2059, 1764, 2342],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1444,  988,  452, 1889],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1658,  575, 2248, 1953, 1495,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          614, 1616, 2193, 1670,  942, 2118,  280, 1656,  422,  693, 2019,  698,\n",
      "         2385, 1085, 2316, 1409,   83,  299],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   82,  239,\n",
      "         2020,  988,  115, 1797, 1089, 1386],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1745,  627,  425,   70,\n",
      "         1362,  896,  333, 1084, 1292, 1862]], dtype=torch.int32) Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1], dtype=torch.int32)\n",
      "i: 2\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0, 1573, 1103,  154,  464, 1820,\n",
      "         1525,  417,  464,  660,  956,  167,  154,  464,  956, 1267,    3, 1429,\n",
      "         1573, 1951, 1276,  417,  464,  480],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1227,  684, 1644,\n",
      "         2020,  252, 2205, 1760, 2200, 2020],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  684, 1124],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1402,  205,  479,\n",
      "          681, 1719,  479, 1351, 1719,  479,  963, 1719,  479,  718, 1719,  479,\n",
      "         2014,  956, 2364, 1393, 1402, 1140],\n",
      "        [   0,    0,    0,    0,    0,    0, 1951,   11,  184,  247,  417,  464,\n",
      "          956,  160,  486,  956,  160,  486,  956,  160,  486,  956,  160,  486,\n",
      "          782, 1267, 2119,  931, 2384, 1682],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  497,  252, 1682, 1050, 1630,\n",
      "           23, 1831,  514,   23, 1682,  330],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  654, 1914,  464,  217, 1114, 1393, 1267,\n",
      "         1761, 1148,  464, 1393,   11, 1684],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2063, 2008, 1391,  464, 1761, 2124, 1499,\n",
      "          464, 1188,   41, 1393,   11,  988],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          855, 1722, 1681, 1303, 1449, 1014],\n",
      "        [   0,    0,    0,    0,    0,    0, 1951,   11,  184,  247,  417,  464,\n",
      "          956,  160,  486,  956,  160,  486,  956,  160,  486,  956,  160,  486,\n",
      "          782, 1267, 2119,  931, 2384, 1682],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  654,\n",
      "         1350,   23, 2355, 1707, 1779,  965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1722, 2214,  277, 1151, 1829, 2115,  869, 2040, 1501, 1687, 2090,\n",
      "         1008, 1719, 1831, 1548, 2291, 2172],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2065,  548, 2369, 1094,  507, 1038, 1852,\n",
      "         2019,  660, 1719, 1719,  433, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  853, 2225,  352, 1732,  507, 1563,\n",
      "          223, 1852,  560,  378,  272, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  884, 2075,\n",
      "          740,  638, 2290,  504,   23,  329,  534, 1188,  574,  672,  514, 1078,\n",
      "         2230, 2384, 1257,  340,  988,  786],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1221,  532, 1050, 1031, 1702, 1181,\n",
      "         1905, 2172, 2250,  272, 1393, 1684],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1402,  474,\n",
      "          672,  486, 1908, 1292,  904,  988, 1267,  996,  474, 1163, 1163,  811,\n",
      "         1596, 1349, 2273, 2090, 2222, 1894],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1564,   96, 1735,  313, 1964],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  227, 1281, 1281,  546,   29,   69,  317, 1335, 1595,\n",
      "          190,  999, 1175, 1048,  889, 1691],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1700,  464, 2264,  931, 2366,  399],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1843,  383, 1196, 1615,  921,  542],\n",
      "        [1845,  110, 1223, 1860,  322, 2357, 1573,  460,  565,  948,  637,  789,\n",
      "          803, 1737,  606, 1891,  987, 1019,  779,  427, 1790, 1954,  835,  178,\n",
      "         1292, 1588, 1075,  779, 1280, 1296],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  563,  134,  544, 2298, 1247, 1053,  435,  643, 2049,  415,\n",
      "          215, 1498,  215,  433, 2127, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  762, 1249, 1973, 1378,  440,  417, 1042, 2050,  680,\n",
      "         1378,  116,  272, 1145, 1507, 1475],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1860, 1505, 2016, 1106, 1588,\n",
      "         1075, 1089, 1302,  974, 2263, 2329],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 2225,  327, 1745,  980, 1004, 1383,\n",
      "         1852,  216,  660, 1568,  272, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1687,  972, 1433, 1261, 1267,  584,  247, 1168,  324, 1784,\n",
      "         1168, 2038,  703, 1242,  272, 1255],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          696, 1200, 1202, 1212,  365,  246, 1200, 2402,  328,  549,  971, 1191,\n",
      "          744, 1628, 1789, 1823, 2269, 1754],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1662, 1350,  245,  597, 2351,  642,  384, 1883, 1098,  680, 1612,\n",
      "          941,  208,  995, 1847,  427,  642]], dtype=torch.int32) Labels: tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "i: 3\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,  272, 1110, 2369,\n",
      "         2203, 1835,  931, 1378,  440,  417, 1042, 1562,  931,  345, 1067, 1373,\n",
      "           37,  264,  433, 2391, 1628, 1968],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           81, 1188,  767, 1831,  514,   23, 1933, 1078,  626, 1221, 1776, 1292,\n",
      "          539,   23, 1933,  434, 1498,  347],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  956,\n",
      "         2350, 1030, 1272, 2339, 2097,  956],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1658, 2148, 1250, 1170,  176, 1539, 1523,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1166, 2025, 2140,  672, 1719, 1110, 1774, 2063,\n",
      "         1743,  131,  272, 2363, 1112, 2156],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,   87,  791, 1061,  140],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  417, 1042,  672, 1531, 1693,  335, 1027,\n",
      "         1402, 1299,  351, 2390,  654, 1928],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  753, 1543, 1496],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1835,  167,\n",
      "          154,  464,  478,  154, 1188,   41,  713,  219,  464,   23,  642, 1265,\n",
      "         1146, 2156,  800, 2257, 1924,  138],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1745,  980, 1004,\n",
      "         1383, 1852,  660, 1568,  272, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  467,  996, 1682,   41,  221, 1035, 1682, 1777, 1050,\n",
      "         1887,  897,   90, 1682, 1698, 2230],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  177],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1695,  558,   61,  358,  672,  680,\n",
      "          111,  472, 1257, 2100, 1683, 1001],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1000, 1052,  931, 1825, 1378,  680, 2155,\n",
      "         1084, 2184,  680, 1389, 1110, 1538],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1144, 1110,  694, 2361, 1249, 1280, 2391, 1027, 1719,\n",
      "         1402,  694, 2039, 2054, 1628,   62],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1206, 1252, 1573, 1562, 2158, 1936, 1608, 2004, 2391,\n",
      "         1027, 1772,  484,  433, 1628, 1775],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  546, 1089, 2209, 1234,  933, 1970, 1523,  933,  446, 1290, 2252,\n",
      "         1348, 2217,  214, 1749, 1546,  979],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 2341, 1547,  779, 1110,  893, 2391, 1027, 1772, 1334, 1693,\n",
      "         1079,  209,  433, 2391, 1628, 1838],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 2309,  700, 1089, 2231, 1665,\n",
      "         1960, 1089, 1336, 2096,  895, 1889],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,  154,  464,  956,\n",
      "         1951,  417,  464,  660,  956,  180,  154,  464,  956, 1267,    3, 1429,\n",
      "         1573, 1951, 1276,  417,  464,  480],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,   19, 1436, 1267, 1938, 1563,\n",
      "         1719,  924, 2364, 1393, 1005,  325],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 2033, 1745,    8,  134,  471,\n",
      "          660,  847, 1852,  203, 1393, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  288,  988],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  598, 1350, 2369,  507, 1563,\n",
      "          223, 1755, 1568,  378, 2127, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          191, 1402,  745, 2000,  514, 2210, 1354, 1132, 1849,  826, 2000,  514,\n",
      "         2210,  994,  102,  396, 2291, 2172],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1221, 1620,  249, 1287,  395,  247,\n",
      "         1168, 2038, 2291, 2172,  272, 1255],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1658, 2101, 1004, 1019, 1347,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1687, 2210,  464,   17,  247, 1766, 1894, 2230,   17,\n",
      "         1027,  312,  464, 1267, 2119,   31],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,   94,  756,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639]], dtype=torch.int32) Labels: tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "i: 4\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  412, 1874,  207, 2020,   35, 2300, 1902, 1149,  606,\n",
      "         2011, 1874, 1292, 2020,  931, 1150],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1835,  816,  905,   23, 1796, 1531,\n",
      "          564, 1810, 1188,  564, 1801, 1807],\n",
      "        [   0,    0,    0,    0,    0, 1835, 2063, 1562, 1973,  256, 2391,  374,\n",
      "         1367,  286, 1860, 1676, 1296, 1562, 1387,  272, 1628,  906,  443, 1247,\n",
      "         2078,  931,  121,  689, 1161, 1744],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  291, 2225, 2146, 2133,\n",
      "          554,  938, 2057, 2020, 2290,  172],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1973, 1824, 1867, 1920, 1660, 1719,\n",
      "         1079, 1660,  272, 1326, 1257, 2154],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         2015,  464, 2096, 1831,  417, 1042],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 2002, 2376, 2294,  263, 1934,\n",
      "         1727, 1828,  218, 2218, 1078,  537],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1821, 1330, 1901, 1666, 2350],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  236, 2378],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2244,  412, 2342,  121,  579,   35, 1003,\n",
      "          500, 2324, 1292, 2020, 2135, 2260],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  514,\n",
      "           23,  505,  504,  464, 1506,  443,  904,   11,   23,  505, 1089,  706,\n",
      "           11, 2042,  938,  464, 1506,  676],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  437, 1437, 2032],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1761, 1402, 1663, 1188,   41,  956, 2141,  956, 1951,\n",
      "          514, 2230, 1257, 1467,  988,  786],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  277, 2350,  273, 1027,  464, 2015, 1188,  242,\n",
      "         1831,  855, 1801, 1863,  252, 1758],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  933, 2392,  424, 1355,\n",
      "          546,  914, 1564, 2338, 1089, 2071],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1144, 1110, 1350, 1804, 1110, 1280, 2391, 1027, 1334,  544,\n",
      "         2050,  247,  179,  433, 1507,  624],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1229,   13,  544,\n",
      "         1040, 2010, 1468,  781,  398, 1464, 2084,  499, 1418,  266,   93,  215,\n",
      "         1321,   93, 1125, 1393,   14, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  627,  425, 1089, 1913, 1711, 1791, 1016,  476,\n",
      "           33,  333, 1365,  907, 1214, 1779],\n",
      "        [   0,    0,    0,  760, 2066,  225, 1795, 2368, 1368, 1213, 1046,  672,\n",
      "         1792, 2063,   51,  731, 1798, 2368, 2388,  555, 1860,  641,  294,  858,\n",
      "         1711,   30,  323,  931, 1678, 2112],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1607,  169],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  556,  497, 1210, 1682, 1050, 2141, 1431, 1498,\n",
      "         1456, 2172, 1050, 1431, 1498,  272],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         2389,  986, 1042,  247,  552,   74],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1835, 1973,   41, 2210, 1178, 1027, 1205,\n",
      "          224, 1240,  538, 1214, 1801, 2230],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 2302,  186,  672,  680,\n",
      "          111,  380,  272, 2022, 1214, 1001],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1564,\n",
      "         1840, 1226,  349, 1979, 1361, 1653],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1682,  769, 1188,  382, 1188,  903,\n",
      "         1188,  903, 1188,  903, 2152, 1242],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1703,  464, 1761,  617, 2210, 2383,\n",
      "         2210, 2355, 1532, 1402, 2273, 2136],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1658, 2159,  348, 2327, 1617, 1027,  736,  988,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1835,   23,\n",
      "          990, 2210, 2069,   23,  990, 2172]], dtype=torch.int32) Labels: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "i: 5\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1480, 1179,  956],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  863, 2378, 1350,  675, 1027, 1772, 1334, 1860, 1247, 2378,  276,\n",
      "           28, 1507, 1900,  272, 1145, 1393],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2096,  750,  306,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1350,   51, 1634, 1824,  980,  752,  366,    8, 1004, 1112, 1634,  366,\n",
      "         1071,  680,  379,  433, 1628, 1672],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          696, 1200, 1202, 1212,  365,  246, 1200, 2402,  328,  549,  971, 1191,\n",
      "          744, 1628, 1789, 1823, 2269, 1754],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   88, 1552,\n",
      "          587, 2290,  428,  825,  506, 1599],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          839,  431,   29, 2229,  616,  660],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0, 1835,   23, 2355,  406,\n",
      "          864, 1685,  992,  672,  890, 2172, 1498, 1848,   37,  965, 1027, 1932,\n",
      "          925,  454, 2156, 1310, 1393,  272],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  486,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,  491,  687, 2392,\n",
      "         1021, 2306, 1199, 1553, 2020, 2046],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  462,  315, 1155, 1683,\n",
      "          571,  214,  511, 1931,  166,  855],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  920, 1219,  978, 1605, 1499,  282, 2258,  143,  637,  691,\n",
      "         1753,  244, 1172, 1267, 2119,  143],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1144, 1110, 2037,  787,  417, 1042,  464,  931, 1110,  493, 1792, 1562,\n",
      "          680, 1378,  113,  433, 1628, 1083],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1402,   94,  719,\n",
      "          514,   23,   41,  486, 1172, 1908, 2008,   41, 1685,  864,  486, 1172,\n",
      "         1908, 2008, 1628,  163, 1498,  932],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  817, 2110,  810, 1197, 1082],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2015,  154, 1188, 1787, 1456, 1221,   31, 1219,\n",
      "         1031, 2117, 1188, 1027, 1188, 2242],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  684, 1840, 1817, 2274, 1068],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2375,   77, 1761, 1188, 2210,   41, 2230,  300, 1831,\n",
      "         1027, 1205,  224, 1628, 1251,  538],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1824,  752, 1839, 1860,\n",
      "         2359,  930, 2074, 1628, 1683, 1675],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  750,  306,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0, 1835,   23,  265, 1456,  672, 2069, 2172, 1498, 1848,\n",
      "         2006,  965,   23, 1511, 1738,  890,   23, 1050, 2273, 1670, 1027, 1932,\n",
      "          925,  272, 1024, 1050,  636,  145],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1144, 1110,\n",
      "          544, 1573, 1562, 1547, 1635, 2391,  788, 1829,  374, 1367,  286,  429,\n",
      "         1378,  692, 1267,  272, 1507, 1581],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1658, 1245,  830,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,\n",
      "         2270, 1699,  365, 1707,  444,  501],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1106, 1708,  103, 1648,  103, 1687,\n",
      "           81,  154,  509,  833, 2268, 1487],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1402,   94,  719,\n",
      "          514,   23,   41,  486, 1172, 1908, 2008,   41, 1685,  864,  486, 1172,\n",
      "         1908, 2008, 1628,  163, 1498,  932],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1680,  558,  186,  611,\n",
      "          672,  911,  111, 1257, 1943,  784],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,  933, 1175,  107,\n",
      "         1394,  546, 2352, 1580,  793,  684],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,   76, 1575]], dtype=torch.int32) Labels: tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "i: 6\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1835, 1378, 1296, 2088, 2391,  788, 1829,  101, 1216,\n",
      "          680,   43,  272, 1628, 1661, 1322],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1835, 1188,  992,  568,\n",
      "         1027, 1188,  252,  454, 2156,  368],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,   69, 1002,   87,\n",
      "         1044, 2104, 1262, 1301, 1314, 1881],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1261,\n",
      "         1089, 1403,   31,  956,  478,  698,   31, 1667, 1092, 1188,   41, 1276,\n",
      "          904,  618, 1300, 1498, 1261, 2063],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1965,  882, 2238,  634, 1312,  944, 1723,\n",
      "         2315, 1193, 1017, 1457,  642, 2172],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1658,   67, 2233,  766, 1313, 1539,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  563, 1468,  204,  796,  544, 1634,  366, 1334,  272,\n",
      "         1628, 1683,  612, 1803, 2365,  730],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  740, 2218, 1800, 1370, 1360,  504,  464, 2218, 1241,\n",
      "         1105,  514, 1914,  805, 1027, 1710],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1687,  154, 1292, 1627, 2342, 1292,  154, 1416, 1292, 2237,\n",
      "          504, 1031, 2117,  873, 1992, 2103],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1965, 1390, 1491,   44, 2273, 1390,   59,\n",
      "         1293, 1990, 1017, 1457,  642, 2172],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  654, 1350, 1835, 2125,  578, 1110, 1280, 1296, 2025,  588,  459,\n",
      "         2316,  680, 2061,  433, 1628,  831],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1687, 2210,  464,   17,  247, 1766, 1894, 2230,   17,\n",
      "         1027,  312,  464, 1267, 2119,   31],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1835,    1, 1973, 2281, 2391, 1027,  484,  680, 1378,\n",
      "          819, 2371, 1027,  680, 1628, 1640],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,   23,  191,  769, 1066,  114,  155, 1200,  455, 1060,   20, 1292,\n",
      "          141,  961, 2289,  790,   71,  852],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2153, 1188, 1506,  904,  988, 1687, 1597,  672,  938,\n",
      "          417, 1117,  636, 1014,  505,  767],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,   31, 1285, 2020, 1745, 1188,   23,\n",
      "         1892,   41, 1219,  514,  272, 2011],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  214,\n",
      "          862, 2057,  623, 2377,  623,  684],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  384, 1472,  805, 2191, 1500,\n",
      "          606,  777,  384,  868, 1369, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0, 1089, 2209, 1234, 1089, 2270, 2209,\n",
      "         1089, 1320,   10, 2319,  726, 1196, 1664, 1954, 1175, 1550,  580, 1089,\n",
      "         2145, 2331, 1580, 1089, 2082, 1889],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  881, 2020, 1353, 1450,  684],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  742, 1214,  127],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1658, 2133, 1974,  738, 1442,  247,  527,  945,\n",
      "         2256,  707, 1267,  467,  904, 1728],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  482, 1402,\n",
      "         1738, 1498,  233, 1261, 1499, 2008,   85, 2126, 1610,  655,  956, 1860,\n",
      "          956, 1951, 1134,   40,  958, 1242],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2153, 1188, 1506,  904,  988, 1687, 1597,  672,  938,\n",
      "          417, 1117,  636, 1014,  505,  767],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1206, 1252, 1573, 1562, 2158, 1936,  680, 1378,  653,\n",
      "         1973, 1027, 1719,  272, 1507, 1029],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 2289,  988,  214, 1666],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 2386,  507,  758,  966,  694, 1427, 1918, 1586, 2059, 2059,\n",
      "         2059, 1767,  281, 2342, 1131, 1546]], dtype=torch.int32) Labels: tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1], dtype=torch.int32)\n",
      "i: 7\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0, 1687,  154,  464, 1921,\n",
      "          956,  417, 1042, 1092,  417, 1042, 1860,  956,  859, 2210,  123, 2326,\n",
      "          904,  678,  123, 1027,  154, 2207],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1172,  249,\n",
      "         1287, 1233, 1848,  249, 1118,  262, 2380, 1422,  815, 1823,  828, 2245,\n",
      "         2303, 1498,   73,  104, 1078,  957],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,   22, 1154, 1655,  512, 1603,  960, 1142,\n",
      "          305, 1551, 1717, 2208, 1147,  170],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,  417, 2090,  649,\n",
      "         1219, 1515, 1515, 1483, 1019, 1832, 1247,   11, 1247, 1168, 1693, 1247,\n",
      "          931, 2008, 1980,  931,  121,  645],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1951,\n",
      "          514,   41, 1172,  486, 1627, 1908, 1734,   41, 1172, 1267,  486, 1627,\n",
      "          857, 1734, 1292,  163, 1498,  932],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1239,\n",
      "           36,  838, 1402, 1343,  205, 1649,  672,  718,  672, 1998, 1033, 2226,\n",
      "         1594,  272, 1879,  680,  479,  630],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1027, 1933,  918,   41, 1356, 1188, 1310, 1724, 1172,   23,\n",
      "          769, 2273,  668,   60,  514, 2239],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2282,\n",
      "         2121,  840,  336,  274,  291, 1889],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1280, 1016,  333,  270,  907, 2190,  627, 2131,  464,  956,  304,\n",
      "          931,  578, 1261, 1913, 2001, 2377],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  272,  164,  892,  223, 2214, 1773, 1563, 1412, 1719,\n",
      "         1498, 1719,  560, 2019,  378,  372],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1361,\n",
      "         2034, 1315,  631, 1985, 2020, 1564],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  240,  112,  632, 1742, 1901, 1059, 2158, 1059, 1106, 1588, 1075,\n",
      "         1267, 1106,  230, 1206, 1145,  318],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  214, 2160,  114, 2323, 1294,\n",
      "         2163, 1954,  197, 2379,   32, 1034],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  914, 1960, 1659,  664],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 2390, 1402, 2293, 1779,   41,  486,  672, 2293,   41,  486, 1779,\n",
      "           41,   41,  672,  897, 2227,  421],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          503, 2369,  853, 2225,  352, 1745, 2149,  536, 1594, 1852, 1751, 1719,\n",
      "         1498, 1719, 1084, 2127, 1344, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          999, 1889,  865,   60, 1513, 2098],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1126,  794, 1417, 1021],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  127,  419, 1359, 1451,  899,  981,\n",
      "          697, 1395,  156,  331,  841, 1628],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           81, 1188,  767, 1831,  514,   23, 1933, 1078,  626, 1221, 1776, 1292,\n",
      "          539,   23, 1933,  434, 1498,  296],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1265, 1817],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  584, 1588,\n",
      "         1075, 1084, 1109, 2363, 1047, 1540],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2350,\n",
      "         1991, 1558, 1282,  517, 1390, 1101],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  291, 2018, 1545],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1402, 2310, 2308,  205, 1850,  311, 1033,  514,  901,\n",
      "          272, 1477,  680,  479, 1291,  521],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2198, 2012, 1592, 1016, 1827,  223,  354,  873,  867,\n",
      "         2199,  792, 1661, 1016,  223, 1009],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2372],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          277,   64, 2320, 1188, 1682, 2350,  273,  417, 1042,  464,  855, 2020,\n",
      "         1801, 1863,  644, 1567, 1339, 1510],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1835, 2063, 1973, 1378, 1541,\n",
      "         2158, 1079,  490,  433, 1628, 1811]], dtype=torch.int32) Labels: tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 1], dtype=torch.int32)\n",
      "i: 8\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0, 1625, 1267, 1471,\n",
      "         1656, 2141, 1172, 1267, 1471,  667, 2355, 1292,  406, 2384, 1164, 1188,\n",
      "         1486,  260,  904,  988, 1498,  466],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1495, 1188, 1682,   41,  486,  514,  371,  918,  956,  168, 2396,  368,\n",
      "         1511, 1440, 1402, 1498,  590, 2297],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          491, 1361, 1173,  337, 2350,  426],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2063, 2008, 1391,  464, 1761, 2124, 1499,\n",
      "          464, 1188,   41, 1393,   11,  988],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,   10, 2029, 1979, 1289, 2048],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  721,  936, 2093,  464,\n",
      "          956,  931,   15, 1089,  720,  557],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1235, 1670, 1220,  369, 1424, 1376,    6,  259,  746, 1915,  723,\n",
      "         1458, 1119, 1670,  704, 1187, 1398],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1144, 1110, 1541,  779, 1110, 2391, 1027, 1772, 1334,\n",
      "          680, 1378, 1906,  433, 1628,  235],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          769, 2218, 1933,  918,   41,  660, 1292, 1267,  486, 1656,  956,   23,\n",
      "          769, 2273,  668,   60,  514, 2239],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  471, 1704,  499, 2279,\n",
      "          896,  848, 1531, 1947, 1027,  430],\n",
      "        [2079, 2005, 2114,  654, 1177, 1045, 1896,  931, 2280, 1212,  417, 1042,\n",
      "         1203,  230, 1270, 2053, 1284,  461, 1070, 1070, 1192, 2296, 1292,  757,\n",
      "          463, 1270,  623, 1284,  988, 2276],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  640, 1157,   41, 1214,  121, 1146,  531,\n",
      "         1483, 1473,  368, 2156,  800,  991],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  154,\n",
      "          464,  956,  247,  417, 1042,  956,  395, 1031, 1089,  833, 2172, 1498,\n",
      "          734,  154,  367, 1267, 1031, 2117],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1461, 2172,    2, 1188, 1077,\n",
      "          866,   41,  890,  272, 1024, 2354],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1564, 1027,  660, 1677, 1624],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0, 1638, 2356, 1923, 2026,\n",
      "          223,  938,  684, 1265, 1008,  713,  219, 2090, 1213, 1326,  672, 2020,\n",
      "          533, 1499, 2025,  217, 1683,  629],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          979,  547, 1535,  551, 1256, 1542, 1735,  970, 1027, 1256, 1089,  453,\n",
      "         1628, 1435,  912, 1089, 1160, 1317],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1835, 2063, 1973, 1378, 1541,\n",
      "         2158, 1079,  490,  433, 1628, 1811],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1144, 1110, 1860,  494, 2158, 1835,  779, 1012, 2391,  247,\n",
      "          429, 1129,  735, 1084, 1507,  658],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1221, 1525,   41,  221,   41,  956,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2153, 1188, 1506,  904,  988, 1687, 1597,  672,  938,\n",
      "          417, 1117,  636, 1014,  505,  767],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  532, 1188, 2355,  885,  610, 2237, 1172, 2141, 1172, 1627,  890,\n",
      "          857, 1292,  247,  904, 1573, 2059],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  417,  464, 2234, 1706,  901,  464, 1885,  901, 1747,  532,  901,\n",
      "         2391,  901, 2040, 1831, 1628, 2360],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  563, 2225,  314,  531,  559, 1006,  607, 2288, 1586, 1393,\n",
      "           47, 1439, 1428, 1337, 1047, 1540],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  503,  507, 1563,  223, 2214,  277, 1567, 1852, 1751,\n",
      "         1171, 1568,  433, 1084, 2127, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  227, 1813,\n",
      "         1831,  599,  650, 2221, 1813,  988],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  873, 1512,  532, 1861, 1823, 1682, 1726, 1292, 1402,  771,\n",
      "         1499, 2156, 2025,  570, 2396, 1310],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          647,   29,  700, 1175,  161, 2011],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2374, 1188, 1682, 1164,  334,  983, 1292,   23,\n",
      "          329, 1933, 1031, 2117, 1393, 1757]], dtype=torch.int32) Labels: tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "i: 9\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  272,  683, 1089, 1989,  642, 1527,\n",
      "         1576, 1912, 1206,   27, 1114,   52],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  154, 1188, 1787,   41, 1188,\n",
      "         1729,    5,   23,  272, 1027, 2230],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  464, 1506,  672,   80,  417, 1042,  814,\n",
      "          672,  904,  988, 2342,  272,  676],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  695, 1941, 1430, 1993,  901,  362, 1453,  561,   58,  363,\n",
      "          199,   51, 1833, 2147,  723, 1398],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1564,  901, 1926,  129, 2020, 1333],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  933,  988],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 2369, 1350,  507, 1563, 1852, 2232,\n",
      "         1058,  435,  215,  272, 2127, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  563, 1468,  204,  796,  544, 1634,  366, 1334,  272,\n",
      "         1628, 1683,  612, 1803, 2365,  730],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  272,  164,  892,  223, 2214, 1773, 1882, 1412, 1719,\n",
      "         1498, 1719,  560, 2019,  378, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1802, 1128,  817, 1137,  677, 1677,\n",
      "          175,  125, 1756,  817, 2363, 1677],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1835, 2210,   41,  924,  219,  702, 1761,  335,  247, 1008, 2230,\n",
      "         1310, 1628, 1121,  538, 1027, 1268],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          214, 2346, 2104, 2166, 2020,  554],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  769, 1172, 2383,   41,   23,   31, 1267,  486,\n",
      "           31,  740,  897, 1402, 1498, 2250],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1703,\n",
      "         1848, 2385,  483, 2020,  100, 1696],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          272, 2060,  775,  987,  787,  417,  464, 1835, 1945, 1022, 1635, 2391,\n",
      "          680, 1378, 1991,  272, 1628, 1311],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1618,\n",
      "         1247, 2378,   35, 2182,  148, 2324],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1835, 1188,  992,  568,\n",
      "         1027, 1188,  252,  454, 2156,  368],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 2225,  352, 1732,\n",
      "          507, 1563,  223, 1852,  378, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  795,  754, 1032,  943, 1951,  973,  473, 1100, 1673, 2381,\n",
      "         1078, 1586, 1100,  769, 1241,  272],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  492, 1853,  876,  739,  400, 1626,  109, 1411,  591, 2254,\n",
      "         2134, 1089,  751, 1869,  778, 1853],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 2341, 1547,  779, 1110,  893, 2391, 1027, 1772, 1334, 1693,\n",
      "         1079,  209,  433, 2391, 1628, 1838],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  830, 1544, 1019,  613,  120,  173, 1713,  532,  613, 1221,  532,\n",
      "         1870, 1463, 2367, 1951,  970, 2111],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  654,  139,  648, 2008,   85, 1627, 2126, 1610,  978,\n",
      "          655,  395,  394,  956, 1505,  958],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1779, 2034,\n",
      "         1530,  234, 1669, 1769,  349,  705],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1230,  142,  479, 1620, 2052,  205, 2014,   17, 1351, 1719,  681,\n",
      "          215,  909, 1850,  272, 1628, 1927],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1327,\n",
      "         2340,  532, 1834,  952,  479,  681, 1792,  479,  718, 1719, 2226, 1594,\n",
      "         1393, 1292,  680,  479,  661, 1253],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          272, 1110, 1573, 1562, 1835, 2063, 1973, 2391,  788, 1829,  544, 1573,\n",
      "         1562,  680, 1378, 1275,  272,  728],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  497,  252, 1682, 1050, 1630,\n",
      "           23, 1831,  514,   23, 1682,  330]], dtype=torch.int32) Labels: tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0], dtype=torch.int32)\n",
      "i: 10\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1188,   23, 1655, 1086,   41, 2099, 2124,\n",
      "          965, 1831,  417, 1042, 1188,  217],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  198,  787,  752,  181, 1731,  278,  227,\n",
      "         2047, 1909,  314, 1425, 1628, 2030],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  272,  164,  892,  223, 2214, 2045, 1559,  164, 1412, 1719,\n",
      "         1498, 1719, 1762, 2019,  378, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  225,   69,  192, 1176,  158],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1232, 1215,\n",
      "         1587, 1089,  397, 1089, 1712, 1383],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  321,  986,  464,  910, 2356, 2391, 1676, 2025,  417, 1042,\n",
      "          910,  433, 1110, 1378, 1628, 2181],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1130, 2316, 1309,\n",
      "           55, 1221,  532, 2210, 2303, 1231],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  832,  690],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  922,  812,  684,  401, 2008, 1761,  474, 2210,  684,\n",
      "          252, 1687, 1402, 2210, 2273, 2136],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2153, 1188, 1506,  904,  988, 1687, 1597,  672,  938,\n",
      "          417, 1117,  636, 1014,  505,  767],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 2111, 1041,  504,  898, 1623,  120,\n",
      "         1963, 2144,  254, 1288, 2044, 1854],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  198, 1930, 1316,  284, 2344,   46,  284,\n",
      "          196, 2266,  212, 1153,  135,  438],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  497,  252, 1682, 1050, 1630,\n",
      "           23, 1831,  514,   23, 1682,  330],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0, 1835,   23, 2355,  406,\n",
      "          864, 1685,  992,  672,  890, 2172, 1498, 1848,   37,  965, 1027, 1932,\n",
      "          925,  454, 2156, 1310, 1393,  272],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  353, 1434,  487,  888, 1715, 1660, 1399,\n",
      "         2391,  499, 1719,  272, 1628,  475],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  516,  282,  543, 2063, 1687,   23,   94,   41, 1958,\n",
      "         1507,  703,  625, 1292,  959, 2243],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  343, 2342,  887,  604],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  492, 1853,  876,  739,  400, 1626,  109, 1411,  591, 2254,\n",
      "         2134, 1089,  751, 1869,  778, 1853],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1911, 1506,  277,  916, 1687,  464,\n",
      "          642, 2207,  722,  938,  417, 1042],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2202],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1567,  417,  420,   23, 1682, 1062, 1027,\n",
      "         1871,  884,  902,  223,  582, 2297],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1429, 1089, 1226, 1485,\n",
      "         1292,  621,  227,  985,   87, 1081],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1144, 1110, 1835,  931, 1804,  675, 2391, 1027,  484, 1860, 1573,\n",
      "         1562,  680, 1584,  272, 1628, 1432],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1081, 2178],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  137, 1562, 1835, 1804, 2391,  788, 1701,\n",
      "         1258,  680, 1332,  272, 1628, 1379],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1965, 1390, 1491,   44, 2273, 1390,   59,\n",
      "         1293, 1990, 1017, 1457,  642, 2172],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  850,  675, 2391, 1027, 1772,  672, 1719, 1031, 1975, 2155,  433,\n",
      "          387, 2391, 1507, 1815, 1031,  402],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639]], dtype=torch.int32) Labels: tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0], dtype=torch.int32)\n",
      "i: 11\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1835,\n",
      "           23, 1292, 2237,  890, 1027, 1188],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 2057, 2161, 1919,  554],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 2057,  410,  988, 1779],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  381,  912,  515, 1840, 2070,\n",
      "          970, 2350,  698, 1174,  539, 2137],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2206, 1779,  604],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          894,  126,  441, 2401,  126, 2220, 1514,  126,  360,  984, 2362,  711,\n",
      "         1184, 2220, 1633, 1628, 1683, 1754],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  606,  550,  427,  724, 2361, 1249, 2391,\n",
      "          295,  544,  686,  680, 1628, 1190],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1745,    8, 1004,\n",
      "          346,  413, 1750,  916, 1009, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  931, 2078,  689],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1835, 2210,   41,  924,  219,  702, 1761,  335,  247, 1008, 2230,\n",
      "         1310, 1628, 1121,  538, 1027, 1268],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  287,   55, 2043, 1740, 1169, 2318,  926, 1719, 2226, 1033, 1594,\n",
      "          680,  479, 2132, 1978, 1507,   95],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1835,  354, 1627, 2384,  202, 1221,  593,  513,  654, 2330,  316,\n",
      "         1856,  824, 1479, 1372,  202, 1207],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,  191,  243,  745, 2000,  396,\n",
      "           89, 2085, 1823,  146, 1465,  369, 1971, 1611, 1823, 1897,  117, 1479,\n",
      "          185,  201, 1823, 1123,  699, 1454],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1786, 1806,   25, 1931, 1910,  497,\n",
      "          496, 2057, 1574, 1699, 1027,  988],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1835, 1973,   41, 2210, 1178, 1027, 1205,\n",
      "          224, 1240,  538, 1214, 1801, 2230],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  227,  947],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1618, 2378,   35, 1478,  577],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  833,   94, 1267,  417, 1042,  660, 1172, 2210, 1188, 1682,\n",
      "           41, 2172,  272, 2127,  307, 1528],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  654,\n",
      "         1350,   23, 2355, 1707, 1779,  965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1221,  341,\n",
      "           41,  978,  655,  341, 1747, 2026,  956, 1627,  514,   90,  265,  767,\n",
      "          672,  956, 1027, 1050, 2122, 2297],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1188, 1944, 2334, 2099, 2124,\n",
      "          965, 1831,  417, 1042, 1188,  217],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 2225,  352, 2369,   92,  210,\n",
      "          346,  917,  247, 1852,  272, 1257],\n",
      "        [1929,  478, 1935, 2237, 1573,  803, 1737, 1298, 1890, 1206, 2175, 1942,\n",
      "            5,  282, 1573, 2036, 2342, 1292,  272, 1875, 1280, 1296, 1573,  149,\n",
      "         1864, 1848,  162, 1378,  680,  605],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1021, 1496, 1903],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  654,\n",
      "         1350,   23, 2355, 1707, 1779,  965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1835, 2361, 1249, 1973, 1378,  440,  417, 1042, 2050,  680,\n",
      "         1378,  116,  272, 1145, 1507, 1475],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1658, 2159,  348, 2327, 1617, 1027,  736,  988,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1252, 1110, 1562, 1835,  779, 1973, 1635,\n",
      "         2394, 1334, 1209,  272, 1996, 2267],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1361,  291, 2387,  234, 1782, 1535, 1564,\n",
      "         2080, 1363,  600, 1954, 1564,   60],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  214, 1857, 1355, 1972, 2350]], dtype=torch.int32) Labels: tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0], dtype=torch.int32)\n",
      "i: 12\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2244,\n",
      "         1096, 2236,  464,  956,  931, 1479, 2236,  683, 2236,    7, 1078, 2107,\n",
      "         1705,  407,  364, 2236, 1733, 2275],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1358, 2213, 1043,\n",
      "          785, 1785, 1043,  457, 1785, 2189],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1835,  349, 1406, 1104, 1090, 1671],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,   94,  750,  306,  226,   41,  221,   41,\n",
      "          221,   41,  956, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  497,  252, 1682, 1050, 1630,\n",
      "           23, 1831,  514,   23, 1682,  330],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1835,  924,  219,  702,   41, 2210, 1761,  335,  247, 1008, 2230,\n",
      "         1310, 1628, 1121,  538, 1027, 1268],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1402,  474, 2210,\n",
      "          672, 1267,  486, 1908, 1292,  904,  988, 1267,  996,  474, 2072, 2174,\n",
      "         1596, 2210, 1349,  272, 2246, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 2234,  938, 1893, 1894, 1801, 2230, 1511, 1761, 2210,   41,\n",
      "          277, 1567,  938, 2230, 1628, 1999],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1722, 2214,  277, 1151, 1829, 2115,  869, 2040, 1501, 1687, 2090,\n",
      "         1008, 1719, 1831, 1548, 2291, 2172],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  827,  975, 2391,  672,  290,  222, 1641,  837,\n",
      "          261,  268, 1393,  937, 1392,  556],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1703,\n",
      "         2377,  414, 2020, 1089,  250, 1676],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1745,    8, 1004,  346, 1568, 1852,\n",
      "         1338,  413, 1750,  916, 1009, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  101, 1419,  514, 2088, 1078, 2342,  456,\n",
      "         2068, 1107,  597, 1965, 1628,  780],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         2056, 1686, 1511,  498, 1686,  684],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          696, 1200, 1202, 1212,  365,  246, 1200, 2402,  328,  549,  971, 1191,\n",
      "          744, 1628, 1789, 1823, 2269, 1754],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  338, 2091,  227,  373, 1366],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2063,   79,  464,  217, 1761, 1393, 1267, 1761,\n",
      "         1499,  464, 1188,  300, 1393,  988],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  492, 1853,  876,  739,  400, 1626,  109, 1411,  591, 2254,\n",
      "         2134, 1089,  751, 1869,  778, 1853],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2337,  962,  278,  871,  787,  752,  781, 1571, 1328,\n",
      "          147, 1518,  308,  525, 1657, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1035, 1682,  606,  671, 1136,  989,\n",
      "         1329, 1600, 1502,  672, 1986,  272],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  768, 2081, 1889],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2210, 1292, 2138, 1031,  900,  292, 1292,  466,\n",
      "          154,  367, 1414,  764,  272,  124],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1655,  950, 1857, 2139, 1511, 1026],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          282, 2361, 1531, 1305, 1047, 1540],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1095,  291,\n",
      "          999, 1323, 1741,   69,   72, 2188],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,  383, 1061, 1304, 1156,\n",
      "         1089, 1745, 1688, 1277, 1212,  908, 2003, 1816,   41, 1621,   41,   97,\n",
      "         2015, 1688, 2323, 1265, 1586, 1246],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1658, 1245, 2321,  443, 2255, 1081,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  563, 1745,  435,  643, 2049,  844,\n",
      "          215, 1498,  215,  433, 2127, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1206, 1252, 1110,  694, 1936,  779, 1110, 2281, 2391,\n",
      "          544, 2060,  652,  433, 1628,  390],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1143]], dtype=torch.int32) Labels: tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0], dtype=torch.int32)\n",
      "i: 13\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  988, 2344,   75, 1089, 2008, 1994, 1540, 1664,  217, 1819,\n",
      "          670, 1426, 2373, 1220, 1536, 2165],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           81, 1188,  767, 1831,  514,   23, 1933, 1078,  626, 1221, 1776, 1292,\n",
      "          539,   23, 1933,  434, 1498,  347],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 2225,  352, 1732,\n",
      "          507, 1563,  223, 1852,  378, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  450, 1814, 1089, 1075,   12, 1829, 1324, 1097, 1393, 1108,\n",
      "          314,  842, 1405, 1480, 1519, 2008],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  589,   69,  642,  258, 1577, 1099,  291, 1622, 2240,  291,\n",
      "         1619,  940, 1793, 1546,  258,  968],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  727,  417, 1042, 1526,\n",
      "         1946, 1531, 1693, 1561, 1959, 1382],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          272, 1110, 1835,  931, 1804, 1012,  499, 1772,  672, 1719, 1860, 1788,\n",
      "          665,  272, 1393,  540, 1628,  759],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1835,  975, 2391,  672, 1792, 2156,\n",
      "         2040,  459, 1455, 1393,  228, 1112],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          491,  309, 1594,  448, 1745, 1342],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 2293, 1779, 1456,  486,  890, 1221, 1456, 2172,\n",
      "          890, 1498, 1402, 1928,  572,  272],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1221,   81,  464,  247,  417, 1042,  660,  956, 1801, 2396,\n",
      "         1168,  855, 1643, 1402, 1498,  231],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1547,  931,  975, 2391, 2007,  934,  931,\n",
      "          680,  998, 2391, 2371, 1628,  257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1662, 1350,  245,  597, 2351,  642,  384, 1883, 1098,  680, 1612,\n",
      "          941,  208,  995, 1847,  427,  642],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1547,  779, 1110, 2391, 1027, 1772, 1334, 1693,\n",
      "         1079,  385,  433, 2391, 1628, 1640],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  132,  872, 2025, 2025,  648,   50, 1378,  770, 1363,  978,   41,\n",
      "         1378,  872, 2025, 2287, 1498, 1808],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2196,\n",
      "         2276, 1557, 1244,  741, 1779, 1650],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1629,  830, 1719, 1615, 1454,  755],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1682,  769, 1188,  382, 1188,  903,\n",
      "         1188,  903, 1188,  903, 2152, 1242],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  467,  925,   23,  265,  904,  150,\n",
      "         2283, 1899, 1292, 1636,  594,   23],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1402,  514,   23,\n",
      "          265,   41,  454, 2156,  368,  901,  341, 1188,   41,  978,  655,  341,\n",
      "         2355, 1115, 1276, 1027, 2055, 2291],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1698,  332, 2233, 1637,\n",
      "         2290, 1397,  546, 1752,  468, 1262],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  654, 1835, 2063, 1186, 1391,\n",
      "          464, 1393, 1267, 1761, 1499,  464],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  503,\n",
      "         1180, 2017, 1857, 1072,  749, 1274],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1835,   26,   41, 2210, 1761, 1261, 1276, 1628, 1400,\n",
      "          538, 1027,   23, 1302, 2230, 1310],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  471, 1704,\n",
      "          499,  409,  848, 1947, 1027, 1279],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2211,  369, 2158, 1570,  954,  495, 1799, 1679, 1719,\n",
      "         1376,  529, 1719, 1376,  529, 1398],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  249,\n",
      "         1118, 1287, 1441, 1273, 2042,  818,  956, 1848, 1823, 1222,  389, 2303,\n",
      "         1267,  732, 2162, 2250,  104,  957],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,  272, 1023, 1089, 1761,  725,\n",
      "          749, 1453,  193, 1087,  100,  162,  592, 1459,  657,  874, 2285,  229,\n",
      "          948,  283, 1888, 1746, 2011, 1308],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1703,  528,   34,   23,\n",
      "          329, 1933, 1031, 2117, 1393, 1757]], dtype=torch.int32) Labels: tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "i: 14\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1682,  769, 1188,  382, 1188,  903,\n",
      "         1188,  903, 1188,  903, 2152, 1242],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  659,  508,   55, 1188,   41,\n",
      "         2130, 1172,  833, 1140, 1027, 2011],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,  249, 1287, 1233, 1848,\n",
      "         1172,  249, 1118,  262,    9,  815, 1594, 1823,  828, 2245, 2303, 1267,\n",
      "          732, 2162, 1498,  466,  104,  957],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  165,  946,  820,  578,  931,  361,  152, 1631, 1987,  152,\n",
      "         1848, 1735, 2262, 1984, 1540,  310],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  938,  417,  464,   23, 1506, 1188,   41, 2355,  885, 1292,\n",
      "         1586,  672,  897, 2297,  272,  583],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1061, 1555, 1889,  645],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1144, 2332, 2313, 1144, 2332, 1084, 2207,\n",
      "         2207,  982, 1703, 1213, 1047, 1540],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1982, 1266,  144, 2168],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1557, 1522,  975, 2391, 1860, 1966, 2202, 1031,\n",
      "          680,   43,  737, 2391, 1628, 2187],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  344,  977, 1894, 2271, 1025, 1383,\n",
      "         1835,  384, 1031, 2117, 1047, 1540],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          769, 2218, 1933,  918,   41,  660, 1292, 1267,  486, 1656,  956,   23,\n",
      "          769, 2273,  668,   60,  514, 2239],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2235,  243,  978, 1682, 1860,  956, 1188,   41,  969,\n",
      "         1848,  932,  514,  635, 2249, 2297],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  595,  356, 1687,  474,  684, 2210,\n",
      "          252, 1687, 1402, 2210, 2273, 2136],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  214,\n",
      "          884,  486,  377, 1081, 1225, 1345],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1684,\n",
      "          642,  804, 1254,  988,  701,  567],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2225, 2064,\n",
      "         1732,  507, 1563, 1852,  378, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 2021, 1835, 1378,\n",
      "         1334,  680, 1071, 1584,  272, 1194],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,  314, 1410, 2116,\n",
      "          602,   53, 1054, 1940,  169, 2077, 2212,  929,  760, 1916,  576, 1591,\n",
      "         2388, 1848,  734, 1780, 2216,  919],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1557, 1350, 2015, 1188,  265, 1887,  855, 2396, 1310,  993,\n",
      "         1267, 1027,  622,   23,  265, 1393],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1238,\n",
      "         1817,  359, 1558,   15, 1558,   56, 2395,  357, 1511,  285,  720, 2020,\n",
      "          747,  359,  931,  572,   15, 1743],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  194,  678, 2092,  159,  247, 1647,   66, 1221,\n",
      "          972, 1639, 1230, 1937, 1628, 1721],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1089, 1878, 1460],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 2105, 1018, 1917,\n",
      "           65,   65,   65, 1794,  897,  474, 1204, 1162,  628,  451, 1546, 1771,\n",
      "          672,  486, 1738,  672,  486, 2273],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1402,  474,  672,  486, 1908, 1292,  904,  988, 1267,\n",
      "          996,  474,  342,  272, 2246, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1642, 1836, 1397, 1089, 1488],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  214, 2400, 2284, 2020, 1244],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  972,  194, 1452, 2277,  585, 1498,\n",
      "          708,  432,  523, 2228,  275,  562],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  198,  787,  752,  181, 1731,  278,  227,\n",
      "         2047, 1909,  314, 1425, 1628, 2030],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  524, 1465,\n",
      "          182,  188, 1470, 2272, 2353, 1950]], dtype=torch.int32) Labels: tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0], dtype=torch.int32)\n",
      "i: 15\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  154, 1188, 1787,   41, 1188,\n",
      "         1729,    5,   23,  272, 1027, 2230],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1961, 2113,  443,   48,   48],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1687, 2210,  464,   17,  247, 1766, 1894, 2230,   17,\n",
      "         1027,  312,  464, 1267, 2119,   31],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  503,\n",
      "         1089, 2392,  214,  511, 1011, 1291],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  272, 1110,  779, 1825,  883, 2391,\n",
      "          458, 1868,  609,  433, 1628, 1585],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  662,  219,  913, 1141],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1835,  213, 1027, 1772,\n",
      "          484,  680, 1613,  272, 1981, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1228,  914,  772, 1355],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  850,  213,  672, 1715,  469, 1071,\n",
      "         1079,  272,  171,  836, 1257,  375],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1130, 2316, 1309,\n",
      "           55, 1221,  532, 2210, 2303, 1231],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  654, 1835, 2063, 1186, 1391,\n",
      "          464, 1393, 1267, 1761, 1499,  464],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1957, 1271,   63, 1783,\n",
      "           75, 1556, 1175, 1374, 1081, 2251],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  618,  227,   96,  631,  832],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1658, 1166,  512, 1259,   38,  512,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1273],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1745,  892,\n",
      "          536, 1812, 1065, 2019,  472, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 2142,  927, 1805, 2335,\n",
      "          326,  251, 1739, 2027,  702, 1908],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1319, 1589, 1394, 1397,  355,\n",
      "         2350, 2345, 2350, 2399,  128,  979],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2293, 1779,   41,  486,  672,  773,   41,  486,   41,\n",
      "         2164,  672,  897, 2227,  272, 1413],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1094, 1859,  507, 1563,  525, 2265, 1654, 2158, 1171,\n",
      "         1076, 2214,  277, 1567,  433, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  464, 1506,  672,   80,  417, 1042,  814,\n",
      "          672,  904,  988, 2342,  272,  676],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1350,  238,  507, 1563,  223, 1852,\n",
      "         1448, 1171, 1563,  215,  272, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1983,  464,  897,   23, 1506, 1456,  642, 1292,  864,  885,  864,\n",
      "          938,  672,  857, 1020, 1267, 1824],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  497,  252, 1682, 1050, 1630,\n",
      "           23, 1831,  514,   23, 1682,  330],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1720,   60, 1217,\n",
      "         1822,  988,   10, 1852,  761, 2196],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   23,\n",
      "          386,   78, 1091, 1682,  206,  267,  255, 1682, 2273,  774,  267,  282,\n",
      "         1711,  897,   23, 1682,  272,   42],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,  227,  846,  933,\n",
      "         2393,  157, 1089, 1224,  621, 2317],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2104,\n",
      "         1265,  411, 1865, 2350,  200, 2398],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639]], dtype=torch.int32) Labels: tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "i: 16\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  497,  252, 1682, 1050, 1630,\n",
      "           23, 1831,  514,   23, 1682,  330],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  465, 2369, 1745,  253, 1004, 2214,\n",
      "          277, 1852,  660,  215,  272, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1835, 1188,  864,  568,\n",
      "         1027, 1188,  252,  454, 2156,  368],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1393, 1363,  214,  227,  798, 1689],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1965,  882, 2238,  634, 1312,  944, 1723,\n",
      "         2315, 1193, 1017, 1457,  642, 2172],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1047, 1540,  344,  143, 1036, 1248,\n",
      "          154, 2210, 2355,  885, 1393, 1604],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2286, 1745,  980, 1884, 1004, 1383,  660,\n",
      "         1719, 1498, 1719, 1852, 2371, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1835,  213, 1952, 1772,  672, 1976,\n",
      "         1112, 1071, 1079, 1027, 1079,  784],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  530, 1951,  970, 1544,\n",
      "         2280,  669,  277,  916, 1628, 1694],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1402,  474, 2201, 1384,  672,  486, 1738,  672,  486, 2273, 2090, 2222,\n",
      "         1894, 2283, 1908,  988,  272, 2246],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1547,  931,  256, 2391, 1860,  987, 2025, 1164, 1164, 1031,  931,\n",
      "         1877,  272, 1378, 1084, 1628, 2073],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1658,  633, 2195,  988,  189, 1539, 1539,  860,  512, 1739,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2041,\n",
      "          499, 2279, 1507, 2087,  147, 1328],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1835,  975, 2391,  484, 1052, 2257,  823,\n",
      "          680, 1073, 1393,  228, 1112, 1237],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,  191, 1402,  699, 2333,   89,\n",
      "         2085,  826, 2000,  396,   89, 2085,  745, 2000,  396,  102,  396, 1606,\n",
      "          601,  405,  320, 1898,  877, 1521],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  654, 1914,  464,  217, 1114, 1393, 1267,\n",
      "         1761, 1148,  464, 1664, 2124, 1393],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2153, 1188, 1506,  904,  988, 1687, 1597,  672,  938,\n",
      "          417, 1117,  636, 1014,  505,  767],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  598, 1350, 2369,  507, 1563,\n",
      "          223, 1755, 1568,  378, 2127, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2372],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 2244,  412, 1832, 1247,\n",
      "           35,  801, 1560, 1292, 1047, 1540],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1857,   23,  329, 1682,   41, 1172, 2015, 1188, 2124,\n",
      "          857, 1684, 2283, 1346,  427, 2283],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  854,  369, 1335,  953, 2224, 1167, 1415,\n",
      "           24,  953, 1438,  152,  133, 1398],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1725, 1503],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1206, 1252, 1110,  827, 2361, 1249, 1572, 1378,  544,\n",
      "         1138,  680, 1056,  433, 1628, 1037],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1381, 1049, 2312, 2008, 1907, 1969, 1508, 1295,   18,  136,  477,  573,\n",
      "         1682, 1290,  976, 1084, 1089,  107],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1674,  507,  439,\n",
      "         1120,   86,  748, 2179, 1568, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1651, 1100, 1983, 1188, 1831,  897,   23, 1506, 2396,\n",
      "          504,  938, 1713,  417, 1042,  464],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1658,  575, 2248, 1953, 1495,\n",
      "         1267,  467,  904, 1243, 1658, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          567, 2251, 1269, 1089, 2325,  988],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   88,\n",
      "          510,  569, 1748, 1599, 1185, 1889]], dtype=torch.int32) Labels: tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "i: 17\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  787,  417,\n",
      "         1042,  464,  931, 1973, 1980, 1378, 1166, 1573,  975, 1027, 1772, 1719,\n",
      "         1860,  118, 1318, 1027, 2391, 1063],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  232,  567,  237,  683,\n",
      "          277, 1333,   75, 2157, 1904, 1208],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,   88,  511,  935,  448],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  843, 1350, 1804,  408,\n",
      "         1027, 1111, 1183, 1027,  680,  392],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         2095, 1988,  938,  684, 2034,  122],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1130, 2316, 1309,\n",
      "           55, 1221,  532, 2210, 2303, 1231],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1745,    8, 1004,\n",
      "          346,  413, 1750,  916, 1009, 1868],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2175,   49, 1572, 1809, 2223,  672, 1860, 1130,\n",
      "         1693,  799, 2184, 1079, 1628,  875],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1664, 2024,  277,  712, 1951,  970,  620,  535, 1922, 1144,\n",
      "          127, 2158,  272, 2391, 1074,  620],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0, 1835,   23, 2355,  406,\n",
      "          864, 1685,  992,  672,  890, 2172, 1498, 1848,   37,  965, 1027, 1932,\n",
      "          925,  454, 2156, 1310, 1393,  272],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2153, 1188, 1506,  904,  988, 1687, 1597,  672,  938,\n",
      "          417, 1117,  636, 1014,  505,  767],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          894,  126,  441, 2401,  126, 2220, 1514,  126,  360,  984, 2362,  711,\n",
      "         1184, 2220, 1633, 1628, 1683, 1754],\n",
      "        [ 174,   84, 1260, 1886,  776, 1397, 2261, 1995, 1397, 1886,  106, 1089,\n",
      "         2261, 2278,  248, 1886, 1089, 1886, 1842, 1260,  645,  776,  404, 1278,\n",
      "         2350, 2172, 1260,  725, 1089,  418],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,  191, 1402,  699, 2333,   89,\n",
      "         2085,  826, 2000,  396,   89, 2085,  745, 2000,  396,  102,  396, 1606,\n",
      "          601,  405,  320, 1898,  877, 1521],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  249,\n",
      "         1118, 1287, 1441, 1273, 2042,  818,  956, 1848, 1823, 1222,  389, 2303,\n",
      "         1267,  732, 2162, 2250,  104,  957],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1658, 1668, 1014,  939, 1165, 1601,\n",
      "         2311, 1267,  467,  904, 1243,  822],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1445,  526,  197,  870,\n",
      "          410, 1323,  410,  349, 2382, 1446],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1835, 1973,   41, 2210, 1178, 1027,\n",
      "         1205,  224, 1240, 1214, 1801, 2230],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,   88,  442, 2101, 1677,  449,\n",
      "          291, 1954,  955, 1089, 1189, 1956],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2350, 1325],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1445, 1064, 2013,\n",
      "         1088, 1552, 1401, 1490, 2350, 1218],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2392, 1735,\n",
      "         2358,  445, 2328,  586,   54, 1335],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,   94,  756,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1745, 2218,  464, 1188, 1687,\n",
      "         1911, 1506, 2207,  938,  417, 1042],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1948,  470,  912,   57, 2219, 2102],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          464,  956, 2210, 1261, 1276,  855, 1578, 1687, 1466, 1578, 1469, 1385,\n",
      "          297,  928,  878, 1628, 1583, 2291],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  938,  417,  464,   23, 1506, 1188,   41, 2355,  885, 1292,\n",
      "         1586,  672,  897, 2297,  272,  583],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1028,  539,\n",
      "          130, 2020, 1954,  863,  603, 1889],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1206, 1252, 1573, 1562, 2158, 1936, 1608, 1482, 2391,\n",
      "          788, 1829,  374, 1367, 2173,  856]], dtype=torch.int32) Labels: tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1], dtype=torch.int32)\n",
      "i: 18\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1835, 1188,  752, 1682,  767, 2051,\n",
      "         2172, 1498,  272, 1801, 2230, 1866],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,   90, 1682,   41,  221,  535,  464,\n",
      "         1267, 1596,   23, 1259, 1050,  255],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2026,  830, 1549,  727, 1747,  532,  219,  702,  956,\n",
      "         1499, 2025,  217, 1078, 1683,  629],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,   60, 2288, 1363, 2350,\n",
      "         1497, 2128,  522,  197, 2259,  489],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 2341, 1547,  779, 1110,  893, 2391, 1027, 1772, 1334, 1693, 1079,\n",
      "         1378, 1906,  433, 2391, 1628, 2076],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089,  514,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1047, 1540,\n",
      "           23,  642, 1031,  272, 1024, 2207],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1693,  301,\n",
      "          949, 2172,  885,  335, 1393,  545],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, 1831, 1749, 2026, 1931, 1015,\n",
      "         1567, 2218, 1241,  504,  464, 1710],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1402, 1007, 1230, 1327,  142,  479,  681, 1719, 1169, 1719,  479,\n",
      "          718, 1719, 2226, 1594, 1477, 1292],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          514,   23,   41,  486, 1172, 1908, 2008,   41, 1685,  864,  486, 1172,\n",
      "         1908, 2008, 1628,  163, 1498,  932],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         2244,  412,  304, 2020,  599, 1247, 2171,   35,  715, 1462,  931, 2324,\n",
      "         1292, 2020, 1149,  227,  304, 1770],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  465, 2369, 1745,  253, 1004, 2214,\n",
      "          277, 1852,  660,  215,  272, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  272, 1634,  465, 1350,  238, 1824,  980,\n",
      "          752, 1634, 1112,  272, 1027,  802],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1534,  197, 2350, 2347],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  198,  962,  278,  871,  787,  752,  781, 1571, 1328,\n",
      "          147, 1518,  308,  525, 1657, 1257],\n",
      "        [   0,    0, 1759,  825, 1619,  825, 1646, 2096,  220,  581,  955, 1095,\n",
      "          478, 1841, 2304, 1099,  730,  674,  666,  733,  381,  470,  730,  211,\n",
      "         1948,  688, 2009, 2336, 1948, 1437],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1565, 1474, 1069, 2225],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,  467,  996,   23, 2194,   41,   98,\n",
      "         1035, 1682, 1509, 1050, 2343,   23],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   10,  541,\n",
      "           29, 1315,    4, 1341,  685,  870],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  654, 1144, 1391, 1294, 1711,\n",
      "          743, 1188, 1682,  873,  154,  464],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2153, 1188, 1506,  904,  988, 1687, 1597,  672,  938,\n",
      "          417, 1117,  636, 1014,  505,  767],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1537,\n",
      "          355,  582, 2057, 1546, 2151, 1081],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1263, 1783, 1480, 2290, 2023,   41,  221,   41,\n",
      "          221,   41,  221, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   23, 1188,   94,  488,   41,  221,   41,  221,  532,\n",
      "          329, 1728,  334, 1089, 1027,  639],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,  151,  465, 1609, 2204,\n",
      "          651,  834,  187, 1594, 1484, 2176],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          863, 1247, 2078, 1350, 1835, 1408, 1027, 1772, 1334, 1247, 1529, 2158,\n",
      "          680, 2028,  433, 1628, 1507, 1520],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  198,  787,  752,  181, 1731,  278,  227,\n",
      "         2047, 1909,  314, 1425, 1628, 2030],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          870,  870,  988, 1152,   35, 2129]], dtype=torch.int32) Labels: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0], dtype=torch.int32)\n",
      "i: 19\n",
      "Inputs: tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 2090, 2222, 1894, 1264, 1908,  988,\n",
      "          813,  374, 2192, 1113, 2246, 1965],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1835,  779, 1110, 2281, 2391, 1541, 1334,  680,\n",
      "         1378, 1906,  433, 1084, 2106, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1130, 2316, 1309,   55,\n",
      "          932, 1221,  532, 2210, 2303, 1231],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2225,  604, 2186, 1504, 2288, 1690,  596, 1084, 1711,\n",
      "           47, 1439, 1331,  806, 2008, 1770],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1133,\n",
      "          291, 1361,  741, 1979,  127, 2146],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, 2225,  352, 1732,\n",
      "          507, 1563,  223, 1852,  378, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1680,  558,  186,  611,\n",
      "          672,  911,  111, 1257, 1943,  784],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,  272, 1110, 1135,\n",
      "         2203, 1835,  931, 1378,  440,  417, 1042, 1562,  931,  345, 1067, 1373,\n",
      "           37,  264,  433, 2391, 1628, 1388],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1658, 1668, 1014,  939, 1165, 1601,\n",
      "         2311, 1267,  467,  904, 1243,  822],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1517,\n",
      "         1096, 2057,  464,  807,  931,  195],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  654,\n",
      "          964, 2340,  507,  816,  370,  694,   68, 1158, 1586,  464, 2059,  672,\n",
      "         2059, 1267,  354,  904,  272, 2342],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 2083, 2020,  870,  319, 1357],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  654, 1914,  464,  217, 1114, 1393, 1267,\n",
      "         1761, 1148,  464, 1393,   11, 1684],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  272, 1634, 1824,  980,  752,\n",
      "         2363, 1634, 1112,  272, 1628, 1818],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  503, 2225, 2064, 1716, 1563, 2214, 1837, 1768, 1852,\n",
      "          660, 1719, 1498, 1719,  272, 1257],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  353, 1434,  487,  888, 1715, 1660, 1399,\n",
      "         2391,  499, 1719,  272, 1628,  475],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1824, 1525,  186,  105,\n",
      "         1693, 1079,  272, 1965, 1214, 1001],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2063, 2008, 1391,  464, 1761, 2124, 1499,  464,\n",
      "         1188,  642,  300, 1393,   11,  988],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  272, 1634,  465, 1350,  238, 1824,  980,\n",
      "          752, 1634, 1112,  272, 1027,  802],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  435, 1567, 2049, 1492, 2214,  277, 2301, 1195, 2170,\n",
      "         1039, 1997,  519, 2364, 1477, 1880],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1602, 2180, 2241, 1803,  680, 2389],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  696, 1200, 1202, 1212,  365,  246, 1200, 2402,  328,\n",
      "          549,  971, 1191,  744, 1369, 1243],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          272, 1563,  844, 1719, 2067, 1763, 1247, 1178,  428, 1039,  428,  282,\n",
      "          519, 1110,  378, 1393,  646,  663],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,  410, 1434, 1697,   21, 1707,\n",
      "          823, 2270, 2035, 1042, 1977,  997],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,  353, 1434,  487,  888, 1715, 1660, 1399,\n",
      "         2391,  499, 1719,  272, 1628,  475],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1779, 1173, 2020,  879,  291],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1955,  223, 1778, 1093,  485,  873,  154, 1102,  154,\n",
      "          924, 1949, 1092, 2062, 1512,  447],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0, 1632,  880,  608,  709,\n",
      "         1569, 1047, 1540, 1493, 1055,   91],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1047, 1540,  344,  143,  301, 1283,\n",
      "         1371,  713,  154, 2210, 2355, 1198]], dtype=torch.int32) Labels: tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "Best Threshold=0.270063, G-Mean=0.496\n",
      "Amount of test data: 600\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Accuracy Score: 0.45667214275220047\n",
      "\n",
      "Accuracy Score: 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(load_model.parameters(), lr=lr)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "    eval_losses = []\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        print(f\"i: {i}\\nInputs: {inputs} Labels: {labels}\")\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "\n",
    "        output, _ = load_model(inputs.to(torch.long), h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        eval_losses.append(loss)\n",
    "        preds = output.squeeze()\n",
    "        if len(labels) > 1:\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.numpy().astype(int))\n",
    "        # print(f\"Preds: {preds}\\n Preds Type: {type(preds)}\")\n",
    "\n",
    "\n",
    "roc_acc_score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels_list, test_preds)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# Index of largest G-means\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "threshold = thresholds[ix]\n",
    "\n",
    "# Print how many data is being tested\n",
    "print(f\"Amount of test data: {len(test_labels_list)}\")\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "print(f\"ROC Accuracy Score: {roc_acc_score}\")\n",
    "\n",
    "# Normalize probability with threshold\n",
    "test_preds_thresholded = np.where(test_preds > threshold, 1, 0)\n",
    "for i in range(len(test_preds)-1140):\n",
    "    print(\"Test Preds Prob: {}    \\\n",
    "    Test Preds Label: {}  \\\n",
    "    True Label: {}  \\\n",
    "    \".format(test_preds[i], test_preds_thresholded[i], test_labels_list[i]))\n",
    "\n",
    "acc_score = accuracy_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"\\nAccuracy Score: {acc_score}\")\n",
    "\n",
    "# Calculate F1 Score\n",
    "# f1_score = f1_score(test_labels_list, test_preds_thresholded)\n",
    "# print(f\"F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, PySyft does not support optimizers with momentum. Therefore, we are going to stick with the classical [Stochastic Gradient Descent](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) optimizer.\n",
    "\n",
    "As our task consists of a binary classification, we are going to use the [Binary Cross Entropy Loss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T20:00:23.084933Z",
     "start_time": "2019-06-03T20:00:23.078688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining loss and optimizer\n",
    "second_model = make_model()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(second_model.parameters(), lr=lr)\n",
    "\n",
    "# Create data\n",
    "# # Creating federated datasets, an extension of Pytorch TensorDataset class\n",
    "federated_train_dataset = sy.FederatedDataset([bob_train_dataset, anne_train_dataset])\n",
    "federated_test_dataset = sy.FederatedDataset([bob_test_dataset, anne_test_dataset])\n",
    "\n",
    "# Creating federated dataloaders, an extension of Pytorch DataLoader class for TRAINIG METHOD #2\n",
    "federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "federated_test_loader = sy.FederatedDataLoader(federated_test_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:56:01.459697Z",
     "start_time": "2019-06-03T19:33:42.666174Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100...      AUC: 48.403%...      Training loss: 0.68405...      Validation loss: 0.65751\n",
      "Trigger Times: 0\n",
      "Epoch 2/100...      AUC: 51.496%...      Training loss: 0.66479...      Validation loss: 0.63914\n",
      "Trigger Times: 0\n",
      "Epoch 3/100...      AUC: 55.271%...      Training loss: 0.65010...      Validation loss: 0.62453\n",
      "Trigger Times: 0\n",
      "Epoch 4/100...      AUC: 59.010%...      Training loss: 0.64391...      Validation loss: 0.61195\n",
      "Trigger Times: 0\n",
      "Epoch 5/100...      AUC: 61.341%...      Training loss: 0.63333...      Validation loss: 0.60147\n",
      "Trigger Times: 0\n",
      "Epoch 6/100...      AUC: 63.991%...      Training loss: 0.62502...      Validation loss: 0.59284\n",
      "Trigger Times: 0\n",
      "Epoch 7/100...      AUC: 66.229%...      Training loss: 0.62000...      Validation loss: 0.58536\n",
      "Trigger Times: 0\n",
      "Epoch 8/100...      AUC: 68.633%...      Training loss: 0.61222...      Validation loss: 0.57813\n",
      "Trigger Times: 0\n",
      "Epoch 9/100...      AUC: 70.190%...      Training loss: 0.60666...      Validation loss: 0.57225\n",
      "Trigger Times: 0\n",
      "Epoch 10/100...      AUC: 71.190%...      Training loss: 0.60387...      Validation loss: 0.56651\n",
      "Trigger Times: 0\n",
      "Epoch 11/100...      AUC: 72.166%...      Training loss: 0.60086...      Validation loss: 0.56133\n",
      "Trigger Times: 0\n",
      "Epoch 12/100...      AUC: 72.884%...      Training loss: 0.60020...      Validation loss: 0.55672\n",
      "Trigger Times: 0\n",
      "Epoch 13/100...      AUC: 73.602%...      Training loss: 0.59065...      Validation loss: 0.55192\n",
      "Trigger Times: 0\n",
      "Epoch 14/100...      AUC: 74.142%...      Training loss: 0.58475...      Validation loss: 0.54734\n",
      "Trigger Times: 0\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# For Early Stopping\n",
    "last_loss = 100\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "SECOND_EPOCHS = 100\n",
    "\n",
    "for e in range(SECOND_EPOCHS):\n",
    "    \n",
    "    ######### Training ##########\n",
    "\n",
    "    losses = []\n",
    "    # Batch loop\n",
    "    for inputs, labels in federated_train_loader:\n",
    "        # Location of current batch\n",
    "        worker = inputs.location\n",
    "        # Initialize hidden state and send it to worker\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
    "        # Send model to current worker\n",
    "        second_model.send(worker)\n",
    "        # Setting accumulated gradients to zero before backward step\n",
    "        optimizer.zero_grad()\n",
    "        # Output from the model\n",
    "        output, _ = second_model(inputs.to(torch.long), h)\n",
    "        # print(f\"Output:{output}\")\n",
    "        # Calculate the loss and perform backprop\n",
    "        # print(f\"Output Shape: {output.shape} Labels Shape: {labels.shape}\")\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # # Clipping the gradient to avoid explosion\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        # Backpropagation step\n",
    "        optimizer.step() \n",
    "        # Get the model back to the local worker\n",
    "        second_model.get()\n",
    "        losses.append(loss.get())\n",
    "    \n",
    "    \n",
    "    ######## Evaluation ##########\n",
    "    \n",
    "    # Model in evaluation mode\n",
    "    second_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        test_labels_list = []\n",
    "        eval_losses = []\n",
    "\n",
    "        for inputs, labels in federated_test_loader:\n",
    "            # get current location\n",
    "            worker = inputs.location\n",
    "            # Initialize hidden state and send it to worker\n",
    "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)    \n",
    "            # Send model to worker\n",
    "            second_model.send(worker)\n",
    "            output, _ = second_model(inputs.to(torch.long), h)\n",
    "            # loss = criterion(output.squeeze(), labels.float())\n",
    "            loss = criterion(output, labels.float())\n",
    "            eval_losses.append(loss.get())\n",
    "            preds = output.squeeze().get()\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.get().numpy().astype(int))\n",
    "            # Get the model back to the local worker\n",
    "            second_model.get()\n",
    "\n",
    "    # Check test preds\n",
    "    score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "    train_loss = sum(losses)/len(losses)\n",
    "    eval_loss = sum(eval_losses)/len(eval_losses)\n",
    "    \n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(eval_loss.item())\n",
    "    \n",
    "    print(\"Epoch {}/{}...  \\\n",
    "    AUC: {:.3%}...  \\\n",
    "    Training loss: {:.5f}...  \\\n",
    "    Validation loss: {:.5f}\".format(e+1, SECOND_EPOCHS, score, train_loss, eval_loss))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if eval_loss > last_loss:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times: {trigger_times}\")\n",
    "        \n",
    "        if trigger_times >= patience:\n",
    "            print(\"EARLY STOPPING! STARTING TEST PROCESS...\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Trigger Times: 0\")\n",
    "        trigger_times = 0\n",
    "    \n",
    "    last_loss = eval_loss\n",
    "    \n",
    "    second_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Method #2 Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Losses: {train_losses}\")\n",
    "plt.plot(train_losses, 'r')\n",
    "plt.plot(test_losses, 'b')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "PATH = \"state_dict_model.pt\"\n",
    "torch.save(second_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask for input and pre-process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('manually_collected_sms_600.csv', sep=',', names=['Teks', 'Label'])\n",
    "# data = pd.read_csv('manually_collected_sms_600.csv', sep=',', names=['Teks', 'Label'], header=0)\n",
    "data = data.sample(frac = 1)\n",
    "# Lowercase, remove unnecessary char with regex, remove stop words\n",
    "data.Teks = data.Teks.apply(clean_text)\n",
    "#     print(data.Teks)\n",
    "words = set((' '.join(data.Teks)).split())\n",
    "#     print(words)\n",
    "word_to_idx = {word: i for i, word in enumerate(words, start=1)}\n",
    "#     pprint(word_to_idx)\n",
    "tokens = data.Teks.apply(lambda x: tokenize(x, word_to_idx))\n",
    "#     print(tokens)\n",
    "inputs = pad_and_truncate(tokens)\n",
    "#     pprint(inputs)\n",
    "labels = np.array((data.Label == '1').astype(int))\n",
    "\n",
    "np.save('test_labels.npy', labels)\n",
    "np.save('test_inputs.npy', inputs)\n",
    "\n",
    "test_inputs = torch.tensor(np.load('test_inputs.npy'))\n",
    "test_labels = torch.tensor(np.load('test_labels.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing params\n",
    "VOCAB_SIZE = int(test_inputs.max()) + 1\n",
    "TEST_VOCAB_SIZE = TRAIN_VOCAB_SIZE\n",
    "lr = 0.01\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"state_dict_model.pt\"\n",
    "load_model = GRU(vocab_size=TEST_VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(second_model.parameters(), lr=lr)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "    eval_losses = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
    "\n",
    "        output, _ = second_model(inputs.to(torch.long), h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        eval_losses.append(loss)\n",
    "        preds = output.squeeze()\n",
    "        test_preds += list(preds.numpy())\n",
    "        test_labels_list += list(labels.numpy().astype(int))\n",
    "\n",
    "\n",
    "roc_acc_score = roc_auc_score(test_labels_list, test_preds)\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels_list, test_preds)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# Index of largest G-means\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "threshold = thresholds[ix]\n",
    "\n",
    "# Print how many data is being tested\n",
    "print(f\"Amount of test data: {len(test_labels_list)}\")\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "print(f\"ROC Accuracy Score: {roc_acc_score}\")\n",
    "\n",
    "# Normalize probability with threshold\n",
    "test_preds_thresholded = np.where(test_preds > threshold, 1, 0)\n",
    "for i in range(len(test_preds)-1140):\n",
    "    print(\"Test Preds Prob: {}    \\\n",
    "    Test Preds Label: {}  \\\n",
    "    True Label: {}  \\\n",
    "    \".format(test_preds[i], test_preds_thresholded[i], test_labels_list[i]))\n",
    "\n",
    "acc_score = accuracy_score(test_labels_list, test_preds_thresholded)\n",
    "print(f\"\\nAccuracy Score: {acc_score}\")\n",
    "\n",
    "# Calculate F1 Score\n",
    "# f1_score = f1_score(test_labels_list, test_preds_thresholded)\n",
    "# print(f\"F1 Score: {f1_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "27726a6b9d0b3aecf19ddb8fb5d165165384e9a9dccccc704489801e8c9c2418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('spam_classifier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
